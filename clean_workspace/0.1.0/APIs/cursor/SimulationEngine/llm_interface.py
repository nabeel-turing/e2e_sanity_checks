from common_utils.print_log import print_log
# cursor/SimulationEngine/llm_interface.py
import os
import logging
import inspect
from typing import Optional
from google import genai
from google.genai import types
from cachetools import LRUCache
import pickle

# Load environment variables from .env file in the parent 'cursor' directory.
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv(filename=".env", raise_error_if_not_found=False, usecwd=False))

# Logger for this module's operations.
logger = logging.getLogger(__name__)


def _log_with_caller_info(level: int, message: str, exc_info: bool = False) -> None:
    """Logs a message including the caller's function name and line number."""
    log_message = message
    try:
        frame = inspect.currentframe()
        caller_frame = frame.f_back.f_back
        if caller_frame and caller_frame.f_code:
            func_name = caller_frame.f_code.co_name
            line_no = caller_frame.f_lineno
            log_message = f"{func_name}:{line_no} - {message}"
    except Exception:
        pass
    if level == logging.ERROR:
        logger.error(log_message, exc_info=exc_info)
    elif level == logging.WARNING:
        logger.warning(log_message, exc_info=exc_info)
    elif level == logging.INFO:
        logger.info(log_message)
    else:
        logger.debug(log_message)


# --- Configuration Values ---
# Attempt to get API key from both GEMINI_API_KEY and GOOGLE_API_KEY
# and assign to GOOGLE_API_KEY in the environment and to GEMINI_API_KEY_FROM_ENV.
GEMINI_API_KEY_FROM_ENV = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
if GEMINI_API_KEY_FROM_ENV:
    os.environ["GOOGLE_API_KEY"] = GEMINI_API_KEY_FROM_ENV

# The google-genai library uses GOOGLE_API_KEY from env.
# A single client instance is created for reuse.
if not os.getenv("GOOGLE_API_KEY"):
    logger.warning("Google API Key not found in environment. Functions requiring it will fail.")
    client = None
else:
    client = genai.Client()


# Read default model name from environment; fallback to a hardcoded general model if not set.
DEFAULT_LLM_MODEL = os.getenv(
    "DEFAULT_GEMINI_MODEL_NAME", "gemini-2.5-pro-preview-03-25"
)


def call_llm(
    prompt_text: str,
    model_name: Optional[str] = None,
    temperature: float = 0.2,
    timeout_seconds: Optional[float] = None
) -> str:
    """
    Sends a prompt to a Generative Model and returns its text response.

    This function interfaces with the Google Generative AI service using the google-genai
    library. The API key for authentication is expected to be available as the 
    'GOOGLE_API_KEY' environment variable (which can be sourced from 'GEMINI_API_KEY'
    via .env). The default model can be configured via the 'DEFAULT_GEMINI_MODEL_NAME'
    environment variable.

    Args:
        prompt_text (str): The textual prompt to be sent to the LLM.
        model_name (Optional[str]): The identifier of the specific model to use
            (e.g., "gemini-1.5-pro-latest"). If None, uses the application's
            configured default model.
        temperature (float): Controls the randomness of the LLM's output, affecting
            creativity (0.0-1.0). Lower values (e.g., 0.2) are more deterministic.
            Defaults to 0.2.
        timeout_seconds (Optional[float]): Maximum time in seconds to wait for the
            API response. If None, the library's default timeout applies.
            Defaults to None.

    Returns:
        str: The text content generated by the LLM.

    Raises:
        ValueError: If the API client has not been initialized (due to a missing API key).
        RuntimeError: If the LLM API call fails for any reason (e.g., network issues,
                      API errors, content filtering, or if no usable text content
                      is returned by the model).
    """
    if not client:
        msg = "Google API Key not configured, client not initialized. Set GOOGLE_API_KEY (or GEMINI_API_KEY in .env)."
        _log_with_caller_info(logging.ERROR, msg)
        raise ValueError(msg)

    active_model_name = model_name or DEFAULT_LLM_MODEL
    actual_read_timeout_for_log = "Default"
    
    try:
        http_options = None
        if timeout_seconds is not None:
            try:
                timeout_val_float = float(timeout_seconds)
                if timeout_val_float > 0:
                    timeout_val_ms = timeout_val_float * 1000 # convert to milliseconds 
                    http_options = types.HttpOptions(timeout=timeout_val_float)
                    actual_read_timeout_for_log = f"{timeout_val_float} seconds"
                else:
                    _log_with_caller_info(logging.WARNING, f"Timeout value must be positive; ignoring non-positive value: {timeout_val_float}")
            except (ValueError, TypeError) as e:
                _log_with_caller_info(logging.WARNING, f"Invalid timeout value '{timeout_seconds}'. Using library default. Error: {e}")

        _log_with_caller_info(logging.INFO, f"Initiating LLM call. Model: {active_model_name}, Temp: {temperature}, Client Read Timeout: {actual_read_timeout_for_log}.")

        response = client.models.generate_content(
            model=f"{active_model_name}",
            contents=prompt_text,
            config=types.GenerateContentConfig(
                temperature=temperature,
                http_options=http_options if http_options else None,
            ),
        )

        if response.text:
            return response.text
        else:
            feedback_info = "N/A"
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                feedback_info = str(response.prompt_feedback)
            
            msg = f"LLM (model: {active_model_name}) returned no usable text content. Feedback: {feedback_info}"
            _log_with_caller_info(logging.ERROR, msg)
            raise RuntimeError(msg)

    except Exception as e:
        _log_with_caller_info(logging.ERROR, f"LLM call to model {active_model_name} encountered an error: {type(e).__name__} - {e}", exc_info=True)
        raise RuntimeError(f"LLM call failed: {type(e).__name__} - {e}") from e


# TODO: Also integrate the call_llm function into this class.
class GeminiEmbeddingManager:
    def __init__(self, gemini_api_key: str, lru_cache_file_path: str = None, max_cache_size: int = 1000):
        """
        Initializes the GeminiEmbeddingManager.

        Args:
            gemini_api_key (str): API key for Gemini.
            lru_cache_file_path (Optional[str]): Path to the LRUCache file. If None, caching is disabled. Defaults to None.
            max_cache_size (int): Maximum size of the LRUCache.
        """
        self.gemini_api_key = gemini_api_key
        self.cache = None
        self.lru_cache_file_path = lru_cache_file_path
        if lru_cache_file_path:
            self.cache = LRUCache(maxsize=max_cache_size)
            if os.path.exists(lru_cache_file_path):
                with open(lru_cache_file_path, "rb") as f:
                    # Use list to avoid unnecessary dict conversion
                    try:
                        for k, v in pickle.load(f):
                            self.cache[k] = v
                    except (pickle.UnpicklingError, EOFError, ValueError) as e:
                        print_log(f"Warning: Could not load cache file at '{lru_cache_file_path}'. Starting with an empty cache. Error: {e}")
                        self.cache = LRUCache(maxsize=max_cache_size) # Reset cache
            else:
                self._save_cache()
        
        self.client = None

    def _save_cache(self):
        if self.lru_cache_file_path and self.cache is not None:
            # Use list(self.cache.items()) for efficient serialization
            with open(self.lru_cache_file_path, "wb") as f:
                pickle.dump(list(self.cache.items()), f)

    def _embed_batch(self, gemini_model: str, texts: list[str], embedding_task_type: str, embedding_size: int):
        """Helper to call the embedding API for a batch of texts."""
        if self.client is None:
            self.client = genai.Client(api_key=self.gemini_api_key)
            
        embedContentResponse = self.client.models.embed_content(
            model=gemini_model,
            contents=texts,
            config=types.EmbedContentConfig(
                task_type=embedding_task_type,
                output_dimensionality=embedding_size,
            ),
        )

        return [embedding.values for embedding in embedContentResponse.embeddings]

    def embed_content(self, gemini_model: str, uncached_texts: list[str], embedding_task_type: str, embedding_size: int):
        """
        Calls the Gemini API to generate embeddings for the provided texts, using the LRUCache for caching.

        Args:
            gemini_model (str): Model name to use.
            uncached_texts (list): List of texts to embed.
            embedding_task_type (str): Task type for embedding.
            embedding_size (int): Output dimensionality for embeddings.

        Returns:
            Embeddings as returned by genai.embed_content.
        """
        # Fast path: all empty
        if not any(uncached_texts):
            return {"embedding": [[0.0] * embedding_size for _ in uncached_texts]}

        # If no cache, batch call for all non-empty
        if self.cache is None:
            non_empty_indices = [i for i, t in enumerate(uncached_texts) if t]
            non_empty_texts = [uncached_texts[i] for i in non_empty_indices]
            if not non_empty_texts:
                return {"embedding": [[0.0] * embedding_size for _ in uncached_texts]}
            
            embeddings = self._embed_batch(gemini_model, non_empty_texts, embedding_task_type, embedding_size)

            # Pre-allocate result and fill in
            result = [[0.0] * embedding_size for _ in uncached_texts]
            for idx, emb in zip(non_empty_indices, embeddings):
                result[idx] = emb
            return {"embedding": result}

        # With cache: check all at once, batch only uncached
        results = [None] * len(uncached_texts)
        uncached = []
        uncached_indices = []
        for idx, text in enumerate(uncached_texts):
            if not text:
                results[idx] = [0.0] * embedding_size
            else:
                cached = self.cache.get(text, None)
                if cached is not None:
                    results[idx] = cached
                else:
                    uncached.append(text)
                    uncached_indices.append(idx)

        if uncached:
            # Batch call for all uncached
            embeddings = self._embed_batch(gemini_model, uncached, embedding_task_type, embedding_size)
            for idx, emb, text in zip(uncached_indices, embeddings, uncached):
                self.cache[text] = emb
                results[idx] = emb
            self._save_cache()

        return {"embedding": results}
