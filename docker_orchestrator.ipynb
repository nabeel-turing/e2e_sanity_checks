{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import shlex\n",
    "import shutil\n",
    "import random\n",
    "import pathlib\n",
    "import subprocess\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "\n",
    "import docker\n",
    "import nbformat\n",
    "import gspread\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable, Dict, List, Sequence, Iterable, Union\n",
    "\n",
    "import pandas as pd\n",
    "from rclone_python import rclone\n",
    "from nbclient import NotebookClient\n",
    "from rclone_python.remote_types import RemoteTypes\n",
    "\n",
    "from google.auth import default\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "from googleapiclient.http import BatchHttpRequest, MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_FILE = 'turing-delivery-g-ga-e36eb2300714.json'\n",
    "\n",
    "# Combine scopes for both Drive and Sheets\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "]\n",
    "\n",
    "def authenticate_with_service_account():\n",
    "    \"\"\"Authenticate using a service account and return credentials.\"\"\"\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    return creds\n",
    "\n",
    "# Get the shared credentials object\n",
    "credentials = authenticate_with_service_account()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "# @title Logger Configs\n",
    "custom_theme = Theme({\n",
    "    \"info\": \"cyan\",\n",
    "    \"warning\": \"magenta\",\n",
    "    \"error\": \"bold red\"\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "class Logger:\n",
    "  @staticmethod\n",
    "  def log(message):\n",
    "    console.print(message, style=\"info\")\n",
    "\n",
    "  def error(message):\n",
    "    console.print(message, style=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleService Class\n",
    "class GoogleService:\n",
    "\n",
    "  @classmethod\n",
    "  def extract_file_id(cls, url):\n",
    "      patterns = [\n",
    "          r\"/spreadsheets/d/([^/]+)\",\n",
    "          r\"/file/d/([^/]+)\",     # Matches /file/d/{file_id}\n",
    "          r\"[?&]id=([^&]+)\",       # Matches ?id={file_id} or &id={file_id}\n",
    "          r\"/drive/([^/?#]+)\",     # Matches /drive/{file_id} and stops at /, ?, or #\n",
    "          r\"/folders/([^/]+)\"      # Matches /folders/{folder_id}\n",
    "      ]\n",
    "\n",
    "      for pattern in patterns:\n",
    "          match_ = re.search(pattern, url)\n",
    "          if match_:\n",
    "              return match_.group(1).strip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleDrive Functionality\n",
    "class GoogleDrive(GoogleService):\n",
    "    \n",
    "    service = build(\"drive\", \"v3\", credentials=credentials)\n",
    "\n",
    "    @classmethod\n",
    "    def get_file_names_in_batch(cls, file_ids):\n",
    "        \"\"\"\n",
    "        Retrieves the names of multiple files from Google Drive in a single batch request.\n",
    "        \n",
    "        Args:\n",
    "            drive_service: An authenticated Google Drive API service object.\n",
    "            file_ids: A list of file IDs.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary mapping file IDs to their names.\n",
    "        \"\"\"\n",
    "        file_names = []\n",
    "    \n",
    "        def callback(request_id, response, exception):\n",
    "            \"\"\"\n",
    "            Callback function to process the result of each individual request.\n",
    "            \"\"\"\n",
    "            if exception:\n",
    "                print(f\"Error for file ID {request_id}: {exception}\")\n",
    "                \n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': None\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': response.get('name')\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "        # Create a batch request with the callback\n",
    "        batch = cls.service.new_batch_http_request(callback=callback)\n",
    "    \n",
    "        # Add a 'files().get()' request for each file ID\n",
    "        for file_id in file_ids:\n",
    "            batch.add(\n",
    "                cls.service.files().get(\n",
    "                    fileId=file_id,\n",
    "                    fields='name'\n",
    "                ),\n",
    "                request_id=file_id  # Use the file ID to track each request\n",
    "            )\n",
    "    \n",
    "        # Execute the batch request\n",
    "        batch.execute()\n",
    "    \n",
    "        return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleSheets Functionality\n",
    "class GoogleSheet(GoogleService):\n",
    "\n",
    "  # service = build(\"sheets\", \"v4\")\n",
    "  service = build(\"sheets\", \"v4\", credentials=credentials)\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_data(cls, sheet_id: str, tab_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from existing Google Sheet and returns it as Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sheet_id: The ID of the existing Google Sheet.\n",
    "        tab_name: The desired name for the new tab.\n",
    "        filter_col [Optional]: column name to filter the data.\n",
    "        filter_val [Optional]: value to filter the data on.\n",
    "    \"\"\"\n",
    "    vals = (\n",
    "        cls.service.spreadsheets()\n",
    "        .values()\n",
    "        .get(spreadsheetId=sheet_id, range=tab_name)\n",
    "        .execute()\n",
    "        .get(\"values\", [])\n",
    "    )\n",
    "    if len(vals) > 0:\n",
    "      header = vals[0]\n",
    "      data_values = vals[1:]\n",
    "      max_columns = min(len(header), len(data_values[0]))\n",
    "      data_values = [row[:max_columns] for row in data_values]\n",
    "      header = header[:max_columns]\n",
    "      df = pd.DataFrame(data_values, columns=header)\n",
    "      df.columns = [column.strip() for column in df.columns]\n",
    "      filter_cols = [col.strip() for col in kwargs.keys()]\n",
    "      if filter_cols:\n",
    "        if all(col in df.columns for col in filter_cols):\n",
    "          query = \" & \".join([\n",
    "              f\"{col}=='{kwargs[col]}'\"\n",
    "              if isinstance(kwargs[col], str)\n",
    "              else f\"{col}=={kwargs[col]}\"\n",
    "              for col in filter_cols])\n",
    "          df = df.query(query)\n",
    "        else:\n",
    "          missing_cols = [col for col in filter_cols if col not in df.columns]\n",
    "          raise Exception(f\"Could not find column(s) in the sheet. {missing_cols}\")\n",
    "      return df\n",
    "    sheet_name = cls.get_spreadsheet_name_by_id(sheet_id)\n",
    "    raise Exception(f\"No data found in the Tab: {tab_name}. Sheet ID: {sheet_name}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def tab_exists(cls, spreadsheet_id, tab_name):\n",
    "\n",
    "    spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        fields='sheets.properties'\n",
    "    ).execute()\n",
    "\n",
    "    sheets = spreadsheet_metadata.get('sheets', [])\n",
    "    for sheet in sheets:\n",
    "        properties = sheet.get('properties')\n",
    "        if properties and (properties.get('title') == tab_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dataframe_to_sheet(cls, spreadsheet_id, df, tab_name, valueInputOption='RAW', drop_duplicates_on=['sample_id']):\n",
    "    \"\"\"\n",
    "    Adds a new tab to an existing Google Sheet and populates it with data from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spreadsheet_id: The ID of the existing Google Sheet.\n",
    "        df: The Pandas DataFrame to export.\n",
    "        tab_name: The desired name for the new tab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      if cls.tab_exists(spreadsheet_id, tab_name):\n",
    "        Logger.log(f\"Tab '{tab_name}' already exists in the spreadsheet.\")\n",
    "        existing_df = cls.get_sheet_data(spreadsheet_id, tab_name)\n",
    "        # TODO: Add dataframe validation check\n",
    "        Logger.log(f\"Existing Dataframe\")\n",
    "        Logger.log(existing_df.info())\n",
    "\n",
    "        combined_df = pd.concat([df, existing_df], ignore_index=True)\n",
    "        df_to_upload = combined_df.drop_duplicates(subset=drop_duplicates_on, keep='first', ignore_index=True)\n",
    "        Logger.log(f\"Combined Dataframe\")\n",
    "        Logger.log(df_to_upload.info())\n",
    "\n",
    "      else:\n",
    "        Logger.log(f\"Tab '{tab_name}' does not exist in the spreadsheet. Creating a new tab.\")\n",
    "        requests = [{\n",
    "            'addSheet': {\n",
    "                'properties': {\n",
    "                    'title': tab_name\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        batch_update_body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=batch_update_body\n",
    "        ).execute()\n",
    "        # Get the ID of the newly created sheet (optional, but useful)\n",
    "        new_sheet_id = response.get('replies')[0].get('addSheet').get('properties').get('sheetId')\n",
    "        Logger.log(f\"Successfully added new tab: '{tab_name}' with ID: {new_sheet_id}\")\n",
    "        df_to_upload = df\n",
    "\n",
    "      values = [df_to_upload.columns.tolist()] + df_to_upload.values.tolist()\n",
    "      Logger.log(f\"Uploading {len(df_to_upload)} rows to tab '{tab_name}'.\")\n",
    "      range_name = f\"'{tab_name}'!A1\" # Ensure tab name is quoted if it has spaces or special characters\n",
    "      body = {\n",
    "          'values': values\n",
    "      }\n",
    "      result = cls.service.spreadsheets().values().update(\n",
    "          spreadsheetId=spreadsheet_id,\n",
    "          range=range_name,\n",
    "          valueInputOption=valueInputOption,\n",
    "          body=body\n",
    "      ).execute()\n",
    "\n",
    "      Logger.log(f\"{result.get('updatedCells')} cells updated in tab '{tab_name}'.\")\n",
    "\n",
    "    except HttpError as err:\n",
    "      Logger.error(f\"An error occurred: {err}\")\n",
    "      if err.resp.status == 400: # Bad Request, often due to sheet name already existing\n",
    "        Logger.error(\"Error details: The tab name might already exist or the request is malformed.\")\n",
    "      elif err.resp.status == 403: # Forbidden, often due to incorrect permissions\n",
    "        Logger.error(\"Error details: Check your API permissions or if the service account/user has access to the sheet.\")\n",
    "      elif err.resp.status == 404: # Not Found, often due to incorrect spreadsheet ID\n",
    "        Logger.error(\"Error details: The spreadsheet ID might be incorrect.\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_spreadsheet_name_by_id(cls, spreadsheet_id):\n",
    "      \"\"\"\n",
    "      Retrieves the name (title) of a Google Spreadsheet given its ID.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id: The ID of the Google Spreadsheet.\n",
    "\n",
    "      Returns:\n",
    "          The title of the spreadsheet, or None if an error occurs or spreadsheet is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request the 'properties.title' field for efficiency\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='properties.title'\n",
    "          ).execute()\n",
    "\n",
    "          # Extract the title from the properties\n",
    "          title = spreadsheet_metadata.get('properties', {}).get('title')\n",
    "          return title\n",
    "      except HttpError as error:\n",
    "          print(f'An error occurred: {error}')\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found.\")\n",
    "          return None\n",
    "\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dropdown_to_range(cls, spreadsheet_id: str, sheet_id: str,\n",
    "                            dropdown_options: list,\n",
    "                            range_start_row: int, range_end_row: int,\n",
    "                            range_start_col: int, range_end_col: int):\n",
    "    requests = [\n",
    "        {\n",
    "            'setDataValidation': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': range_start_row,\n",
    "                    'endRowIndex': range_end_row,\n",
    "                    'startColumnIndex': range_start_col,\n",
    "                    'endColumnIndex': range_end_col\n",
    "                },\n",
    "                'rule': {\n",
    "                    'condition': {\n",
    "                        'type': 'ONE_OF_LIST',\n",
    "                        'values': [{'userEnteredValue': option} for option in dropdown_options]\n",
    "                    },\n",
    "                    'strict': True,  # Users can only enter values from the list\n",
    "                    'showCustomUi': True, # Show dropdown arrow\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # --- Execute the batch update request ---\n",
    "    try:\n",
    "        body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=body\n",
    "        ).execute()\n",
    "        print(f\"Dropdown added to Sheet ID {sheet_id}, Range row{range_start_row+1}:row{range_end_row}.\")\n",
    "        # You can inspect the response for more details if needed\n",
    "        # print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_id_by_name(cls, spreadsheet_id: str, tab_name: str):\n",
    "      \"\"\"\n",
    "      Retrieves the numerical Sheet ID (gid) for a given tab name within a spreadsheet.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id (str): The ID of the Google Spreadsheet.\n",
    "          tab_name (str): The exact name (title) of the tab/sheet to find.\n",
    "\n",
    "      Returns:\n",
    "          int: The numerical sheet ID (gid) if found.\n",
    "          None: If an error occurs, the spreadsheet is not found, or the tab name is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request 'sheets.properties' to get sheet IDs and titles efficiently\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='sheets.properties'\n",
    "          ).execute()\n",
    "\n",
    "          sheets = spreadsheet_metadata.get('sheets', [])\n",
    "          for sheet in sheets:\n",
    "              properties = sheet.get('properties')\n",
    "              # Check if properties exist and if the title matches the tab_name\n",
    "              if properties and properties.get('title') == tab_name:\n",
    "                  return properties.get('sheetId') # Return the sheetId (gid)\n",
    "\n",
    "          # If the loop completes, the tab was not found\n",
    "          print(f\"Tab '{tab_name}' not found in spreadsheet with ID '{spreadsheet_id}'.\")\n",
    "          return None\n",
    "      except HttpError as error:\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Error: {error}\")\n",
    "          else:\n",
    "              print(f'An HTTP error occurred: {error}')\n",
    "          return None\n",
    "      except Exception as e:\n",
    "          print(f\"An unexpected error occurred while retrieving sheet ID for tab '{tab_name}': {e}\")\n",
    "          return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download APIs Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_apis(VERSION=\"0.1.0\", download_datasets=False):\n",
    "    import io\n",
    "    import os\n",
    "    import sys\n",
    "    import zipfile\n",
    "    import shutil\n",
    "    import re\n",
    "    # from google.colab import auth\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "    drive_service = GoogleDrive.service\n",
    "    # Version to download\n",
    "    # VERSION = \"0.0.rev22final\" # Version of the API\n",
    "    \n",
    "    # Define paths\n",
    "    CONTENT_DIR = os.path.join('clean_workspace', VERSION)\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        os.remove(CONTENT_DIR)\n",
    "    os.makedirs(CONTENT_DIR, exist_ok=True)\n",
    "    \n",
    "    APIS_DIR = os.path.join(CONTENT_DIR, 'APIs')\n",
    "    DBS_DIR = os.path.join(CONTENT_DIR, 'DBs')\n",
    "    SCRIPTS_DIR = os.path.join(CONTENT_DIR, 'Scripts')\n",
    "    FC_DIR = os.path.join(CONTENT_DIR, 'Schemas')\n",
    "    ZIP_PATH = os.path.join(CONTENT_DIR, f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # Google Drive Folder ID where versioned APIs zip files are stored\n",
    "    APIS_FOLDER_ID = '1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4'\n",
    "    \n",
    "    # List of items to extract from the zip file\n",
    "    ITEMS_TO_EXTRACT = ['APIs/', 'DBs/', 'Scripts/']\n",
    "    \n",
    "    # Clean up existing directories and files\n",
    "    for path in [APIS_DIR, DBS_DIR, SCRIPTS_DIR, FC_DIR, ZIP_PATH]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "    \n",
    "    # Authenticate and create the drive service\n",
    "    # auth.authenticate_user()\n",
    "    # drive_service = build('drive', 'v3')\n",
    "    # drive_service\n",
    "    # Helper function to download a file from Google Drive\n",
    "    def download_drive_file(service, file_id, output_path, file_name=None, show_progress=True):\n",
    "        \"\"\"Downloads a file from Google Drive\"\"\"\n",
    "        destination = output_path\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        with io.FileIO(destination, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if show_progress:\n",
    "                    print(f\"Download progress: {int(status.progress() * 100)}%\")\n",
    "    \n",
    "    \n",
    "    # 1. List files in the specified APIs folder\n",
    "    print(f\"Searching for APIs zip file with version {VERSION} in folder: {APIS_FOLDER_ID}...\")\n",
    "    apis_file_id = None\n",
    "    \n",
    "    try:\n",
    "        query = f\"'{APIS_FOLDER_ID}' in parents and trashed=false\"\n",
    "        results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "        files = results.get('files', [])\n",
    "        for file in files:\n",
    "            file_name = file.get('name', '')\n",
    "            if file_name.lower() == f'apis_v{VERSION.lower()}.zip':\n",
    "                apis_file_id = file.get('id')\n",
    "                print(f\"Found matching file: {file_name} (ID: {apis_file_id})\")\n",
    "                break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing files in Google Drive: {e}\")\n",
    "    \n",
    "    if not apis_file_id:\n",
    "        print(f\"Error: Could not find APIs zip file with version {VERSION} in the specified folder.\")\n",
    "        sys.exit(\"Required APIs zip file not found.\")\n",
    "    \n",
    "    # 2. Download the found APIs zip file\n",
    "    print(f\"Downloading APIs zip file with ID: {apis_file_id}...\")\n",
    "    download_drive_file(drive_service, apis_file_id, ZIP_PATH, file_name=f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # 3. Extract specific items from the zip file to /content\n",
    "    print(f\"Extracting specific items from {ZIP_PATH} to {CONTENT_DIR}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_contents = zip_ref.namelist()\n",
    "    \n",
    "            for member in zip_contents:\n",
    "                extracted = False\n",
    "                for item_prefix in ITEMS_TO_EXTRACT:\n",
    "                  if member == item_prefix or member.startswith(item_prefix):\n",
    "                        zip_ref.extract(member, CONTENT_DIR)\n",
    "                        extracted = True\n",
    "                        break\n",
    "    \n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The downloaded file at {ZIP_PATH} is not a valid zip file.\")\n",
    "        sys.exit(\"Invalid zip file downloaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "        sys.exit(\"Extraction failed.\")\n",
    "    \n",
    "    \n",
    "    # 4. Clean up\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)\n",
    "    \n",
    "    # 5. Add APIs to path\n",
    "    if os.path.exists(APIS_DIR):\n",
    "        sys.path.append(APIS_DIR)\n",
    "    else:\n",
    "        print(f\"Error: APIS directory not found at {APIS_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    # 6. Quick verification\n",
    "    # Check for the presence of the extracted items\n",
    "    verification_paths = [APIS_DIR, DBS_DIR, SCRIPTS_DIR]\n",
    "    all_present = True\n",
    "    print(\"\\nVerifying extracted items:\")\n",
    "    for path in verification_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✅ {path} is present.\")\n",
    "        else:\n",
    "            print(f\"❌ {path} is MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(f\"\\n✅ Setup complete! Required items extracted to {CONTENT_DIR}.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Setup failed! Not all required items were extracted.\")\n",
    "\n",
    "    # 7. Generate Schemas\n",
    "\n",
    "    # Add Scripts to path\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        sys.path.append(CONTENT_DIR)\n",
    "    else:\n",
    "        print(f\"Error: CONTENT_DIR directory not found at {CONTENT_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    from Scripts.FCSpec import generate_package_schema\n",
    "    \n",
    "    print(\"\\nGenerating FC Schemas\")\n",
    "    os.makedirs(FC_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Iterate through the packages in the /content/APIs directory\n",
    "    for package_name in os.listdir(APIS_DIR):\n",
    "        package_path = os.path.join(APIS_DIR, package_name)\n",
    "    \n",
    "        # Check if it's a directory (to avoid processing files)\n",
    "        if os.path.isdir(package_path):\n",
    "            # Call the function to generate schema for the current package\n",
    "            generate_package_schema(package_path, output_folder_path=FC_DIR)\n",
    "    print(f\"✅ Successfully generated {len(os.listdir(FC_DIR))} FC Schemas to {FC_DIR}\")\n",
    "\n",
    "    if download_datasets:\n",
    "        def download_drive_folder(service, folder_id, destination_path):\n",
    "            \"\"\"\n",
    "            Recursively downloads all files in a Google Drive folder using the `download_drive_file`\n",
    "            \"\"\"\n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "        \n",
    "            query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "            page_token = None\n",
    "        \n",
    "            while True:\n",
    "                results = service.files().list(\n",
    "                    q=query,\n",
    "                    spaces='drive',\n",
    "                    fields='nextPageToken, files(id, name, mimeType)',\n",
    "                    pageToken=page_token\n",
    "                ).execute()\n",
    "        \n",
    "                for item in results.get('files', []):\n",
    "                    file_id = item['id']\n",
    "                    file_name = item['name']\n",
    "                    mime_type = item['mimeType']\n",
    "        \n",
    "                    if mime_type == 'application/vnd.google-apps.folder':\n",
    "                        # Recursively download subfolders\n",
    "                        new_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Creating subfolder and downloading: {new_path}\")\n",
    "                        download_drive_folder(service, file_id, new_path)\n",
    "                    else:\n",
    "                        # Construct full file path and pass it as output_path\n",
    "                        full_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Downloading file: {file_name} to {full_path}\")\n",
    "                        download_drive_file(service, file_id, full_path, file_name=file_name, show_progress=False)\n",
    "        \n",
    "                page_token = results.get('nextPageToken', None)\n",
    "                if not page_token:\n",
    "                    break\n",
    "        \n",
    "        # --- Configuration for Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'Quotewk.csv' file.\n",
    "        FOLDER_ID = \"1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml\"\n",
    "        # DATASET_FOLDER = \"./workspace/Datasets\"\n",
    "        DATASET_FOLDER = os.path.join(CONTENT_DIR, 'workspace/Datasets')\n",
    "        \n",
    "        print(f\"Starting download of folder {FOLDER_ID} to {DATASET_FOLDER}...\")\n",
    "        download_drive_folder(drive_service, FOLDER_ID, DATASET_FOLDER)\n",
    "        print(\"Dataset download complete.\")\n",
    "\n",
    "        # --- Configuration for WS Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'WS Multihop Datasets' file.\n",
    "        WS_DATA_ID = \"1kmXZ1oarBPlE0OQL52eGoc1xPbupJ1n9\"\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        \n",
    "        print(f\"Downloading WS Dataset zip file with ID: {WS_DATA_ID}...\")\n",
    "        download_drive_file(drive_service, WS_DATA_ID, WS_DATA_ZIP_PATH, file_name=f'WS_DATA.zip')\n",
    "        print(\"Dataset download complete.\")\n",
    "        \n",
    "        # Extract the Datasets\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        with zipfile.ZipFile(WS_DATA_ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(CONTENT_DIR)\n",
    "        print(f\"Extracted to {CONTENT_DIR}\")\n",
    "        \n",
    "        # Moving 'file_dataset_pb2.py' to root directory\n",
    "        src_path = os.path.join(CONTENT_DIR, 'WS_DATA', 'file_dataset_pb2.py')\n",
    "        dst_path = os.path.join(CONTENT_DIR, 'file_dataset_pb2.py')\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"Moved {src_path} to {dst_path}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {src_path}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(WS_DATA_ZIP_PATH):\n",
    "            os.remove(WS_DATA_ZIP_PATH)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for APIs zip file with version 0.1.0 in folder: 1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4...\n",
      "Found matching file: APIs_V0.1.0.zip (ID: 1hLV2slrHhH0RquKU-8oWRJRs_nHh5CT_)\n",
      "Downloading APIs zip file with ID: 1hLV2slrHhH0RquKU-8oWRJRs_nHh5CT_...\n",
      "Download progress: 100%\n",
      "Extracting specific items from clean_workspace/0.1.0/APIs_V0.1.0.zip to clean_workspace/0.1.0...\n",
      "\n",
      "Verifying extracted items:\n",
      "✅ clean_workspace/0.1.0/APIs is present.\n",
      "✅ clean_workspace/0.1.0/DBs is present.\n",
      "✅ clean_workspace/0.1.0/Scripts is present.\n",
      "\n",
      "✅ Setup complete! Required items extracted to clean_workspace/0.1.0.\n",
      "\n",
      "Generating FC Schemas\n",
      "✅ notes_and_lists Schema generation complete: clean_workspace/0.1.0/Schemas/notes_and_lists.json\n",
      "\n",
      "\n",
      "Processing mutation notes_and_lists.mutations.m01...\n",
      "✅ notes_and_lists.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/notes_and_lists.json\n",
      "\n",
      "✅ google_maps Schema generation complete: clean_workspace/0.1.0/Schemas/google_maps.json\n",
      "\n",
      "\n",
      "Processing mutation google_maps.mutations.m01...\n",
      "✅ google_maps.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_maps.json\n",
      "\n",
      "Error: Could not find a valid _function_map in clean_workspace/0.1.0/APIs/common_utils/__init__.py.\n",
      "✅ google_home Schema generation complete: clean_workspace/0.1.0/Schemas/google_home.json\n",
      "\n",
      "\n",
      "Processing mutation google_home.mutations.m01...\n",
      "✅ google_home.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_home.json\n",
      "\n",
      "✅ google_calendar Schema generation complete: clean_workspace/0.1.0/Schemas/google_calendar.json\n",
      "\n",
      "\n",
      "Processing mutation google_calendar.mutations.m01...\n",
      "✅ google_calendar.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_calendar.json\n",
      "\n",
      "✅ device_setting Schema generation complete: clean_workspace/0.1.0/Schemas/device_setting.json\n",
      "\n",
      "\n",
      "Processing mutation device_setting.mutations.m01...\n",
      "✅ device_setting.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/device_setting.json\n",
      "\n",
      "✅ call_llm Schema generation complete: clean_workspace/0.1.0/Schemas/call_llm.json\n",
      "\n",
      "\n",
      "Processing mutation call_llm.mutations.m01...\n",
      "✅ call_llm.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/call_llm.json\n",
      "\n",
      "✅ generic_media Schema generation complete: clean_workspace/0.1.0/Schemas/generic_media.json\n",
      "\n",
      "\n",
      "Processing mutation generic_media.mutations.m01...\n",
      "✅ generic_media.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/generic_media.json\n",
      "\n",
      "✅ messages Schema generation complete: clean_workspace/0.1.0/Schemas/messages.json\n",
      "\n",
      "\n",
      "Processing mutation messages.mutations.m01...\n",
      "✅ messages.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/messages.json\n",
      "\n",
      "✅ google_sheets Schema generation complete: clean_workspace/0.1.0/Schemas/google_sheets.json\n",
      "\n",
      "\n",
      "Processing mutation google_sheets.mutations.m01...\n",
      "✅ google_sheets.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_sheets.json\n",
      "\n",
      "✅ google_chat Schema generation complete: clean_workspace/0.1.0/Schemas/google_chat.json\n",
      "\n",
      "\n",
      "Processing mutation google_chat.mutations.m01...\n",
      "✅ google_chat.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_chat.json\n",
      "\n",
      "✅ google_cloud_storage Schema generation complete: clean_workspace/0.1.0/Schemas/google_cloud_storage.json\n",
      "\n",
      "\n",
      "Processing mutation google_cloud_storage.mutations.m01...\n",
      "✅ google_cloud_storage.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_cloud_storage.json\n",
      "\n",
      "✅ puppeteer Schema generation complete: clean_workspace/0.1.0/Schemas/puppeteer.json\n",
      "\n",
      "\n",
      "Processing mutation puppeteer.mutations.m01...\n",
      "Error: Could not find a valid _function_map in clean_workspace/0.1.0/APIs/puppeteer/mutations/m01/__init__.py.\n",
      "✅ copilot Schema generation complete: clean_workspace/0.1.0/Schemas/copilot.json\n",
      "\n",
      "\n",
      "Processing mutation copilot.mutations.m01...\n",
      "✅ copilot.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/copilot.json\n",
      "\n",
      "✅ cursor Schema generation complete: clean_workspace/0.1.0/Schemas/cursor.json\n",
      "\n",
      "\n",
      "Processing mutation cursor.mutations.m01...\n",
      "✅ cursor.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/cursor.json\n",
      "\n",
      "✅ workday Schema generation complete: clean_workspace/0.1.0/Schemas/workday.json\n",
      "\n",
      "\n",
      "Processing mutation workday.mutations.m01...\n",
      "✅ workday.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/workday.json\n",
      "\n",
      "✅ azure Schema generation complete: clean_workspace/0.1.0/Schemas/azure.json\n",
      "\n",
      "\n",
      "Processing mutation azure.mutations.m01...\n",
      "✅ azure.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/azure.json\n",
      "\n",
      "✅ media_control Schema generation complete: clean_workspace/0.1.0/Schemas/media_control.json\n",
      "\n",
      "\n",
      "Processing mutation media_control.mutations.m01...\n",
      "✅ media_control.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/media_control.json\n",
      "\n",
      "✅ google_meet Schema generation complete: clean_workspace/0.1.0/Schemas/google_meet.json\n",
      "\n",
      "\n",
      "Processing mutation google_meet.mutations.m01...\n",
      "✅ google_meet.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_meet.json\n",
      "\n",
      "✅ google_maps_live Schema generation complete: clean_workspace/0.1.0/Schemas/google_maps_live.json\n",
      "\n",
      "\n",
      "Processing mutation google_maps_live.mutations.m01...\n",
      "✅ google_maps_live.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_maps_live.json\n",
      "\n",
      "✅ contacts Schema generation complete: clean_workspace/0.1.0/Schemas/contacts.json\n",
      "\n",
      "\n",
      "Processing mutation contacts.mutations.m01...\n",
      "✅ contacts.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/contacts.json\n",
      "\n",
      "✅ spotify Schema generation complete: clean_workspace/0.1.0/Schemas/spotify.json\n",
      "\n",
      "\n",
      "Processing mutation spotify.mutations.m01...\n",
      "✅ spotify.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/spotify.json\n",
      "\n",
      "✅ canva Schema generation complete: clean_workspace/0.1.0/Schemas/canva.json\n",
      "\n",
      "\n",
      "Processing mutation canva.mutations.m01...\n",
      "✅ canva.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/canva.json\n",
      "\n",
      "✅ shopify Schema generation complete: clean_workspace/0.1.0/Schemas/shopify.json\n",
      "\n",
      "\n",
      "Processing mutation shopify.mutations.m01...\n",
      "✅ shopify.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/shopify.json\n",
      "\n",
      "✅ terminal Schema generation complete: clean_workspace/0.1.0/Schemas/terminal.json\n",
      "\n",
      "\n",
      "Processing mutation terminal.mutations.m01...\n",
      "✅ terminal.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/terminal.json\n",
      "\n",
      "✅ youtube_tool Schema generation complete: clean_workspace/0.1.0/Schemas/youtube_tool.json\n",
      "\n",
      "\n",
      "Processing mutation youtube_tool.mutations.m01...\n",
      "✅ youtube_tool.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/youtube_tool.json\n",
      "\n",
      "✅ tool_explorer Schema generation complete: clean_workspace/0.1.0/Schemas/tool_explorer.json\n",
      "\n",
      "\n",
      "Processing mutation tool_explorer.mutations.m01...\n",
      "✅ tool_explorer.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/tool_explorer.json\n",
      "\n",
      "✅ retail Schema generation complete: clean_workspace/0.1.0/Schemas/retail.json\n",
      "\n",
      "\n",
      "Processing mutation retail.mutations.m01...\n",
      "✅ retail.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/retail.json\n",
      "\n",
      "\n",
      "Processing mutation retail.mutations.smaller_toolset...\n",
      "✅ retail.mutations.smaller_toolset Schema generation complete: clean_workspace/0.1.0/MutationSchemas/smaller_toolset/retail.json\n",
      "\n",
      "✅ bigquery Schema generation complete: clean_workspace/0.1.0/Schemas/bigquery.json\n",
      "\n",
      "\n",
      "Processing mutation bigquery.mutations.m01...\n",
      "✅ bigquery.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/bigquery.json\n",
      "\n",
      "✅ google_docs Schema generation complete: clean_workspace/0.1.0/Schemas/google_docs.json\n",
      "\n",
      "\n",
      "Processing mutation google_docs.mutations.m01...\n",
      "✅ google_docs.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_docs.json\n",
      "\n",
      "✅ gdrive Schema generation complete: clean_workspace/0.1.0/Schemas/gdrive.json\n",
      "\n",
      "\n",
      "Processing mutation gdrive.mutations.m01...\n",
      "✅ gdrive.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/gdrive.json\n",
      "\n",
      "✅ youtube Schema generation complete: clean_workspace/0.1.0/Schemas/youtube.json\n",
      "\n",
      "\n",
      "Processing mutation youtube.mutations.m01...\n",
      "✅ youtube.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/youtube.json\n",
      "\n",
      "✅ google_slides Schema generation complete: clean_workspace/0.1.0/Schemas/google_slides.json\n",
      "\n",
      "\n",
      "Processing mutation google_slides.mutations.m01...\n",
      "✅ google_slides.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_slides.json\n",
      "\n",
      "✅ mongodb Schema generation complete: clean_workspace/0.1.0/Schemas/mongodb.json\n",
      "\n",
      "\n",
      "Processing mutation mongodb.mutations.m01...\n",
      "✅ mongodb.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/mongodb.json\n",
      "\n",
      "✅ sapconcur Schema generation complete: clean_workspace/0.1.0/Schemas/sapconcur.json\n",
      "\n",
      "\n",
      "Processing mutation sapconcur.mutations.m01...\n",
      "✅ sapconcur.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/sapconcur.json\n",
      "\n",
      "✅ device_actions Schema generation complete: clean_workspace/0.1.0/Schemas/device_actions.json\n",
      "\n",
      "\n",
      "Processing mutation device_actions.mutations.m01...\n",
      "✅ device_actions.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/device_actions.json\n",
      "\n",
      "✅ supabase Schema generation complete: clean_workspace/0.1.0/Schemas/supabase.json\n",
      "\n",
      "\n",
      "Processing mutation supabase.mutations.m01...\n",
      "✅ supabase.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/supabase.json\n",
      "\n",
      "✅ hubspot Schema generation complete: clean_workspace/0.1.0/Schemas/hubspot.json\n",
      "\n",
      "\n",
      "Processing mutation hubspot.mutations.m01...\n",
      "✅ hubspot.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/hubspot.json\n",
      "\n",
      "✅ google_search Schema generation complete: clean_workspace/0.1.0/Schemas/google_search.json\n",
      "\n",
      "\n",
      "Processing mutation google_search.mutations.m01...\n",
      "✅ google_search.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_search.json\n",
      "\n",
      "✅ gemini_cli Schema generation complete: clean_workspace/0.1.0/Schemas/gemini_cli.json\n",
      "\n",
      "\n",
      "Processing mutation gemini_cli.mutations.m01...\n",
      "✅ gemini_cli.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/gemini_cli.json\n",
      "\n",
      "✅ home_assistant Schema generation complete: clean_workspace/0.1.0/Schemas/home_assistant.json\n",
      "\n",
      "\n",
      "Processing mutation home_assistant.mutations.m01...\n",
      "✅ home_assistant.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/home_assistant.json\n",
      "\n",
      "✅ jira Schema generation complete: clean_workspace/0.1.0/Schemas/jira.json\n",
      "\n",
      "\n",
      "Processing mutation jira.mutations.m01...\n",
      "✅ jira.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/jira.json\n",
      "\n",
      "✅ github Schema generation complete: clean_workspace/0.1.0/Schemas/github.json\n",
      "\n",
      "\n",
      "Processing mutation github.mutations.m01...\n",
      "✅ github.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/github.json\n",
      "\n",
      "✅ figma Schema generation complete: clean_workspace/0.1.0/Schemas/figma.json\n",
      "\n",
      "\n",
      "Processing mutation figma.mutations.m01...\n",
      "✅ figma.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/figma.json\n",
      "\n",
      "✅ github_actions Schema generation complete: clean_workspace/0.1.0/Schemas/github_actions.json\n",
      "\n",
      "\n",
      "Processing mutation github_actions.mutations.m01...\n",
      "✅ github_actions.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/github_actions.json\n",
      "\n",
      "✅ zendesk Schema generation complete: clean_workspace/0.1.0/Schemas/zendesk.json\n",
      "\n",
      "\n",
      "Processing mutation zendesk.mutations.m01...\n",
      "✅ zendesk.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/zendesk.json\n",
      "\n",
      "✅ google_people Schema generation complete: clean_workspace/0.1.0/Schemas/google_people.json\n",
      "\n",
      "\n",
      "Processing mutation google_people.mutations.m01...\n",
      "✅ google_people.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_people.json\n",
      "\n",
      "✅ mysql Schema generation complete: clean_workspace/0.1.0/Schemas/mysql.json\n",
      "\n",
      "\n",
      "Processing mutation mysql.mutations.m01...\n",
      "✅ mysql.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/mysql.json\n",
      "\n",
      "✅ reddit Schema generation complete: clean_workspace/0.1.0/Schemas/reddit.json\n",
      "\n",
      "\n",
      "Processing mutation reddit.mutations.m01...\n",
      "✅ reddit.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/reddit.json\n",
      "\n",
      "✅ confluence Schema generation complete: clean_workspace/0.1.0/Schemas/confluence.json\n",
      "\n",
      "\n",
      "Processing mutation confluence.mutations.m01...\n",
      "✅ confluence.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/confluence.json\n",
      "\n",
      "✅ phone Schema generation complete: clean_workspace/0.1.0/Schemas/phone.json\n",
      "\n",
      "\n",
      "Processing mutation phone.mutations.m01...\n",
      "✅ phone.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/phone.json\n",
      "\n",
      "✅ sdm Schema generation complete: clean_workspace/0.1.0/Schemas/sdm.json\n",
      "\n",
      "\n",
      "Processing mutation sdm.mutations.m01...\n",
      "✅ sdm.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/sdm.json\n",
      "\n",
      "✅ salesforce Schema generation complete: clean_workspace/0.1.0/Schemas/salesforce.json\n",
      "\n",
      "\n",
      "Processing mutation salesforce.mutations.m01...\n",
      "✅ salesforce.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/salesforce.json\n",
      "\n",
      "✅ slack Schema generation complete: clean_workspace/0.1.0/Schemas/slack.json\n",
      "\n",
      "\n",
      "Processing mutation slack.mutations.m01...\n",
      "✅ slack.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/slack.json\n",
      "\n",
      "✅ tiktok Schema generation complete: clean_workspace/0.1.0/Schemas/tiktok.json\n",
      "\n",
      "\n",
      "Processing mutation tiktok.mutations.m01...\n",
      "✅ tiktok.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/tiktok.json\n",
      "\n",
      "✅ service_template Schema generation complete: clean_workspace/0.1.0/Schemas/service_template.json\n",
      "\n",
      "\n",
      "Processing mutation service_template.mutations.m01...\n",
      "✅ service_template.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/service_template.json\n",
      "\n",
      "✅ code_execution Schema generation complete: clean_workspace/0.1.0/Schemas/code_execution.json\n",
      "\n",
      "\n",
      "Processing mutation code_execution.mutations.m01...\n",
      "✅ code_execution.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/code_execution.json\n",
      "\n",
      "✅ blender Schema generation complete: clean_workspace/0.1.0/Schemas/blender.json\n",
      "\n",
      "\n",
      "Processing mutation blender.mutations.m01...\n",
      "✅ blender.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/blender.json\n",
      "\n",
      "✅ airline Schema generation complete: clean_workspace/0.1.0/Schemas/airline.json\n",
      "\n",
      "\n",
      "Processing mutation airline.mutations.m01...\n",
      "✅ airline.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/airline.json\n",
      "\n",
      "✅ instagram Schema generation complete: clean_workspace/0.1.0/Schemas/instagram.json\n",
      "\n",
      "\n",
      "Processing mutation instagram.mutations.m01...\n",
      "✅ instagram.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/instagram.json\n",
      "\n",
      "✅ generic_reminders Schema generation complete: clean_workspace/0.1.0/Schemas/generic_reminders.json\n",
      "\n",
      "\n",
      "Processing mutation generic_reminders.mutations.m01...\n",
      "✅ generic_reminders.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/generic_reminders.json\n",
      "\n",
      "✅ notifications Schema generation complete: clean_workspace/0.1.0/Schemas/notifications.json\n",
      "\n",
      "\n",
      "Processing mutation notifications.mutations.m01...\n",
      "✅ notifications.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/notifications.json\n",
      "\n",
      "✅ gmail Schema generation complete: clean_workspace/0.1.0/Schemas/gmail.json\n",
      "\n",
      "\n",
      "Processing mutation gmail.mutations.m01...\n",
      "✅ gmail.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/gmail.json\n",
      "\n",
      "✅ whatsapp Schema generation complete: clean_workspace/0.1.0/Schemas/whatsapp.json\n",
      "\n",
      "\n",
      "Processing mutation whatsapp.mutations.m01...\n",
      "✅ whatsapp.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/whatsapp.json\n",
      "\n",
      "✅ clock Schema generation complete: clean_workspace/0.1.0/Schemas/clock.json\n",
      "\n",
      "\n",
      "Processing mutation clock.mutations.m01...\n",
      "✅ clock.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/clock.json\n",
      "\n",
      "✅ linkedin Schema generation complete: clean_workspace/0.1.0/Schemas/linkedin.json\n",
      "\n",
      "\n",
      "Processing mutation linkedin.mutations.m01...\n",
      "✅ linkedin.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/linkedin.json\n",
      "\n",
      "✅ stripe Schema generation complete: clean_workspace/0.1.0/Schemas/stripe.json\n",
      "\n",
      "\n",
      "Processing mutation stripe.mutations.m01...\n",
      "✅ stripe.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/stripe.json\n",
      "\n",
      "✅ generic_tools Schema generation complete: clean_workspace/0.1.0/Schemas/generic_tools.json\n",
      "\n",
      "\n",
      "Processing mutation generic_tools.mutations.m01...\n",
      "✅ generic_tools.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/generic_tools.json\n",
      "\n",
      "✅ Successfully generated 67 FC Schemas to clean_workspace/0.1.0/Schemas\n",
      "Starting download of folder 1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml to clean_workspace/0.1.0/workspace/Datasets...\n",
      "Downloading file: ICEQueries_Files_md5Checksum_17_July_2025.csv to clean_workspace/0.1.0/workspace/Datasets/ICEQueries_Files_md5Checksum_17_July_2025.csv\n",
      "Downloading file: B Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/B Plan Budget.csv\n",
      "Downloading file: Operatorsexcavatio.csv to clean_workspace/0.1.0/workspace/Datasets/Operatorsexcavatio.csv\n",
      "Downloading file: libro_gastronomia.csv to clean_workspace/0.1.0/workspace/Datasets/libro_gastronomia.csv\n",
      "Downloading file: September-November 2021 Billing.csv to clean_workspace/0.1.0/workspace/Datasets/September-November 2021 Billing.csv\n",
      "Downloading file: NOVEMBER MP REPORT - axel bond.csv to clean_workspace/0.1.0/workspace/Datasets/NOVEMBER MP REPORT - axel bond.csv\n",
      "Downloading file: monte_sells.csv to clean_workspace/0.1.0/workspace/Datasets/monte_sells.csv\n",
      "Downloading file: PhenolicBoards.csv to clean_workspace/0.1.0/workspace/Datasets/PhenolicBoards.csv\n",
      "Downloading file: Company_finances_markup.csv to clean_workspace/0.1.0/workspace/Datasets/Company_finances_markup.csv\n",
      "Downloading file: prices_chapas.csv to clean_workspace/0.1.0/workspace/Datasets/prices_chapas.csv\n",
      "Downloading file: MARKAS S.A. BOX Sales.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Sales.csv\n",
      "Downloading file: Waiter_Hourly_Schedule.csv to clean_workspace/0.1.0/workspace/Datasets/Waiter_Hourly_Schedule.csv\n",
      "Downloading file: Untitled spreadsheet - Jill Norton.csv to clean_workspace/0.1.0/workspace/Datasets/Untitled spreadsheet - Jill Norton.csv\n",
      "Downloading file: Services - Dawn Alderson.csv to clean_workspace/0.1.0/workspace/Datasets/Services - Dawn Alderson.csv\n",
      "Downloading file: SC fancy customer survey Aug 2023_Nov 2023... - Aurore Munyeshyaka.csv to clean_workspace/0.1.0/workspace/Datasets/SC fancy customer survey Aug 2023_Nov 2023... - Aurore Munyeshyaka.csv\n",
      "Downloading file: base holii matterport - carla mccadden.csv to clean_workspace/0.1.0/workspace/Datasets/base holii matterport - carla mccadden.csv\n",
      "Downloading file: Vouchers_Dec_2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vouchers_Dec_2023.csv\n",
      "Downloading file: Book_Classes.csv to clean_workspace/0.1.0/workspace/Datasets/Book_Classes.csv\n",
      "Downloading file: ExpensesSushiRest.csv to clean_workspace/0.1.0/workspace/Datasets/ExpensesSushiRest.csv\n",
      "Downloading file: EMERGENCIA 2023 YAKU.csv to clean_workspace/0.1.0/workspace/Datasets/EMERGENCIA 2023 YAKU.csv\n",
      "Downloading file: prices_chapas.csv to clean_workspace/0.1.0/workspace/Datasets/prices_chapas.csv\n",
      "Downloading file: monte_sells.csv to clean_workspace/0.1.0/workspace/Datasets/monte_sells.csv\n",
      "Downloading file: cdlm_purchases.csv to clean_workspace/0.1.0/workspace/Datasets/cdlm_purchases.csv\n",
      "Downloading file: invoices_tracking.csv to clean_workspace/0.1.0/workspace/Datasets/invoices_tracking.csv\n",
      "Downloading file: ControlPanel.csv to clean_workspace/0.1.0/workspace/Datasets/ControlPanel.csv\n",
      "Downloading file: MARKAS S.A. BOX Sales.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Sales.csv\n",
      "Downloading file: W Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/W Plan Budget.csv\n",
      "Downloading file: trash_valet - Jordan Layne.csv to clean_workspace/0.1.0/workspace/Datasets/trash_valet - Jordan Layne.csv\n",
      "Downloading file: P Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/P Plan Budget.csv\n",
      "Downloading file: PhenolicBoards.csv to clean_workspace/0.1.0/workspace/Datasets/PhenolicBoards.csv\n",
      "Downloading file: Final_sales.csv to clean_workspace/0.1.0/workspace/Datasets/Final_sales.csv\n",
      "Downloading file: base holii matterport - carla mccadden.csv to clean_workspace/0.1.0/workspace/Datasets/base holii matterport - carla mccadden.csv\n",
      "Downloading file: Decoration_Items_List_Abril.csv to clean_workspace/0.1.0/workspace/Datasets/Decoration_Items_List_Abril.csv\n",
      "Downloading file: BestBatterEtsySales  - Renae Fultz.csv to clean_workspace/0.1.0/workspace/Datasets/BestBatterEtsySales  - Renae Fultz.csv\n",
      "Downloading file: Services - Dawn Alderson.csv to clean_workspace/0.1.0/workspace/Datasets/Services - Dawn Alderson.csv\n",
      "Downloading file: price_list_2023.csv to clean_workspace/0.1.0/workspace/Datasets/price_list_2023.csv\n",
      "Downloading file: Decoration_Items_List_Abril.csv to clean_workspace/0.1.0/workspace/Datasets/Decoration_Items_List_Abril.csv\n",
      "Downloading file: MARKAS S.A. BOX Sales.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Sales.csv\n",
      "Downloading file: Earnings Report 2023-01-01 to 2023-03-31 - Ryan Meza.csv to clean_workspace/0.1.0/workspace/Datasets/Earnings Report 2023-01-01 to 2023-03-31 - Ryan Meza.csv\n",
      "Downloading file: September-November 2021 Billing.csv to clean_workspace/0.1.0/workspace/Datasets/September-November 2021 Billing.csv\n",
      "Downloading file: FINANCIAL TRANSACTIONS.csv to clean_workspace/0.1.0/workspace/Datasets/FINANCIAL TRANSACTIONS.csv\n",
      "Downloading file: supermarket_stock_december.csv to clean_workspace/0.1.0/workspace/Datasets/supermarket_stock_december.csv\n",
      "Downloading file: Cards Movements.csv to clean_workspace/0.1.0/workspace/Datasets/Cards Movements.csv\n",
      "Downloading file: Untitled spreadsheet - Jill Norton.csv to clean_workspace/0.1.0/workspace/Datasets/Untitled spreadsheet - Jill Norton.csv\n",
      "Downloading file: supermarket_sales_tffb.csv to clean_workspace/0.1.0/workspace/Datasets/supermarket_sales_tffb.csv\n",
      "Downloading file: POPEYES_ANNUALIZED_PROFIT.csv to clean_workspace/0.1.0/workspace/Datasets/POPEYES_ANNUALIZED_PROFIT.csv\n",
      "Downloading file: price_list_2023.csv to clean_workspace/0.1.0/workspace/Datasets/price_list_2023.csv\n",
      "Downloading file: Vouchers_Dec_2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vouchers_Dec_2023.csv\n",
      "Downloading file: e_com.csv to clean_workspace/0.1.0/workspace/Datasets/e_com.csv\n",
      "Downloading file: envios_ventas_exterior.csv to clean_workspace/0.1.0/workspace/Datasets/envios_ventas_exterior.csv\n",
      "Downloading file: cash flow GapIng.csv to clean_workspace/0.1.0/workspace/Datasets/cash flow GapIng.csv\n",
      "Downloading file: cash flow GapIng.csv to clean_workspace/0.1.0/workspace/Datasets/cash flow GapIng.csv\n",
      "Creating subfolder and downloading: clean_workspace/0.1.0/workspace/Datasets/Whole set\n",
      "Downloading file: Quotewk.csv to clean_workspace/0.1.0/workspace/Datasets/Quotewk.csv\n",
      "Downloading file: ICEQueries_Files_md5Checksum.csv to clean_workspace/0.1.0/workspace/Datasets/ICEQueries_Files_md5Checksum.csv\n",
      "Downloading file: torque and flow performance.csv to clean_workspace/0.1.0/workspace/Datasets/torque and flow performance.csv\n",
      "Downloading file: eBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv to clean_workspace/0.1.0/workspace/Datasets/eBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv\n",
      "Downloading file: visits_2023-11-01 - Spencer Miller.csv to clean_workspace/0.1.0/workspace/Datasets/visits_2023-11-01 - Spencer Miller.csv\n",
      "Downloading file: CLIENTS AB USD_CLEAN.csv to clean_workspace/0.1.0/workspace/Datasets/CLIENTS AB USD_CLEAN.csv\n",
      "Downloading file: Sales_Report.csv to clean_workspace/0.1.0/workspace/Datasets/Sales_Report.csv\n",
      "Downloading file: Truck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv to clean_workspace/0.1.0/workspace/Datasets/Truck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv\n",
      "Downloading file: prodqualityanalysis.csv to clean_workspace/0.1.0/workspace/Datasets/prodqualityanalysis.csv\n",
      "Downloading file: GARDEN LOG - Visits (1) - Erica Redling.csv to clean_workspace/0.1.0/workspace/Datasets/GARDEN LOG - Visits (1) - Erica Redling.csv\n",
      "Downloading file: business users.csv to clean_workspace/0.1.0/workspace/Datasets/business users.csv\n",
      "Downloading file: Earnings And Placements - statement.csv - Jonathan Foster.csv to clean_workspace/0.1.0/workspace/Datasets/Earnings And Placements - statement.csv - Jonathan Foster.csv\n",
      "Downloading file: Clockify Detailed Time Report - 2023.csv to clean_workspace/0.1.0/workspace/Datasets/Clockify Detailed Time Report - 2023.csv\n",
      "Downloading file: Retrogasm sales 2023 1 - Rachel Robinson.csv to clean_workspace/0.1.0/workspace/Datasets/Retrogasm sales 2023 1 - Rachel Robinson.csv\n",
      "Downloading file: Construction_costs_delivery.csv to clean_workspace/0.1.0/workspace/Datasets/Construction_costs_delivery.csv\n",
      "Downloading file: Expenses_20152.csv to clean_workspace/0.1.0/workspace/Datasets/Expenses_20152.csv\n",
      "Downloading file: Vat_sales_ledger 02-2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vat_sales_ledger 02-2023.csv\n",
      "Downloading file: Salary and Equity Data.csv to clean_workspace/0.1.0/workspace/Datasets/Salary and Equity Data.csv\n",
      "Downloading file: 2020 sales - Shannon O.csv to clean_workspace/0.1.0/workspace/Datasets/2020 sales - Shannon O.csv\n",
      "Downloading file: Comparison with Wal Mart Catalog Stock.csv to clean_workspace/0.1.0/workspace/Datasets/Comparison with Wal Mart Catalog Stock.csv\n",
      "Downloading file: burbujas_sales_july.csv to clean_workspace/0.1.0/workspace/Datasets/burbujas_sales_july.csv\n",
      "Downloading file: Wood purchased - Patrick Bernier.csv to clean_workspace/0.1.0/workspace/Datasets/Wood purchased - Patrick Bernier.csv\n",
      "Downloading file: Dairy Solids (fat+protein).csv to clean_workspace/0.1.0/workspace/Datasets/Dairy Solids (fat+protein).csv\n",
      "Downloading file: Prices July 2022 EQUILIBRIO.csv to clean_workspace/0.1.0/workspace/Datasets/Prices July 2022 EQUILIBRIO.csv\n",
      "Downloading file: payroll_sheet.csv to clean_workspace/0.1.0/workspace/Datasets/payroll_sheet.csv\n",
      "Downloading file: [External] order_items.csv to clean_workspace/0.1.0/workspace/Datasets/[External] order_items.csv\n",
      "Downloading file: orders - orders - Kimm Topping.csv to clean_workspace/0.1.0/workspace/Datasets/orders - orders - Kimm Topping.csv\n",
      "Downloading file: 2_Debits_and_Credits_Purchases.csv to clean_workspace/0.1.0/workspace/Datasets/2_Debits_and_Credits_Purchases.csv\n",
      "Downloading file: Content Delivery Data Set - Jacob Goldberg.csv to clean_workspace/0.1.0/workspace/Datasets/Content Delivery Data Set - Jacob Goldberg.csv\n",
      "Downloading file: Purchase_invoice_graphics_company.csv to clean_workspace/0.1.0/workspace/Datasets/Purchase_invoice_graphics_company.csv\n",
      "Downloading file: Accounting - Jake Chase.csv to clean_workspace/0.1.0/workspace/Datasets/Accounting - Jake Chase.csv\n",
      "Downloading file: Higher Ed Threads 2023 - Phil Nance.csv to clean_workspace/0.1.0/workspace/Datasets/Higher Ed Threads 2023 - Phil Nance.csv\n",
      "Downloading file: Cars_Sales_22-23.csv to clean_workspace/0.1.0/workspace/Datasets/Cars_Sales_22-23.csv\n",
      "Downloading file: COMAFI AND SUPER HISTORY.csv to clean_workspace/0.1.0/workspace/Datasets/COMAFI AND SUPER HISTORY.csv\n",
      "Downloading file: Survey Meters.csv to clean_workspace/0.1.0/workspace/Datasets/Survey Meters.csv\n",
      "Downloading file: Juana Events Rentals.csv to clean_workspace/0.1.0/workspace/Datasets/Juana Events Rentals.csv\n",
      "Downloading file: 2201_VoucherCheker.csv to clean_workspace/0.1.0/workspace/Datasets/2201_VoucherCheker.csv\n",
      "Downloading file: Blueprint_budget.csv to clean_workspace/0.1.0/workspace/Datasets/Blueprint_budget.csv\n",
      "Downloading file: Storage (version 1).csv - Sheet1 - Erica Redling.csv to clean_workspace/0.1.0/workspace/Datasets/Storage (version 1).csv - Sheet1 - Erica Redling.csv\n",
      "Downloading file: Productivity2.csv to clean_workspace/0.1.0/workspace/Datasets/Productivity2.csv\n",
      "Downloading file: Woodworking Class Data - Patrick Bernier.csv to clean_workspace/0.1.0/workspace/Datasets/Woodworking Class Data - Patrick Bernier.csv\n",
      "Downloading file: March Sales.csv to clean_workspace/0.1.0/workspace/Datasets/March Sales.csv\n",
      "Downloading file: Survey - Videogames - Brazil 2015 - Alexandre Papanis.csv to clean_workspace/0.1.0/workspace/Datasets/Survey - Videogames - Brazil 2015 - Alexandre Papanis.csv\n",
      "Downloading file: Wine Inventory - Andrew Nelson.csv to clean_workspace/0.1.0/workspace/Datasets/Wine Inventory - Andrew Nelson.csv\n",
      "Downloading file: Toggl Detailed Time Report - 2022.csv to clean_workspace/0.1.0/workspace/Datasets/Toggl Detailed Time Report - 2022.csv\n",
      "Downloading file: IllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv to clean_workspace/0.1.0/workspace/Datasets/IllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv\n",
      "Downloading file: Dataset for Solar Panel Cleaning - Nestor Sanchez.csv to clean_workspace/0.1.0/workspace/Datasets/Dataset for Solar Panel Cleaning - Nestor Sanchez.csv\n",
      "Downloading file: 2022 Districts Monthly Transfer Dataset.csv to clean_workspace/0.1.0/workspace/Datasets/2022 Districts Monthly Transfer Dataset.csv\n",
      "Downloading file: balance_sheet.csv to clean_workspace/0.1.0/workspace/Datasets/balance_sheet.csv\n",
      "Downloading file: Parcels_2023Q1_CO - Guillermo Pardon.csv to clean_workspace/0.1.0/workspace/Datasets/Parcels_2023Q1_CO - Guillermo Pardon.csv\n",
      "Downloading file: OOC-55 - William Webster (SickBoy).csv to clean_workspace/0.1.0/workspace/Datasets/OOC-55 - William Webster (SickBoy).csv\n",
      "Downloading file: Materials_CTM.csv to clean_workspace/0.1.0/workspace/Datasets/Materials_CTM.csv\n",
      "Downloading file: cash_flow_Bakery.csv to clean_workspace/0.1.0/workspace/Datasets/cash_flow_Bakery.csv\n",
      "Downloading file: chicken_groceries_prices.csv to clean_workspace/0.1.0/workspace/Datasets/chicken_groceries_prices.csv\n",
      "Downloading file: Data Set Remotasks - Anita Portnoy.csv to clean_workspace/0.1.0/workspace/Datasets/Data Set Remotasks - Anita Portnoy.csv\n",
      "Downloading file: financial_credits.csv to clean_workspace/0.1.0/workspace/Datasets/financial_credits.csv\n",
      "Downloading file: Cristine's Corner January - Cristine Marquez.csv to clean_workspace/0.1.0/workspace/Datasets/Cristine's Corner January - Cristine Marquez.csv\n",
      "Downloading file: Tecno_MP_activities-feeder_collection-20231206183310-efa71.csv to clean_workspace/0.1.0/workspace/Datasets/Tecno_MP_activities-feeder_collection-20231206183310-efa71.csv\n",
      "Downloading file: airbnb_tax_01_2023-01_2024 - Philip Ferraro.csv to clean_workspace/0.1.0/workspace/Datasets/airbnb_tax_01_2023-01_2024 - Philip Ferraro.csv\n",
      "Downloading file: Log_2016_2020_Redacted - Adam Johnson.csv to clean_workspace/0.1.0/workspace/Datasets/Log_2016_2020_Redacted - Adam Johnson.csv\n",
      "Downloading file: U0089_Clients.csv to clean_workspace/0.1.0/workspace/Datasets/U0089_Clients.csv\n",
      "Downloading file: MarAug2023_ProjInst_Dataset - Diana.csv to clean_workspace/0.1.0/workspace/Datasets/MarAug2023_ProjInst_Dataset - Diana.csv\n",
      "Downloading file: Vat_purchases_journal_02-2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vat_purchases_journal_02-2023.csv\n",
      "Downloading file: ServerProcesses.csv to clean_workspace/0.1.0/workspace/Datasets/ServerProcesses.csv\n",
      "Downloading file: Bold21 Data set - orders_export_1.csv to clean_workspace/0.1.0/workspace/Datasets/Bold21 Data set - orders_export_1.csv\n",
      "Downloading file: Payroll_oct_16-31.csv to clean_workspace/0.1.0/workspace/Datasets/Payroll_oct_16-31.csv\n",
      "Downloading file: Expense_summary.csv to clean_workspace/0.1.0/workspace/Datasets/Expense_summary.csv\n",
      "Downloading file: KookyArtsAi DATABASE - Christy Rivers.csv to clean_workspace/0.1.0/workspace/Datasets/KookyArtsAi DATABASE - Christy Rivers.csv\n",
      "Downloading file: Sales_RLC2.csv to clean_workspace/0.1.0/workspace/Datasets/Sales_RLC2.csv\n",
      "Downloading file: Fragrance June.csv to clean_workspace/0.1.0/workspace/Datasets/Fragrance June.csv\n",
      "Downloading file: Vector360-TravelResponse - Ernesto Herrero.csv to clean_workspace/0.1.0/workspace/Datasets/Vector360-TravelResponse - Ernesto Herrero.csv\n",
      "Downloading file: sales.csv to clean_workspace/0.1.0/workspace/Datasets/sales.csv\n",
      "Downloading file: Paper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv to clean_workspace/0.1.0/workspace/Datasets/Paper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv\n",
      "Downloading file: EtsySoldOrders2021_2023 - Sara Gerges.csv to clean_workspace/0.1.0/workspace/Datasets/EtsySoldOrders2021_2023 - Sara Gerges.csv\n",
      "Downloading file: Student supplies delivery costs Fall 2023 - Robert Russell.csv to clean_workspace/0.1.0/workspace/Datasets/Student supplies delivery costs Fall 2023 - Robert Russell.csv\n",
      "Downloading file: KiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv to clean_workspace/0.1.0/workspace/Datasets/KiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv\n",
      "Downloading file: BurguerhouseJuly21.csv to clean_workspace/0.1.0/workspace/Datasets/BurguerhouseJuly21.csv\n",
      "Downloading file: mill_operations.csv to clean_workspace/0.1.0/workspace/Datasets/mill_operations.csv\n",
      "Downloading file: Store Survey - Patrick Bernier.csv to clean_workspace/0.1.0/workspace/Datasets/Store Survey - Patrick Bernier.csv\n",
      "Downloading file: Monthly Cash Flow.csv to clean_workspace/0.1.0/workspace/Datasets/Monthly Cash Flow.csv\n",
      "Downloading file: supplies.csv to clean_workspace/0.1.0/workspace/Datasets/supplies.csv\n",
      "Downloading file: public_lighting.csv to clean_workspace/0.1.0/workspace/Datasets/public_lighting.csv\n",
      "Downloading file: rtshare - Kris Best.csv to clean_workspace/0.1.0/workspace/Datasets/rtshare - Kris Best.csv\n",
      "Downloading file: First School Term 2023_2024 - Robert Russell.csv to clean_workspace/0.1.0/workspace/Datasets/First School Term 2023_2024 - Robert Russell.csv\n",
      "Downloading file: order.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv to clean_workspace/0.1.0/workspace/Datasets/order.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv\n",
      "Downloading file: R Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/R Plan Budget.csv\n",
      "Downloading file: Report_periodical_records.csv to clean_workspace/0.1.0/workspace/Datasets/Report_periodical_records.csv\n",
      "Downloading file: Submission id - Miss Kaye Estacio.csv to clean_workspace/0.1.0/workspace/Datasets/Submission id - Miss Kaye Estacio.csv\n",
      "Downloading file: Product Dims.csv to clean_workspace/0.1.0/workspace/Datasets/Product Dims.csv\n",
      "Downloading file: SalesFiguresQ1Q2 - Travis Colahan.csv to clean_workspace/0.1.0/workspace/Datasets/SalesFiguresQ1Q2 - Travis Colahan.csv\n",
      "Downloading file: Campaigns BIMO.csv to clean_workspace/0.1.0/workspace/Datasets/Campaigns BIMO.csv\n",
      "Downloading file: prices_ss.csv to clean_workspace/0.1.0/workspace/Datasets/prices_ss.csv\n",
      "Downloading file: ABM_WAGES.csv to clean_workspace/0.1.0/workspace/Datasets/ABM_WAGES.csv\n",
      "Downloading file: Bill_A2.csv to clean_workspace/0.1.0/workspace/Datasets/Bill_A2.csv\n",
      "Downloading file: HEALTHCARE AFFILIATE MANAGEMENT.csv to clean_workspace/0.1.0/workspace/Datasets/HEALTHCARE AFFILIATE MANAGEMENT.csv\n",
      "Downloading file: Gwynstone.Originz.dat - Gwynne Rife.csv to clean_workspace/0.1.0/workspace/Datasets/Gwynstone.Originz.dat - Gwynne Rife.csv\n",
      "Downloading file: Purchase_orders_GAP_ZEN.csv to clean_workspace/0.1.0/workspace/Datasets/Purchase_orders_GAP_ZEN.csv\n",
      "Downloading file: data-export (3) - Harsh Bakshi.csv to clean_workspace/0.1.0/workspace/Datasets/data-export (3) - Harsh Bakshi.csv\n",
      "Downloading file: Waiters Cash_Sales.csv to clean_workspace/0.1.0/workspace/Datasets/Waiters Cash_Sales.csv\n",
      "Downloading file: Mama Milk Flow Data - paul miles.csv to clean_workspace/0.1.0/workspace/Datasets/Mama Milk Flow Data - paul miles.csv\n",
      "Downloading file: JAZ Ceramics.csv to clean_workspace/0.1.0/workspace/Datasets/JAZ Ceramics.csv\n",
      "Downloading file: AccumulatorReadingsReport.csv to clean_workspace/0.1.0/workspace/Datasets/AccumulatorReadingsReport.csv\n",
      "Downloading file: Checks.csv to clean_workspace/0.1.0/workspace/Datasets/Checks.csv\n",
      "Downloading file: Bank_accreditations.csv to clean_workspace/0.1.0/workspace/Datasets/Bank_accreditations.csv\n",
      "Downloading file: vaccinations_city_flores.csv to clean_workspace/0.1.0/workspace/Datasets/vaccinations_city_flores.csv\n",
      "Downloading file: Sheet1 - Brooks Chiro Rehab.csv to clean_workspace/0.1.0/workspace/Datasets/Sheet1 - Brooks Chiro Rehab.csv\n",
      "Downloading file: Values Survey - June 2023.csv to clean_workspace/0.1.0/workspace/Datasets/Values Survey - June 2023.csv\n",
      "Downloading file: ChemicalProductsApplications.csv to clean_workspace/0.1.0/workspace/Datasets/ChemicalProductsApplications.csv\n",
      "Downloading file: BurguerhouseJuly21csv.csv to clean_workspace/0.1.0/workspace/Datasets/BurguerhouseJuly21csv.csv\n",
      "Downloading file: TD_HET Shipping Report - Phil Nance.csv to clean_workspace/0.1.0/workspace/Datasets/TD_HET Shipping Report - Phil Nance.csv\n",
      "Downloading file: shop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv to clean_workspace/0.1.0/workspace/Datasets/shop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv\n",
      "Downloading file: endowment_gm.csv to clean_workspace/0.1.0/workspace/Datasets/endowment_gm.csv\n",
      "Downloading file: Transaction List - Dawn Alderson.csv to clean_workspace/0.1.0/workspace/Datasets/Transaction List - Dawn Alderson.csv\n",
      "Downloading file: high_portability_local_cellphone.csv to clean_workspace/0.1.0/workspace/Datasets/high_portability_local_cellphone.csv\n",
      "Downloading file: Technical_services.csv to clean_workspace/0.1.0/workspace/Datasets/Technical_services.csv\n",
      "Downloading file: MARKAS S.A. BOX Purchases.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Purchases.csv\n",
      "Downloading file: 2023-12-21_transactions_export - Bailey Talley.csv to clean_workspace/0.1.0/workspace/Datasets/2023-12-21_transactions_export - Bailey Talley.csv\n",
      "Downloading file: OCT-QA-Report-2023 - Marlene Portillo.csv to clean_workspace/0.1.0/workspace/Datasets/OCT-QA-Report-2023 - Marlene Portillo.csv\n",
      "Downloading file: Bookshop Sales and Inventory Dataset.csv to clean_workspace/0.1.0/workspace/Datasets/Bookshop Sales and Inventory Dataset.csv\n",
      "Downloading file: november_supermarket_sales_per_product.csv to clean_workspace/0.1.0/workspace/Datasets/november_supermarket_sales_per_product.csv\n",
      "Downloading file: Art Gallery Spending Log 2017 - Erica Redling.csv to clean_workspace/0.1.0/workspace/Datasets/Art Gallery Spending Log 2017 - Erica Redling.csv\n",
      "Downloading file: BranchTransac.csv to clean_workspace/0.1.0/workspace/Datasets/BranchTransac.csv\n",
      "Downloading file: Sparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv to clean_workspace/0.1.0/workspace/Datasets/Sparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv\n",
      "Downloading file: Cegin list of shifts of the month.csv to clean_workspace/0.1.0/workspace/Datasets/Cegin list of shifts of the month.csv\n",
      "Downloading file: Purchase_RLC.csv to clean_workspace/0.1.0/workspace/Datasets/Purchase_RLC.csv\n",
      "Downloading file: 2023 Inventory - Elisabeth Gracyalny.csv to clean_workspace/0.1.0/workspace/Datasets/2023 Inventory - Elisabeth Gracyalny.csv\n",
      "Downloading file: production_costs_Bakery.csv to clean_workspace/0.1.0/workspace/Datasets/production_costs_Bakery.csv\n",
      "Downloading file: metzeler_tires.csv to clean_workspace/0.1.0/workspace/Datasets/metzeler_tires.csv\n",
      "Downloading file: E Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/E Plan Budget.csv\n",
      "Downloading file: Sales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv to clean_workspace/0.1.0/workspace/Datasets/Sales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv\n",
      "Downloading file: Sale_Detail.csv to clean_workspace/0.1.0/workspace/Datasets/Sale_Detail.csv\n",
      "Downloading file: Expenses_2015.csv to clean_workspace/0.1.0/workspace/Datasets/Expenses_2015.csv\n",
      "Downloading file: online_retail.csv to clean_workspace/0.1.0/workspace/Datasets/online_retail.csv\n",
      "Downloading file: Commercial_Inventory.csv to clean_workspace/0.1.0/workspace/Datasets/Commercial_Inventory.csv\n",
      "Downloading file: sales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv to clean_workspace/0.1.0/workspace/Datasets/sales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv\n",
      "Downloading file: Dairy 0051-01.csv to clean_workspace/0.1.0/workspace/Datasets/Dairy 0051-01.csv\n",
      "Downloading file: Champro SKU Specs.csv to clean_workspace/0.1.0/workspace/Datasets/Champro SKU Specs.csv\n",
      "Downloading file: Info_FTTH.xlsx - PPP_INFO.csv to clean_workspace/0.1.0/workspace/Datasets/Info_FTTH.xlsx - PPP_INFO.csv\n",
      "Downloading file: Delivery_Sales Report.csv to clean_workspace/0.1.0/workspace/Datasets/Delivery_Sales Report.csv\n",
      "Downloading file: Tecno_sales_2018-12-06_2023-12-06.csv to clean_workspace/0.1.0/workspace/Datasets/Tecno_sales_2018-12-06_2023-12-06.csv\n",
      "Downloading file: Children's White Sales.csv to clean_workspace/0.1.0/workspace/Datasets/Children's White Sales.csv\n",
      "Downloading file: data - Kayla Banger.csv to clean_workspace/0.1.0/workspace/Datasets/data - Kayla Banger.csv\n",
      "Downloading file: Stamping_Dataset.csv to clean_workspace/0.1.0/workspace/Datasets/Stamping_Dataset.csv\n",
      "Downloading file: LSC _ 2022_sales_activity_report - Casey Montante.csv to clean_workspace/0.1.0/workspace/Datasets/LSC _ 2022_sales_activity_report - Casey Montante.csv\n",
      "Downloading file: BUILDING EXPENSES.csv to clean_workspace/0.1.0/workspace/Datasets/BUILDING EXPENSES.csv\n",
      "Downloading file: BusinessReport-7-28-22 - Travis Colahan.csv to clean_workspace/0.1.0/workspace/Datasets/BusinessReport-7-28-22 - Travis Colahan.csv\n",
      "Downloading file: Bybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv to clean_workspace/0.1.0/workspace/Datasets/Bybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv\n",
      "Downloading file: Untitled spreadsheet - Melissa Jaycox.csv to clean_workspace/0.1.0/workspace/Datasets/Untitled spreadsheet - Melissa Jaycox.csv\n",
      "Downloading file: VF Sample Dataset - rafael guzman.csv to clean_workspace/0.1.0/workspace/Datasets/VF Sample Dataset - rafael guzman.csv\n",
      "Downloading file: SubSystemIO.csv to clean_workspace/0.1.0/workspace/Datasets/SubSystemIO.csv\n",
      "Downloading file: businesspayrolls.csv to clean_workspace/0.1.0/workspace/Datasets/businesspayrolls.csv\n",
      "Downloading file: C1_dbjg.csv to clean_workspace/0.1.0/workspace/Datasets/C1_dbjg.csv\n",
      "Dataset download complete.\n",
      "Downloading WS Dataset zip file with ID: 1kmXZ1oarBPlE0OQL52eGoc1xPbupJ1n9...\n",
      "Download progress: 100%\n",
      "Dataset download complete.\n",
      "Extracted to clean_workspace/0.1.0\n",
      "Moved clean_workspace/0.1.0/WS_DATA/file_dataset_pb2.py to clean_workspace/0.1.0/file_dataset_pb2.py\n"
     ]
    }
   ],
   "source": [
    "download_apis(download_datasets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch & Download Colabs / Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98 entries, 0 to 97\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sample_id  98 non-null     object\n",
      " 1   colab_url  98 non-null     object\n",
      " 2   status     98 non-null     object\n",
      " 3   colab_id   98 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet_id = \"1iWhXc_9PZZ_5pWZF3RMK7npdGzMkJL5rGbYv6FICENk\"\n",
    "data_tab = \"auto_qc_data\"\n",
    "\n",
    "colabs_df = GoogleSheet.get_sheet_data(sheet_id, data_tab)\n",
    "\n",
    "\n",
    "# Any filtering\n",
    "# filter_col = 'status'\n",
    "# filter_val = 'Check Not Executed'\n",
    "\n",
    "# colabs_df = colabs_df[colabs_df[filter_col]==filter_val]\n",
    "# colabs_df.info()\n",
    "\n",
    "colabs_df['colab_id'] = colabs_df['colab_url'].apply(GoogleService.extract_file_id)\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98 entries, 0 to 97\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   colab_id    98 non-null     object\n",
      " 1   colab_name  98 non-null     object\n",
      " 2   sample_id   98 non-null     object\n",
      " 3   colab_url   98 non-null     object\n",
      " 4   status      98 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "colab_names = []\n",
    "name_request_batch_size = 99\n",
    "for start in range(0, len(colabs_df['colab_id']), name_request_batch_size):\n",
    "    colab_names += GoogleDrive.get_file_names_in_batch(colabs_df['colab_id'].tolist()[start:start+name_request_batch_size])\n",
    "colab_name_df = pd.DataFrame(colab_names)\n",
    "colab_name_df = colab_name_df[~colab_name_df['colab_name'].isna()]\n",
    "colabs_df = pd.merge(colab_name_df, colabs_df, on='colab_id')\n",
    "colabs_df = colabs_df.drop_duplicates(['colab_id'])\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrate Notebook AutoRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 30\n",
      "Max Samples Per Batch: 4\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(colabs_df)\n",
    "max_container = 30\n",
    "max_batch_size = math.ceil(total_samples / max_container)\n",
    "print(f'Total Batches: {max_container}\\nMax Samples Per Batch: {max_batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_id\n",
       "0.1.0_0     4\n",
       "0.1.0_13    4\n",
       "0.1.0_23    4\n",
       "0.1.0_22    4\n",
       "0.1.0_21    4\n",
       "0.1.0_20    4\n",
       "0.1.0_19    4\n",
       "0.1.0_18    4\n",
       "0.1.0_17    4\n",
       "0.1.0_16    4\n",
       "0.1.0_15    4\n",
       "0.1.0_14    4\n",
       "0.1.0_12    4\n",
       "0.1.0_1     4\n",
       "0.1.0_11    4\n",
       "0.1.0_10    4\n",
       "0.1.0_9     4\n",
       "0.1.0_8     4\n",
       "0.1.0_7     4\n",
       "0.1.0_6     4\n",
       "0.1.0_5     4\n",
       "0.1.0_4     4\n",
       "0.1.0_3     4\n",
       "0.1.0_2     4\n",
       "0.1.0_24    2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_version = '0.1.0'\n",
    "notebooks = [{'path': notebook, 'api_version': api_version} for notebook in colabs_df['colab_id'].tolist()]\n",
    "notebooks_df = pd.DataFrame(notebooks)\n",
    "for idx, api_version in enumerate(set(notebooks_df['api_version'])):\n",
    "    count_notebooks = len(notebooks_df[notebooks_df['api_version']==api_version])\n",
    "    batches = []\n",
    "    for idx in range(count_notebooks):\n",
    "        batches.append(idx//max_batch_size)\n",
    "    batch_ids = [f\"{api_version}_{batch}\" for batch in batches]\n",
    "    notebooks_df.loc[notebooks_df['api_version'] == api_version, 'batch_id'] = batch_ids\n",
    "notebooks_df = pd.merge(notebooks_df, colabs_df, left_on='path', right_on='colab_id')\n",
    "run_identifiers = list(set(notebooks_df['batch_id']))\n",
    "notebooks_df['batch_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_df.to_csv('execution_configs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Validating Host Environment ---\n",
      "✅ Docker client connected.\n",
      "\n",
      "--- Step 2: Preparing Host Directories ---\n",
      "✅ Created log directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/execution_logs/sanity_check_20250814_184009\n",
      "✅ Created result directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/results/sanity_check_20250814_184009\n",
      "\n",
      "--- Step 4: Launching Containers in Parallel ---\n",
      "  -> Launching container 'sanity_check_20250814_184009-0' for batch 0...\n",
      "  -> Launching container 'sanity_check_20250814_184009-1' for batch 1...\n",
      "  -> Launching container 'sanity_check_20250814_184009-2' for batch 2...\n",
      "  -> Launching container 'sanity_check_20250814_184009-3' for batch 3...\n",
      "  -> Launching container 'sanity_check_20250814_184009-4' for batch 4...\n",
      "  -> Launching container 'sanity_check_20250814_184009-5' for batch 5...\n",
      "  -> Launching container 'sanity_check_20250814_184009-6' for batch 6...\n",
      "  -> Launching container 'sanity_check_20250814_184009-7' for batch 7...\n",
      "  -> Launching container 'sanity_check_20250814_184009-8' for batch 8...\n",
      "  -> Launching container 'sanity_check_20250814_184009-9' for batch 9...\n",
      "  -> Launching container 'sanity_check_20250814_184009-10' for batch 10...\n",
      "  -> Launching container 'sanity_check_20250814_184009-11' for batch 11...\n",
      "  -> Launching container 'sanity_check_20250814_184009-12' for batch 12...\n",
      "  -> Launching container 'sanity_check_20250814_184009-13' for batch 13...\n",
      "  -> Launching container 'sanity_check_20250814_184009-14' for batch 14...\n",
      "  -> Launching container 'sanity_check_20250814_184009-15' for batch 15...\n",
      "  -> Launching container 'sanity_check_20250814_184009-16' for batch 16...\n",
      "  -> Launching container 'sanity_check_20250814_184009-17' for batch 17...\n",
      "  -> Launching container 'sanity_check_20250814_184009-18' for batch 18...\n",
      "  -> Launching container 'sanity_check_20250814_184009-19' for batch 19...\n",
      "  -> Launching container 'sanity_check_20250814_184009-20' for batch 20...\n",
      "  -> Launching container 'sanity_check_20250814_184009-21' for batch 21...\n",
      "  -> Launching container 'sanity_check_20250814_184009-22' for batch 22...\n",
      "  -> Launching container 'sanity_check_20250814_184009-23' for batch 23...\n",
      "  -> Launching container 'sanity_check_20250814_184009-24' for batch 24...\n",
      "\n",
      "--- Step 5: Waiting for All Containers to Finish ---\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-0' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-1' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-2' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-3' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-4' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-5' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-6' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-7' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-8' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-9' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-10' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-11' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-12' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-13' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-14' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-15' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-16' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-17' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-18' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-19' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-20' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-21' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-22' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-23' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250814_184009-24' finished with exit code 0.\n",
      "\n",
      "--- Orchestration Complete ---\n",
      "📄 All results and logs are stored in: /Users/nabeel/PycharmProjects/e2e_sanity_checks/execution_logs/sanity_check_20250814_184009\n",
      "Finished Docker Run. Time Taken: 3858 Seconds\n"
     ]
    }
   ],
   "source": [
    "import sanity_orchestrator_with_download as orchestrator\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    run_name = f'sanity_check_{start_time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    orchestrator.run_orchestration(run_name, run_identifiers)\n",
    "    print(f\"Finished Docker Run. Time Taken: {(datetime.now()-start_time).seconds} Seconds\")\n",
    "except (FileNotFoundError, FileExistsError, ConnectionError) as e:\n",
    "    print(f\"\\n❌ A critical error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98 entries, 0 to 97\n",
      "Data columns (total 6 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   notebook                    98 non-null     object\n",
      " 1   no_action_script_success    98 non-null     bool  \n",
      " 2   no_action_response          98 non-null     object\n",
      " 3   with_action_script_success  98 non-null     bool  \n",
      " 4   with_action_response        98 non-null     object\n",
      " 5   golden_answer_sample        98 non-null     bool  \n",
      "dtypes: bool(3), object(3)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'results/{run_name}'\n",
    "output_files = os.listdir(output_dir)\n",
    "complete_data = []\n",
    "for file in output_files:\n",
    "    full_path = Path(output_dir) / file\n",
    "    with open(full_path, 'r') as f:\n",
    "        complete_data += json.load(f)['result']\n",
    "# Use json_normalize to flatten the data\n",
    "sanity_df = pd.json_normalize(complete_data)\n",
    "sanity_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>golden_answer_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1S0nAZwpVeGMfacGzTtgVNrq4mIf5cO_3</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1PMEIDpHbiBObA5HKNv4mGutdvwsOvEH4</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Action\\nError Type: AttributeError\\nE...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11RqI1NxFu8In93ZkYdkAN7XTTIbKtjmc</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1LzXaiOBUFfBPvR5JkVAppZ2jGx2qROqQ</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Zx746b4UBmDFJSh44w7dv0_dXBWozB3u</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            notebook  no_action_script_success  \\\n",
       "0  1S0nAZwpVeGMfacGzTtgVNrq4mIf5cO_3                      True   \n",
       "1  1PMEIDpHbiBObA5HKNv4mGutdvwsOvEH4                      True   \n",
       "2  11RqI1NxFu8In93ZkYdkAN7XTTIbKtjmc                      True   \n",
       "3  1LzXaiOBUFfBPvR5JkVAppZ2jGx2qROqQ                      True   \n",
       "4  1Zx746b4UBmDFJSh44w7dv0_dXBWozB3u                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "1  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "2  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "3  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "4  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  golden_answer_sample  \n",
       "0  Block: # Final Assertion\\nError Type: Assertio...                 False  \n",
       "1  Block: # Action\\nError Type: AttributeError\\nE...                 False  \n",
       "2  Block: # Final Assertion\\nError Type: Assertio...                 False  \n",
       "3                                                                    False  \n",
       "4                                                                    False  "
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>colab_url</th>\n",
       "      <th>colab_name</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>golden_answer_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>244_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/1dTNOU...</td>\n",
       "      <td>Agent-244_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/14JsFS...</td>\n",
       "      <td>Agent-129_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/1-CVMo...</td>\n",
       "      <td>Agent-276_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>526_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/13YN1G...</td>\n",
       "      <td>Agent-526_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>362_base_MT</td>\n",
       "      <td>https://drive.google.com/file/d/1d5k_QRMRJyFSD...</td>\n",
       "      <td>Agent-362_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_id                                          colab_url  \\\n",
       "0  244_base_MT  https://colab.research.google.com/drive/1dTNOU...   \n",
       "1  129_base_MT  https://colab.research.google.com/drive/14JsFS...   \n",
       "2  276_base_MT  https://colab.research.google.com/drive/1-CVMo...   \n",
       "3  526_base_MT  https://colab.research.google.com/drive/13YN1G...   \n",
       "4  362_base_MT  https://drive.google.com/file/d/1d5k_QRMRJyFSD...   \n",
       "\n",
       "                           colab_name  no_action_script_success  \\\n",
       "0  Agent-244_base_MT-Simulation.ipynb                      True   \n",
       "1  Agent-129_base_MT-Simulation.ipynb                      True   \n",
       "2  Agent-276_base_MT-Simulation.ipynb                      True   \n",
       "3  Agent-526_base_MT-Simulation.ipynb                      True   \n",
       "4  Agent-362_base_MT-Simulation.ipynb                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "1  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "2  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "3  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "4  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "\n",
       "   with_action_script_success with_action_response  golden_answer_sample  \n",
       "0                        True                                      False  \n",
       "1                        True                                      False  \n",
       "2                        True                                      False  \n",
       "3                        True                                      False  \n",
       "4                        True                                      False  "
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_set = [\n",
    "    'sample_id', \n",
    "    'colab_url', \n",
    "    'colab_name', \n",
    "    'no_action_script_success',\n",
    "    'no_action_response',\n",
    "    'with_action_script_success',\n",
    "    'with_action_response',\n",
    "    'golden_answer_sample',\n",
    "    ]\n",
    "merged_df = pd.merge(colabs_df, sanity_df, left_on='colab_id', right_on='notebook')[columns_set]\n",
    "merged_df = merged_df.fillna(\"\")\n",
    "for col in ['with_action_response', 'no_action_response']:\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: x[:49999])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98 entries, 0 to 97\n",
      "Data columns (total 8 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   sample_id                   98 non-null     object\n",
      " 1   colab_url                   98 non-null     object\n",
      " 2   colab_name                  98 non-null     object\n",
      " 3   no_action_script_success    98 non-null     bool  \n",
      " 4   no_action_response          98 non-null     object\n",
      " 5   with_action_script_success  98 non-null     bool  \n",
      " 6   with_action_response        98 non-null     object\n",
      " 7   golden_answer_sample        98 non-null     bool  \n",
      "dtypes: bool(3), object(5)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_FAILED_ASSERTION = 'FA Failed - Assertion Error'\n",
    "IA_FAILED_ASSERTION = 'IA Failed - Assertion Error'\n",
    "NON_ASSERTION_ERROR = 'Non Assertion Error'\n",
    "NO_ERROR_FOUND = 'No Error Found'\n",
    "UNDEFINED_ERROR = 'Undefined Error Type'\n",
    "\n",
    "NEEDS_FIXES = 'Needs Fixes'\n",
    "GOOD_TO_GO = 'Good To Go'\n",
    "NEEDS_MANUAL_REVIEW = 'Needs Manual Review'\n",
    "CHECK_NOT_EXECUTED = 'Check Not Executed'\n",
    "\n",
    "def add_error_type(error_message):\n",
    "    if error_message == \"\":\n",
    "        return NO_ERROR_FOUND\n",
    "    block = error_message.split('\\n')[0].split(':')[-1].strip()\n",
    "    error_type = error_message.split('\\n')[1].split(':')[-1].strip()\n",
    "    initial_assertion_header = 'Initial Assertion'\n",
    "    final_assertion_header = 'Final Assertion'\n",
    "    # Non Assertion Error\n",
    "    if error_type != 'AssertionError':\n",
    "        return NON_ASSERTION_ERROR\n",
    "    if error_type == 'AssertionError':\n",
    "        if initial_assertion_header.lower() in block.lower():\n",
    "            return IA_FAILED_ASSERTION\n",
    "        if final_assertion_header.lower() in block.lower():\n",
    "            return FA_FAILED_ASSERTION\n",
    "    return UNDEFINED_ERROR\n",
    "\n",
    "def get_auto_qc_status(status_w_action, status_wo_action):\n",
    "    if NON_ASSERTION_ERROR in [status_w_action, status_wo_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    if IA_FAILED_ASSERTION in [status_w_action, status_wo_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    if FA_FAILED_ASSERTION in [status_w_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    \n",
    "    if status_w_action == NO_ERROR_FOUND:\n",
    "        if status_wo_action == NO_ERROR_FOUND:\n",
    "            return NEEDS_MANUAL_REVIEW\n",
    "        \n",
    "        if status_wo_action == FA_FAILED_ASSERTION:\n",
    "            return GOOD_TO_GO\n",
    "    print(status_wo_action, status_w_action)\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Auto QC Status\n",
       "Good To Go             71\n",
       "Needs Fixes            25\n",
       "Needs Manual Review     2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Execution Status w/o Action'] = merged_df['no_action_response'].apply(add_error_type)\n",
    "merged_df['Execution Status w Action'] = merged_df['with_action_response'].apply(add_error_type)\n",
    "merged_df['Auto QC Status'] = merged_df.apply(lambda row: get_auto_qc_status(row['Execution Status w Action'], row['Execution Status w/o Action']), axis=1)\n",
    "merged_df['Auto QC Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>colab_url</th>\n",
       "      <th>colab_name</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>golden_answer_sample</th>\n",
       "      <th>Execution Status w/o Action</th>\n",
       "      <th>Execution Status w Action</th>\n",
       "      <th>Auto QC Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>244_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/1dTNOU...</td>\n",
       "      <td>Agent-244_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/14JsFS...</td>\n",
       "      <td>Agent-129_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/1-CVMo...</td>\n",
       "      <td>Agent-276_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>526_base_MT</td>\n",
       "      <td>https://colab.research.google.com/drive/13YN1G...</td>\n",
       "      <td>Agent-526_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>362_base_MT</td>\n",
       "      <td>https://drive.google.com/file/d/1d5k_QRMRJyFSD...</td>\n",
       "      <td>Agent-362_base_MT-Simulation.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_id                                          colab_url  \\\n",
       "0  244_base_MT  https://colab.research.google.com/drive/1dTNOU...   \n",
       "1  129_base_MT  https://colab.research.google.com/drive/14JsFS...   \n",
       "2  276_base_MT  https://colab.research.google.com/drive/1-CVMo...   \n",
       "3  526_base_MT  https://colab.research.google.com/drive/13YN1G...   \n",
       "4  362_base_MT  https://drive.google.com/file/d/1d5k_QRMRJyFSD...   \n",
       "\n",
       "                           colab_name  no_action_script_success  \\\n",
       "0  Agent-244_base_MT-Simulation.ipynb                      True   \n",
       "1  Agent-129_base_MT-Simulation.ipynb                      True   \n",
       "2  Agent-276_base_MT-Simulation.ipynb                      True   \n",
       "3  Agent-526_base_MT-Simulation.ipynb                      True   \n",
       "4  Agent-362_base_MT-Simulation.ipynb                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "1  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "2  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "3  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "4  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "\n",
       "   with_action_script_success with_action_response  golden_answer_sample  \\\n",
       "0                        True                                      False   \n",
       "1                        True                                      False   \n",
       "2                        True                                      False   \n",
       "3                        True                                      False   \n",
       "4                        True                                      False   \n",
       "\n",
       "   Execution Status w/o Action Execution Status w Action Auto QC Status  \n",
       "0  FA Failed - Assertion Error            No Error Found     Good To Go  \n",
       "1  FA Failed - Assertion Error            No Error Found     Good To Go  \n",
       "2  FA Failed - Assertion Error            No Error Found     Good To Go  \n",
       "3  FA Failed - Assertion Error            No Error Found     Good To Go  \n",
       "4  FA Failed - Assertion Error            No Error Found     Good To Go  "
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response'</span><span style=\"color: #008080; text-decoration-color: #008080\"> already exists in the spreadsheet.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mTab \u001b[0m\u001b[32m'auto_qc_response'\u001b[0m\u001b[36m already exists in the spreadsheet.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Existing Dataframe</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mExisting Dataframe\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67 entries, 0 to 66\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   sample_id                    67 non-null     object\n",
      " 1   colab_url                    67 non-null     object\n",
      " 2   colab_name                   67 non-null     object\n",
      " 3   no_action_script_success     67 non-null     object\n",
      " 4   no_action_response           67 non-null     object\n",
      " 5   with_action_script_success   67 non-null     object\n",
      " 6   with_action_response         67 non-null     object\n",
      " 7   golden_answer_sample         67 non-null     object\n",
      " 8   Execution Status w/o Action  67 non-null     object\n",
      " 9   Execution Status w Action    67 non-null     object\n",
      " 10  Auto QC Status               67 non-null     object\n",
      "dtypes: object(11)\n",
      "memory usage: 5.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Combined Dataframe</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCombined Dataframe\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98 entries, 0 to 97\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   sample_id                    98 non-null     object\n",
      " 1   colab_url                    98 non-null     object\n",
      " 2   colab_name                   98 non-null     object\n",
      " 3   no_action_script_success     98 non-null     object\n",
      " 4   no_action_response           98 non-null     object\n",
      " 5   with_action_script_success   98 non-null     object\n",
      " 6   with_action_response         98 non-null     object\n",
      " 7   golden_answer_sample         98 non-null     object\n",
      " 8   Execution Status w/o Action  98 non-null     object\n",
      " 9   Execution Status w Action    98 non-null     object\n",
      " 10  Auto QC Status               98 non-null     object\n",
      "dtypes: object(11)\n",
      "memory usage: 8.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Uploading </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span><span style=\"color: #008080; text-decoration-color: #008080\"> rows to tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mUploading \u001b[0m\u001b[1;36m98\u001b[0m\u001b[36m rows to tab \u001b[0m\u001b[32m'auto_qc_response'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1089</span><span style=\"color: #008080; text-decoration-color: #008080\"> cells updated in tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1089\u001b[0m\u001b[36m cells updated in tab \u001b[0m\u001b[32m'auto_qc_response'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tab = 'auto_qc_response'\n",
    "GoogleSheet.add_dataframe_to_sheet(sheet_id, merged_df, output_tab, drop_duplicates_on = ['sample_id', 'colab_url'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
