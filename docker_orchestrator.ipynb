{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gspread\n",
      "  Using cached gspread-6.2.1-py3-none-any.whl (59 kB)\n",
      "Collecting google-auth>=1.12.0\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1\n",
      "  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /Users/chaitraravada/.pyenv/versions/3.11.0/envs/pivotal/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/chaitraravada/.pyenv/versions/3.11.0/envs/pivotal/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chaitraravada/.pyenv/versions/3.11.0/envs/pivotal/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chaitraravada/.pyenv/versions/3.11.0/envs/pivotal/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chaitraravada/.pyenv/versions/3.11.0/envs/pivotal/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.8.3)\n",
      "Installing collected packages: pyasn1, oauthlib, cachetools, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, gspread\n",
      "Successfully installed cachetools-5.5.2 google-auth-2.40.3 google-auth-oauthlib-1.2.2 gspread-6.2.1 oauthlib-3.3.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install docker \n",
    "!pip install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import shlex\n",
    "import shutil\n",
    "import random\n",
    "import pathlib\n",
    "import subprocess\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "\n",
    "import docker\n",
    "import nbformat\n",
    "import gspread\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable, Dict, List, Sequence, Iterable, Union\n",
    "\n",
    "import pandas as pd\n",
    "from rclone_python import rclone\n",
    "from nbclient import NotebookClient\n",
    "from rclone_python.remote_types import RemoteTypes\n",
    "\n",
    "from google.auth import default\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "from googleapiclient.http import BatchHttpRequest, MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_FILE = 'turing-delivery-g-ga-e36eb2300714.json'\n",
    "\n",
    "# Combine scopes for both Drive and Sheets\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "]\n",
    "\n",
    "def authenticate_with_service_account():\n",
    "    \"\"\"Authenticate using a service account and return credentials.\"\"\"\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    return creds\n",
    "\n",
    "# Get the shared credentials object\n",
    "credentials = authenticate_with_service_account()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "# @title Logger Configs\n",
    "custom_theme = Theme({\n",
    "    \"info\": \"cyan\",\n",
    "    \"warning\": \"magenta\",\n",
    "    \"error\": \"bold red\"\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "class Logger:\n",
    "  @staticmethod\n",
    "  def log(message):\n",
    "    console.print(message, style=\"info\")\n",
    "\n",
    "  def error(message):\n",
    "    console.print(message, style=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleService Class\n",
    "class GoogleService:\n",
    "\n",
    "  @classmethod\n",
    "  def extract_file_id(cls, url):\n",
    "      patterns = [\n",
    "          r\"/spreadsheets/d/([^/]+)\",\n",
    "          r\"/file/d/([^/]+)\",     # Matches /file/d/{file_id}\n",
    "          r\"[?&]id=([^&]+)\",       # Matches ?id={file_id} or &id={file_id}\n",
    "          r\"/drive/([^/?#]+)\",     # Matches /drive/{file_id} and stops at /, ?, or #\n",
    "          r\"/folders/([^/]+)\"      # Matches /folders/{folder_id}\n",
    "      ]\n",
    "\n",
    "      for pattern in patterns:\n",
    "          match_ = re.search(pattern, url)\n",
    "          if match_:\n",
    "              return match_.group(1).strip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleDrive Functionality\n",
    "class GoogleDrive(GoogleService):\n",
    "    \n",
    "    service = build(\"drive\", \"v3\", credentials=credentials)\n",
    "\n",
    "    @classmethod\n",
    "    def get_file_names_in_batch(cls, file_ids):\n",
    "        \"\"\"\n",
    "        Retrieves the names of multiple files from Google Drive in a single batch request.\n",
    "        \n",
    "        Args:\n",
    "            drive_service: An authenticated Google Drive API service object.\n",
    "            file_ids: A list of file IDs.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary mapping file IDs to their names.\n",
    "        \"\"\"\n",
    "        file_names = []\n",
    "    \n",
    "        def callback(request_id, response, exception):\n",
    "            \"\"\"\n",
    "            Callback function to process the result of each individual request.\n",
    "            \"\"\"\n",
    "            if exception:\n",
    "                print(f\"Error for file ID {request_id}: {exception}\")\n",
    "                \n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': None\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': response.get('name')\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "        # Create a batch request with the callback\n",
    "        batch = cls.service.new_batch_http_request(callback=callback)\n",
    "    \n",
    "        # Add a 'files().get()' request for each file ID\n",
    "        for file_id in file_ids:\n",
    "            batch.add(\n",
    "                cls.service.files().get(\n",
    "                    fileId=file_id,\n",
    "                    fields='name',\n",
    "                    supportsAllDrives=True\n",
    "                ),\n",
    "                request_id=file_id  # Use the file ID to track each request\n",
    "            )\n",
    "    \n",
    "        # Execute the batch request\n",
    "        batch.execute()\n",
    "    \n",
    "        return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleSheets Functionality\n",
    "class GoogleSheet(GoogleService):\n",
    "\n",
    "  # service = build(\"sheets\", \"v4\")\n",
    "  service = build(\"sheets\", \"v4\", credentials=credentials)\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_data(cls, sheet_id: str, tab_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from existing Google Sheet and returns it as Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sheet_id: The ID of the existing Google Sheet.\n",
    "        tab_name: The desired name for the new tab.\n",
    "        filter_col [Optional]: column name to filter the data.\n",
    "        filter_val [Optional]: value to filter the data on.\n",
    "    \"\"\"\n",
    "    vals = (\n",
    "        cls.service.spreadsheets()\n",
    "        .values()\n",
    "        .get(spreadsheetId=sheet_id, range=tab_name)\n",
    "        .execute()\n",
    "        .get(\"values\", [])\n",
    "    )\n",
    "    if len(vals) > 0:\n",
    "      header = vals[0]\n",
    "      data_values = vals[1:]\n",
    "      max_columns = min(len(header), len(data_values[0]))\n",
    "      data_values = [row[:max_columns] for row in data_values]\n",
    "      header = header[:max_columns]\n",
    "      df = pd.DataFrame(data_values, columns=header)\n",
    "      df.columns = [column.strip() for column in df.columns]\n",
    "      filter_cols = [col.strip() for col in kwargs.keys()]\n",
    "      if filter_cols:\n",
    "        if all(col in df.columns for col in filter_cols):\n",
    "          query = \" & \".join([\n",
    "              f\"{col}=='{kwargs[col]}'\"\n",
    "              if isinstance(kwargs[col], str)\n",
    "              else f\"{col}=={kwargs[col]}\"\n",
    "              for col in filter_cols])\n",
    "          df = df.query(query)\n",
    "        else:\n",
    "          missing_cols = [col for col in filter_cols if col not in df.columns]\n",
    "          raise Exception(f\"Could not find column(s) in the sheet. {missing_cols}\")\n",
    "      return df\n",
    "    sheet_name = cls.get_spreadsheet_name_by_id(sheet_id)\n",
    "    raise Exception(f\"No data found in the Tab: {tab_name}. Sheet ID: {sheet_name}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def tab_exists(cls, spreadsheet_id, tab_name):\n",
    "\n",
    "    spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        fields='sheets.properties'\n",
    "    ).execute()\n",
    "\n",
    "    sheets = spreadsheet_metadata.get('sheets', [])\n",
    "    for sheet in sheets:\n",
    "        properties = sheet.get('properties')\n",
    "        if properties and (properties.get('title') == tab_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dataframe_to_sheet(cls, spreadsheet_id, df, tab_name, valueInputOption='RAW', drop_duplicates_on=['sample_id']):\n",
    "    \"\"\"\n",
    "    Adds a new tab to an existing Google Sheet and populates it with data from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spreadsheet_id: The ID of the existing Google Sheet.\n",
    "        df: The Pandas DataFrame to export.\n",
    "        tab_name: The desired name for the new tab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      if cls.tab_exists(spreadsheet_id, tab_name):\n",
    "        Logger.log(f\"Tab '{tab_name}' already exists in the spreadsheet.\")\n",
    "        existing_df = cls.get_sheet_data(spreadsheet_id, tab_name)\n",
    "        # TODO: Add dataframe validation check\n",
    "        Logger.log(f\"Existing Dataframe\")\n",
    "        Logger.log(existing_df.info())\n",
    "\n",
    "        combined_df = pd.concat([df, existing_df], ignore_index=True)\n",
    "        df_to_upload = combined_df.drop_duplicates(subset=drop_duplicates_on, keep='first', ignore_index=True)\n",
    "        Logger.log(f\"Combined Dataframe\")\n",
    "        Logger.log(df_to_upload.info())\n",
    "\n",
    "      else:\n",
    "        Logger.log(f\"Tab '{tab_name}' does not exist in the spreadsheet. Creating a new tab.\")\n",
    "        requests = [{\n",
    "            'addSheet': {\n",
    "                'properties': {\n",
    "                    'title': tab_name\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        batch_update_body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=batch_update_body\n",
    "        ).execute()\n",
    "        # Get the ID of the newly created sheet (optional, but useful)\n",
    "        new_sheet_id = response.get('replies')[0].get('addSheet').get('properties').get('sheetId')\n",
    "        Logger.log(f\"Successfully added new tab: '{tab_name}' with ID: {new_sheet_id}\")\n",
    "        df_to_upload = df\n",
    "\n",
    "      values = [df_to_upload.columns.tolist()] + df_to_upload.values.tolist()\n",
    "      Logger.log(f\"Uploading {len(df_to_upload)} rows to tab '{tab_name}'.\")\n",
    "      range_name = f\"'{tab_name}'!A1\" # Ensure tab name is quoted if it has spaces or special characters\n",
    "      body = {\n",
    "          'values': values\n",
    "      }\n",
    "      result = cls.service.spreadsheets().values().update(\n",
    "          spreadsheetId=spreadsheet_id,\n",
    "          range=range_name,\n",
    "          valueInputOption=valueInputOption,\n",
    "          body=body\n",
    "      ).execute()\n",
    "\n",
    "      Logger.log(f\"{result.get('updatedCells')} cells updated in tab '{tab_name}'.\")\n",
    "\n",
    "    except HttpError as err:\n",
    "      Logger.error(f\"An error occurred: {err}\")\n",
    "      if err.resp.status == 400: # Bad Request, often due to sheet name already existing\n",
    "        Logger.error(\"Error details: The tab name might already exist or the request is malformed.\")\n",
    "      elif err.resp.status == 403: # Forbidden, often due to incorrect permissions\n",
    "        Logger.error(\"Error details: Check your API permissions or if the service account/user has access to the sheet.\")\n",
    "      elif err.resp.status == 404: # Not Found, often due to incorrect spreadsheet ID\n",
    "        Logger.error(\"Error details: The spreadsheet ID might be incorrect.\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_spreadsheet_name_by_id(cls, spreadsheet_id):\n",
    "      \"\"\"\n",
    "      Retrieves the name (title) of a Google Spreadsheet given its ID.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id: The ID of the Google Spreadsheet.\n",
    "\n",
    "      Returns:\n",
    "          The title of the spreadsheet, or None if an error occurs or spreadsheet is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request the 'properties.title' field for efficiency\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='properties.title'\n",
    "          ).execute()\n",
    "\n",
    "          # Extract the title from the properties\n",
    "          title = spreadsheet_metadata.get('properties', {}).get('title')\n",
    "          return title\n",
    "      except HttpError as error:\n",
    "          print(f'An error occurred: {error}')\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found.\")\n",
    "          return None\n",
    "\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dropdown_to_range(cls, spreadsheet_id: str, sheet_id: str,\n",
    "                            dropdown_options: list,\n",
    "                            range_start_row: int, range_end_row: int,\n",
    "                            range_start_col: int, range_end_col: int):\n",
    "    requests = [\n",
    "        {\n",
    "            'setDataValidation': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': range_start_row,\n",
    "                    'endRowIndex': range_end_row,\n",
    "                    'startColumnIndex': range_start_col,\n",
    "                    'endColumnIndex': range_end_col\n",
    "                },\n",
    "                'rule': {\n",
    "                    'condition': {\n",
    "                        'type': 'ONE_OF_LIST',\n",
    "                        'values': [{'userEnteredValue': option} for option in dropdown_options]\n",
    "                    },\n",
    "                    'strict': True,  # Users can only enter values from the list\n",
    "                    'showCustomUi': True, # Show dropdown arrow\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # --- Execute the batch update request ---\n",
    "    try:\n",
    "        body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=body\n",
    "        ).execute()\n",
    "        print(f\"Dropdown added to Sheet ID {sheet_id}, Range row{range_start_row+1}:row{range_end_row}.\")\n",
    "        # You can inspect the response for more details if needed\n",
    "        # print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_id_by_name(cls, spreadsheet_id: str, tab_name: str):\n",
    "      \"\"\"\n",
    "      Retrieves the numerical Sheet ID (gid) for a given tab name within a spreadsheet.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id (str): The ID of the Google Spreadsheet.\n",
    "          tab_name (str): The exact name (title) of the tab/sheet to find.\n",
    "\n",
    "      Returns:\n",
    "          int: The numerical sheet ID (gid) if found.\n",
    "          None: If an error occurs, the spreadsheet is not found, or the tab name is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request 'sheets.properties' to get sheet IDs and titles efficiently\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='sheets.properties'\n",
    "          ).execute()\n",
    "\n",
    "          sheets = spreadsheet_metadata.get('sheets', [])\n",
    "          for sheet in sheets:\n",
    "              properties = sheet.get('properties')\n",
    "              # Check if properties exist and if the title matches the tab_name\n",
    "              if properties and properties.get('title') == tab_name:\n",
    "                  return properties.get('sheetId') # Return the sheetId (gid)\n",
    "\n",
    "          # If the loop completes, the tab was not found\n",
    "          print(f\"Tab '{tab_name}' not found in spreadsheet with ID '{spreadsheet_id}'.\")\n",
    "          return None\n",
    "      except HttpError as error:\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Error: {error}\")\n",
    "          else:\n",
    "              print(f'An HTTP error occurred: {error}')\n",
    "          return None\n",
    "      except Exception as e:\n",
    "          print(f\"An unexpected error occurred while retrieving sheet ID for tab '{tab_name}': {e}\")\n",
    "          return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download APIs Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_apis(VERSION=\"0.1.0\", download_datasets=False):\n",
    "    import io\n",
    "    import os\n",
    "    import sys\n",
    "    import zipfile\n",
    "    import shutil\n",
    "    import re\n",
    "    # from google.colab import auth\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "    drive_service = GoogleDrive.service\n",
    "    # Version to download\n",
    "    # VERSION = \"0.0.rev22final\" # Version of the API\n",
    "    \n",
    "    # Define paths\n",
    "    CONTENT_DIR = os.path.join('clean_workspace', VERSION)\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        os.remove(CONTENT_DIR)\n",
    "    os.makedirs(CONTENT_DIR, exist_ok=True)\n",
    "    \n",
    "    APIS_DIR = os.path.join(CONTENT_DIR, 'APIs')\n",
    "    DBS_DIR = os.path.join(CONTENT_DIR, 'DBs')\n",
    "    SCRIPTS_DIR = os.path.join(CONTENT_DIR, 'Scripts')\n",
    "    FC_DIR = os.path.join(CONTENT_DIR, 'Schemas')\n",
    "    ZIP_PATH = os.path.join(CONTENT_DIR, f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # Google Drive Folder ID where versioned APIs zip files are stored\n",
    "    APIS_FOLDER_ID = '1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4'\n",
    "    \n",
    "    # List of items to extract from the zip file\n",
    "    ITEMS_TO_EXTRACT = ['APIs/', 'DBs/', 'Scripts/']\n",
    "    \n",
    "    # Clean up existing directories and files\n",
    "    for path in [APIS_DIR, DBS_DIR, SCRIPTS_DIR, FC_DIR, ZIP_PATH]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "    \n",
    "    # Authenticate and create the drive service\n",
    "    # auth.authenticate_user()\n",
    "    # drive_service = build('drive', 'v3')\n",
    "    # drive_service\n",
    "    # Helper function to download a file from Google Drive\n",
    "    def download_drive_file(service, file_id, output_path, file_name=None, show_progress=True):\n",
    "        \"\"\"Downloads a file from Google Drive\"\"\"\n",
    "        destination = output_path\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        with io.FileIO(destination, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if show_progress:\n",
    "                    print(f\"Download progress: {int(status.progress() * 100)}%\")\n",
    "    \n",
    "    \n",
    "    # 1. List files in the specified APIs folder\n",
    "    print(f\"Searching for APIs zip file with version {VERSION} in folder: {APIS_FOLDER_ID}...\")\n",
    "    apis_file_id = None\n",
    "    \n",
    "    try:\n",
    "        query = f\"'{APIS_FOLDER_ID}' in parents and trashed=false\"\n",
    "        results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "        files = results.get('files', [])\n",
    "        for file in files:\n",
    "            file_name = file.get('name', '')\n",
    "            if file_name.lower() == f'apis_v{VERSION.lower()}.zip':\n",
    "                apis_file_id = file.get('id')\n",
    "                print(f\"Found matching file: {file_name} (ID: {apis_file_id})\")\n",
    "                break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing files in Google Drive: {e}\")\n",
    "    \n",
    "    if not apis_file_id:\n",
    "        print(f\"Error: Could not find APIs zip file with version {VERSION} in the specified folder.\")\n",
    "        sys.exit(\"Required APIs zip file not found.\")\n",
    "    \n",
    "    # 2. Download the found APIs zip file\n",
    "    print(f\"Downloading APIs zip file with ID: {apis_file_id}...\")\n",
    "    download_drive_file(drive_service, apis_file_id, ZIP_PATH, file_name=f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # 3. Extract specific items from the zip file to /content\n",
    "    print(f\"Extracting specific items from {ZIP_PATH} to {CONTENT_DIR}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_contents = zip_ref.namelist()\n",
    "    \n",
    "            for member in zip_contents:\n",
    "                extracted = False\n",
    "                for item_prefix in ITEMS_TO_EXTRACT:\n",
    "                  if member == item_prefix or member.startswith(item_prefix):\n",
    "                        zip_ref.extract(member, CONTENT_DIR)\n",
    "                        extracted = True\n",
    "                        break\n",
    "    \n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The downloaded file at {ZIP_PATH} is not a valid zip file.\")\n",
    "        sys.exit(\"Invalid zip file downloaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "        sys.exit(\"Extraction failed.\")\n",
    "    \n",
    "    \n",
    "    # 4. Clean up\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)\n",
    "    \n",
    "    # 5. Add APIs to path\n",
    "    if os.path.exists(APIS_DIR):\n",
    "        sys.path.append(APIS_DIR)\n",
    "    else:\n",
    "        print(f\"Error: APIS directory not found at {APIS_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    # 6. Quick verification\n",
    "    # Check for the presence of the extracted items\n",
    "    verification_paths = [APIS_DIR, DBS_DIR, SCRIPTS_DIR]\n",
    "    all_present = True\n",
    "    print(\"\\nVerifying extracted items:\")\n",
    "    for path in verification_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✅ {path} is present.\")\n",
    "        else:\n",
    "            print(f\"❌ {path} is MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(f\"\\n✅ Setup complete! Required items extracted to {CONTENT_DIR}.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Setup failed! Not all required items were extracted.\")\n",
    "\n",
    "    # 7. Generate Schemas\n",
    "\n",
    "    # Add Scripts to path\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        if os.path.isdir(CONTENT_DIR):\n",
    "            shutil.rmtree(CONTENT_DIR)\n",
    "        else:\n",
    "            os.remove(CONTENT_DIR)\n",
    "    else:\n",
    "        print(f\"Error: CONTENT_DIR directory not found at {CONTENT_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    from Scripts.FCSpec import generate_package_schema\n",
    "    \n",
    "    print(\"\\nGenerating FC Schemas\")\n",
    "    os.makedirs(FC_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Iterate through the packages in the /content/APIs directory\n",
    "    for package_name in os.listdir(APIS_DIR):\n",
    "        package_path = os.path.join(APIS_DIR, package_name)\n",
    "    \n",
    "        # Check if it's a directory (to avoid processing files)\n",
    "        if os.path.isdir(package_path):\n",
    "            # Call the function to generate schema for the current package\n",
    "            generate_package_schema(package_path, output_folder_path=FC_DIR)\n",
    "    print(f\"✅ Successfully generated {len(os.listdir(FC_DIR))} FC Schemas to {FC_DIR}\")\n",
    "\n",
    "    if download_datasets:\n",
    "        def download_drive_folder(service, folder_id, destination_path):\n",
    "            \"\"\"\n",
    "            Recursively downloads all files in a Google Drive folder using the `download_drive_file`\n",
    "            \"\"\"\n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "        \n",
    "            query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "            page_token = None\n",
    "        \n",
    "            while True:\n",
    "                results = service.files().list(\n",
    "                    q=query,\n",
    "                    spaces='drive',\n",
    "                    fields='nextPageToken, files(id, name, mimeType)',\n",
    "                    pageToken=page_token\n",
    "                ).execute()\n",
    "        \n",
    "                for item in results.get('files', []):\n",
    "                    file_id = item['id']\n",
    "                    file_name = item['name']\n",
    "                    mime_type = item['mimeType']\n",
    "        \n",
    "                    if mime_type == 'application/vnd.google-apps.folder':\n",
    "                        # Recursively download subfolders\n",
    "                        new_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Creating subfolder and downloading: {new_path}\")\n",
    "                        download_drive_folder(service, file_id, new_path)\n",
    "                    else:\n",
    "                        # Construct full file path and pass it as output_path\n",
    "                        full_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Downloading file: {file_name} to {full_path}\")\n",
    "                        download_drive_file(service, file_id, full_path, file_name=file_name, show_progress=False)\n",
    "        \n",
    "                page_token = results.get('nextPageToken', None)\n",
    "                if not page_token:\n",
    "                    break\n",
    "        \n",
    "        # --- Configuration for Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'Quotewk.csv' file.\n",
    "        FOLDER_ID = \"1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml\"\n",
    "        # DATASET_FOLDER = \"./workspace/Datasets\"\n",
    "        DATASET_FOLDER = os.path.join(CONTENT_DIR, 'workspace/Datasets')\n",
    "        \n",
    "        print(f\"Starting download of folder {FOLDER_ID} to {DATASET_FOLDER}...\")\n",
    "        download_drive_folder(drive_service, FOLDER_ID, DATASET_FOLDER)\n",
    "        print(\"Dataset download complete.\")\n",
    "\n",
    "        # --- Configuration for WS Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'WS Multihop Datasets' file.\n",
    "        WS_DATA_ID = \"1kmXZ1oarBPlE0OQL52eGoc1xPbupJ1n9\"\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        \n",
    "        print(f\"Downloading WS Dataset zip file with ID: {WS_DATA_ID}...\")\n",
    "        download_drive_file(drive_service, WS_DATA_ID, WS_DATA_ZIP_PATH, file_name=f'WS_DATA.zip')\n",
    "        print(\"Dataset download complete.\")\n",
    "        \n",
    "        # Extract the Datasets\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        with zipfile.ZipFile(WS_DATA_ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(CONTENT_DIR)\n",
    "        print(f\"Extracted to {CONTENT_DIR}\")\n",
    "        \n",
    "        # Moving 'file_dataset_pb2.py' to root directory\n",
    "        src_path = os.path.join(CONTENT_DIR, 'WS_DATA', 'file_dataset_pb2.py')\n",
    "        dst_path = os.path.join(CONTENT_DIR, 'file_dataset_pb2.py')\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"Moved {src_path} to {dst_path}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {src_path}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(WS_DATA_ZIP_PATH):\n",
    "            os.remove(WS_DATA_ZIP_PATH)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: 'clean_workspace/0.1.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdownload_apis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mdownload_apis\u001b[0;34m(VERSION, download_datasets)\u001b[0m\n\u001b[1;32m     17\u001b[0m CONTENT_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_workspace\u001b[39m\u001b[38;5;124m'\u001b[39m, VERSION)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(CONTENT_DIR):\n\u001b[0;32m---> 19\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(CONTENT_DIR)\n\u001b[1;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(CONTENT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m APIS_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CONTENT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPIs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: 'clean_workspace/0.1.0'"
     ]
    }
   ],
   "source": [
    "download_apis(download_datasets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch & Download Colabs / Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 329 entries, 3 to 2809\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sample_id  329 non-null    object\n",
      " 1   colab_url  329 non-null    object\n",
      " 2   status     329 non-null    object\n",
      " 3   with       329 non-null    object\n",
      " 4   w/o        329 non-null    object\n",
      " 5   colab_id   329 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 18.0+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet_id = \"15XdJpUXvy7NC9Wb4NprQydcdQ5rQYdkij-NDo1saVzk\"\n",
    "data_tab = \"auto_qc_data\"\n",
    "\n",
    "colabs_df = GoogleSheet.get_sheet_data(sheet_id, data_tab)\n",
    "\n",
    "colabs_df = colabs_df.loc[((colabs_df['w/o'] != 'No Error Found') | (colabs_df['with'] != 'No Error Found')) & (colabs_df['status'] == 'Needs Fixes')]\n",
    "\n",
    "colabs_df['colab_id'] = colabs_df['colab_url'].apply(GoogleService.extract_file_id)\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 294 entries, 0 to 382\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   colab_id    294 non-null    object\n",
      " 1   colab_name  294 non-null    object\n",
      " 2   sample_id   294 non-null    object\n",
      " 3   colab_url   294 non-null    object\n",
      " 4   status      294 non-null    object\n",
      " 5   with        294 non-null    object\n",
      " 6   w/o         294 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 18.4+ KB\n"
     ]
    }
   ],
   "source": [
    "colab_names = []\n",
    "name_request_batch_size = 99\n",
    "for start in range(0, len(colabs_df['colab_id']), name_request_batch_size):\n",
    "    colab_names += GoogleDrive.get_file_names_in_batch(colabs_df['colab_id'].tolist()[start:start+name_request_batch_size])\n",
    "colab_name_df = pd.DataFrame(colab_names)\n",
    "colab_name_df = colab_name_df[~colab_name_df['colab_name'].isna()]\n",
    "colabs_df = pd.merge(colab_name_df, colabs_df, on='colab_id')\n",
    "colabs_df = colabs_df.drop_duplicates(['colab_id'])\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batches and Configuration Files for Docker Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 25\n",
      "Max Samples Per Batch: 12\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(colabs_df)\n",
    "max_container = 25\n",
    "max_batch_size = math.ceil(total_samples / max_container)\n",
    "print(f'Total Batches: {max_container}\\nMax Samples Per Batch: {max_batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_id\n",
       "0.1.0_0     12\n",
       "0.1.0_13    12\n",
       "0.1.0_23    12\n",
       "0.1.0_22    12\n",
       "0.1.0_21    12\n",
       "0.1.0_20    12\n",
       "0.1.0_19    12\n",
       "0.1.0_18    12\n",
       "0.1.0_17    12\n",
       "0.1.0_16    12\n",
       "0.1.0_15    12\n",
       "0.1.0_14    12\n",
       "0.1.0_12    12\n",
       "0.1.0_1     12\n",
       "0.1.0_11    12\n",
       "0.1.0_10    12\n",
       "0.1.0_9     12\n",
       "0.1.0_8     12\n",
       "0.1.0_7     12\n",
       "0.1.0_6     12\n",
       "0.1.0_5     12\n",
       "0.1.0_4     12\n",
       "0.1.0_3     12\n",
       "0.1.0_2     12\n",
       "0.1.0_24     6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_version = '0.1.0'\n",
    "notebooks = [{'path': notebook, 'api_version': api_version} for notebook in colabs_df['colab_id'].tolist()]\n",
    "notebooks_df = pd.DataFrame(notebooks)\n",
    "for idx, api_version in enumerate(set(notebooks_df['api_version'])):\n",
    "    count_notebooks = len(notebooks_df[notebooks_df['api_version']==api_version])\n",
    "    batches = []\n",
    "    for idx in range(count_notebooks):\n",
    "        batches.append(idx//max_batch_size)\n",
    "    batch_ids = [f\"{api_version}_{batch}\" for batch in batches]\n",
    "    notebooks_df.loc[notebooks_df['api_version'] == api_version, 'batch_id'] = batch_ids\n",
    "\n",
    "notebooks_df.to_csv('execution_configs.csv', index=False)\n",
    "\n",
    "notebooks_df = pd.merge(notebooks_df, colabs_df, left_on='path', right_on='colab_id')\n",
    "run_identifiers = list(set(notebooks_df['batch_id']))\n",
    "notebooks_df['batch_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local Run (where you have root access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Validating Host Environment ---\n",
      "✅ Docker client connected.\n",
      "\n",
      "--- Step 2: Preparing Host Directories ---\n",
      "✅ Created log directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/execution_logs/sanity_check_20250818_173919\n",
      "✅ Created result directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/results/sanity_check_20250818_173919\n",
      "\n",
      "--- Step 4: Launching Containers in Parallel ---\n",
      "  -> Launching container 'sanity_check_20250818_173919-0' for batch 0...\n",
      "  -> Launching container 'sanity_check_20250818_173919-1' for batch 1...\n",
      "  -> Launching container 'sanity_check_20250818_173919-2' for batch 2...\n",
      "  -> Launching container 'sanity_check_20250818_173919-3' for batch 3...\n",
      "  -> Launching container 'sanity_check_20250818_173919-4' for batch 4...\n",
      "\n",
      "--- Step 5: Waiting for All Containers to Finish ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      4\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msanity_check_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43morchestrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_orchestration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_identifiers\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Docker Run. Time Taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;241m.\u001b[39mseconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mFileExistsError\u001b[39;00m, \u001b[38;5;167;01mConnectionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/sanity_orchestrator_with_download.py:162\u001b[0m, in \u001b[0;36mrun_orchestration\u001b[0;34m(run_name, run_identifiers)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# num_batches = _create_task_batches(run_log_dir, notebooks_per_batch)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# num_batches = 3\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# if num_batches > 0:\u001b[39;00m\n\u001b[1;32m    155\u001b[0m running_containers \u001b[38;5;241m=\u001b[39m _launch_containers(\n\u001b[1;32m    156\u001b[0m     client,\n\u001b[1;32m    157\u001b[0m     run_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     run_identifiers,\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43m_wait_for_containers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_log_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_containers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Orchestration Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📄 All results and logs are stored in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_log_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/sanity_orchestrator_with_download.py:124\u001b[0m, in \u001b[0;36m_wait_for_containers\u001b[0;34m(run_log_dir, running_containers)\u001b[0m\n\u001b[1;32m    122\u001b[0m container \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    123\u001b[0m name \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 124\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m exit_code \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatusCode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    126\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ SUCCESS\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exit_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/docker/models/containers.py:528\u001b[0m, in \u001b[0;36mContainer.wait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    508\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Block until the container stops, then return its exit code. Similar to\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    the ``docker wait`` command.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03m            If the server returns an error.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/docker/utils/decorators.py:19\u001b[0m, in \u001b[0;36mcheck_resource.<locals>.decorator.<locals>.wrapped\u001b[0;34m(self, resource_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_id:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNullResource(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResource ID was not provided\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/docker/api/container.py:1347\u001b[0m, in \u001b[0;36mContainerApiMixin.wait\u001b[0;34m(self, container, timeout, condition)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidVersion(\n\u001b[1;32m   1343\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait condition is not supported for API version < 1.30\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1344\u001b[0m         )\n\u001b[1;32m   1345\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m condition\n\u001b[0;32m-> 1347\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result(res, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/docker/utils/decorators.py:44\u001b[0m, in \u001b[0;36mupdate_headers.<locals>.inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_general_configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHttpHeaders\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/docker/api/client.py:242\u001b[0m, in \u001b[0;36mAPIClient._post\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;129m@update_headers\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_request_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/urllib3/response.py:1088\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/urllib3/response.py:1248\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/urllib3/response.py:1167\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1167\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sanity_orchestrator_with_download as orchestrator\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    run_name = f'sanity_check_{start_time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    orchestrator.run_orchestration(run_name, run_identifiers)\n",
    "    print(f\"Finished Docker Run. Time Taken: {(datetime.now()-start_time).seconds} Seconds\")\n",
    "except (FileNotFoundError, FileExistsError, ConnectionError) as e:\n",
    "    print(f\"\\n❌ A critical error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For VM Run (where you need to use sudo to run docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo .venv/bin/python runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 7 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   notebook                    10 non-null     object\n",
      " 1   no_action_script_success    10 non-null     bool  \n",
      " 2   no_action_response          10 non-null     object\n",
      " 3   with_action_script_success  10 non-null     bool  \n",
      " 4   with_action_response        10 non-null     object\n",
      " 5   contains_golden_answer      10 non-null     bool  \n",
      " 6   contains_final_assert       10 non-null     bool  \n",
      "dtypes: bool(4), object(3)\n",
      "memory usage: 408.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'results/{run_name}'\n",
    "output_files = os.listdir(output_dir)\n",
    "complete_data = []\n",
    "for file in output_files:\n",
    "    full_path = Path(output_dir) / file\n",
    "    with open(full_path, 'r') as f:\n",
    "        complete_data += json.load(f)['result']\n",
    "# Use json_normalize to flatten the data\n",
    "sanity_df = pd.json_normalize(complete_data)\n",
    "sanity_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1fNlXFhd4nLk4Rg1pIiuVo45BO-_FmzWj</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14nilo8LiDc5q8ovgWkHWavARtSKPN1eA</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Action\\nError Type: NameError\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1nyw5UrEA6np_IKooIjNVWKk4yyYgl7cY</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1h2C3rMUeEB6Hvhj7HaA8-gYZaV0hpqCD</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: NameErro...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: NameErro...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1sTaFANN1qm0NBYvHQpW9vhkzQszmh0JR</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            notebook  no_action_script_success  \\\n",
       "0  1fNlXFhd4nLk4Rg1pIiuVo45BO-_FmzWj                      True   \n",
       "1  14nilo8LiDc5q8ovgWkHWavARtSKPN1eA                      True   \n",
       "2  1nyw5UrEA6np_IKooIjNVWKk4yyYgl7cY                      True   \n",
       "3  1h2C3rMUeEB6Hvhj7HaA8-gYZaV0hpqCD                      True   \n",
       "4  1sTaFANN1qm0NBYvHQpW9vhkzQszmh0JR                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "1  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "2  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "3  Block: # Final Assertion\\nError Type: NameErro...   \n",
       "4                                                      \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  contains_golden_answer  \\\n",
       "0                                                                      False   \n",
       "1  Block: # Action\\nError Type: NameError\\nError ...                   False   \n",
       "2                                                                      False   \n",
       "3  Block: # Final Assertion\\nError Type: NameErro...                   False   \n",
       "4                                                                       True   \n",
       "\n",
       "   contains_final_assert  \n",
       "0                   True  \n",
       "1                   True  \n",
       "2                   True  \n",
       "3                   True  \n",
       "4                   True  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>colab_url</th>\n",
       "      <th>colab_name</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1142_base_GC</td>\n",
       "      <td>https://drive.google.com/file/d/1UbL5jd_UFUpkh...</td>\n",
       "      <td>Agent-1142_base_GC-Initial_with_DB_Setup.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Action\\nError Type: NameError\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531_base_GC</td>\n",
       "      <td>https://drive.google.com/file/d/1sdiaZ7lc8ZfFz...</td>\n",
       "      <td>Agent-531_base_GC-Initial_with_DB_Setup.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1134_edge_2</td>\n",
       "      <td>https://colab.research.google.com/drive/1sTaFA...</td>\n",
       "      <td>Agent-1134_edge_2-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1065_edge_1</td>\n",
       "      <td>https://colab.research.google.com/drive/1MFMaM...</td>\n",
       "      <td>Agent-1065_edge_1-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terminal_2155_base_GC</td>\n",
       "      <td>https://colab.research.google.com/drive/1fNlXF...</td>\n",
       "      <td>Agent-2155_base_GC-Initial_with_DB_Setup.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sample_id                                          colab_url  \\\n",
       "0           1142_base_GC  https://drive.google.com/file/d/1UbL5jd_UFUpkh...   \n",
       "1            531_base_GC  https://drive.google.com/file/d/1sdiaZ7lc8ZfFz...   \n",
       "2            1134_edge_2  https://colab.research.google.com/drive/1sTaFA...   \n",
       "3            1065_edge_1  https://colab.research.google.com/drive/1MFMaM...   \n",
       "4  Terminal_2155_base_GC  https://colab.research.google.com/drive/1fNlXF...   \n",
       "\n",
       "                                       colab_name  no_action_script_success  \\\n",
       "0  Agent-1142_base_GC-Initial_with_DB_Setup.ipynb                      True   \n",
       "1   Agent-531_base_GC-Initial_with_DB_Setup.ipynb                      True   \n",
       "2                  Agent-1134_edge_2-Merged.ipynb                      True   \n",
       "3                  Agent-1065_edge_1-Merged.ipynb                      True   \n",
       "4  Agent-2155_base_GC-Initial_with_DB_Setup.ipynb                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "1  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  contains_golden_answer  \\\n",
       "0  Block: # Action\\nError Type: NameError\\nError ...                   False   \n",
       "1                                                                      False   \n",
       "2                                                                       True   \n",
       "3                                                                       True   \n",
       "4                                                                      False   \n",
       "\n",
       "   contains_final_assert  \n",
       "0                   True  \n",
       "1                   True  \n",
       "2                   True  \n",
       "3                   True  \n",
       "4                   True  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_set = [\n",
    "    'sample_id', \n",
    "    'colab_url', \n",
    "    'colab_name', \n",
    "    'no_action_script_success',\n",
    "    'no_action_response',\n",
    "    'with_action_script_success',\n",
    "    'with_action_response',\n",
    "    'contains_golden_answer',\n",
    "    'contains_final_assert',\n",
    "    ]\n",
    "merged_df = pd.merge(colabs_df, sanity_df, left_on='colab_id', right_on='notebook')[columns_set]\n",
    "merged_df = merged_df.fillna(\"\")\n",
    "for col in ['with_action_response', 'no_action_response']:\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: x[:49999])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 9 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   sample_id                   10 non-null     object\n",
      " 1   colab_url                   10 non-null     object\n",
      " 2   colab_name                  10 non-null     object\n",
      " 3   no_action_script_success    10 non-null     bool  \n",
      " 4   no_action_response          10 non-null     object\n",
      " 5   with_action_script_success  10 non-null     bool  \n",
      " 6   with_action_response        10 non-null     object\n",
      " 7   contains_golden_answer      10 non-null     bool  \n",
      " 8   contains_final_assert       10 non-null     bool  \n",
      "dtypes: bool(4), object(5)\n",
      "memory usage: 568.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_FAILED_ASSERTION = 'FA Failed - Assertion Error'\n",
    "IA_FAILED_ASSERTION = 'IA Failed - Assertion Error'\n",
    "NON_ASSERTION_ERROR = 'Non Assertion Error'\n",
    "NO_ERROR_FOUND = 'No Error Found'\n",
    "UNDEFINED_ERROR = 'Undefined Error Type'\n",
    "\n",
    "NEEDS_FIXES = 'Needs Fixes'\n",
    "GOOD_TO_GO = 'Good To Go'\n",
    "NEEDS_MANUAL_REVIEW = 'Needs Manual Review'\n",
    "CHECK_NOT_EXECUTED = 'Check Not Executed'\n",
    "\n",
    "def add_error_type(error_message):\n",
    "    if error_message == \"\":\n",
    "        return NO_ERROR_FOUND\n",
    "    block = error_message.split('\\n')[0].split(':')[-1].strip()\n",
    "    error_type = error_message.split('\\n')[1].split(':')[-1].strip()\n",
    "    initial_assertion_header = 'Initial Assertion'\n",
    "    final_assertion_header = 'Final Assertion'\n",
    "    # Non Assertion Error\n",
    "    if error_type != 'AssertionError':\n",
    "        return NON_ASSERTION_ERROR\n",
    "    if error_type == 'AssertionError':\n",
    "        if initial_assertion_header.lower() in block.lower():\n",
    "            return IA_FAILED_ASSERTION\n",
    "        if final_assertion_header.lower() in block.lower():\n",
    "            return FA_FAILED_ASSERTION\n",
    "    return UNDEFINED_ERROR\n",
    "\n",
    "def get_auto_qc_status(status_w_action, status_wo_action, contains_final_assert, contains_golden_answer):\n",
    "    if NON_ASSERTION_ERROR in [status_w_action, status_wo_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    if IA_FAILED_ASSERTION in [status_w_action, status_wo_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    if FA_FAILED_ASSERTION in [status_w_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    \n",
    "    if status_w_action == NO_ERROR_FOUND:\n",
    "        if status_wo_action == NO_ERROR_FOUND:\n",
    "            if contains_final_assert:\n",
    "                return NEEDS_FIXES\n",
    "            if not contains_final_assert and not contains_golden_answer:\n",
    "                return NEEDS_FIXES\n",
    "            return GOOD_TO_GO\n",
    "        \n",
    "        if status_wo_action == FA_FAILED_ASSERTION:\n",
    "            return GOOD_TO_GO\n",
    "        \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Auto QC Status\n",
       "Needs Fixes    5\n",
       "Good To Go     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Execution Status w/o Action'] = merged_df['no_action_response'].apply(add_error_type)\n",
    "merged_df['Execution Status w Action'] = merged_df['with_action_response'].apply(add_error_type)\n",
    "merged_df['Auto QC Status'] = merged_df.apply(lambda row: get_auto_qc_status(row['Execution Status w Action'], \n",
    "                                                                             row['Execution Status w/o Action'],\n",
    "                                                                             row['contains_final_assert'],\n",
    "                                                                             row['contains_golden_answer']), axis=1)\n",
    "merged_df['Auto QC Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>colab_id</th>\n",
       "      <th>colab_url</th>\n",
       "      <th>colab_name</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "      <th>Execution Status w/o Action</th>\n",
       "      <th>Execution Status w Action</th>\n",
       "      <th>Auto QC Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1142_base_GC</td>\n",
       "      <td>1UbL5jd_UFUpkhOXJV1JSXNSJDzo_nis0</td>\n",
       "      <td>https://drive.google.com/file/d/1UbL5jd_UFUpkh...</td>\n",
       "      <td>Agent-1142_base_GC-Initial_with_DB_Setup.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Action\\nError Type: NameError\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531_base_GC</td>\n",
       "      <td>1sdiaZ7lc8ZfFzpl6KLpQpMTsRYnyNf8k</td>\n",
       "      <td>https://drive.google.com/file/d/1sdiaZ7lc8ZfFz...</td>\n",
       "      <td>Agent-531_base_GC-Initial_with_DB_Setup.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1134_edge_2</td>\n",
       "      <td>1sTaFANN1qm0NBYvHQpW9vhkzQszmh0JR</td>\n",
       "      <td>https://colab.research.google.com/drive/1sTaFA...</td>\n",
       "      <td>Agent-1134_edge_2-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1065_edge_1</td>\n",
       "      <td>1MFMaM1VYU0Nj7gNwtMu4J9S_MahW_RE_</td>\n",
       "      <td>https://colab.research.google.com/drive/1MFMaM...</td>\n",
       "      <td>Agent-1065_edge_1-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terminal_2155_base_GC</td>\n",
       "      <td>1fNlXFhd4nLk4Rg1pIiuVo45BO-_FmzWj</td>\n",
       "      <td>https://colab.research.google.com/drive/1fNlXF...</td>\n",
       "      <td>Agent-2155_base_GC-Initial_with_DB_Setup.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: # Final Assertion\\nError Type: Assertio...</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FA Failed - Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sample_id                           colab_id  \\\n",
       "0           1142_base_GC  1UbL5jd_UFUpkhOXJV1JSXNSJDzo_nis0   \n",
       "1            531_base_GC  1sdiaZ7lc8ZfFzpl6KLpQpMTsRYnyNf8k   \n",
       "2            1134_edge_2  1sTaFANN1qm0NBYvHQpW9vhkzQszmh0JR   \n",
       "3            1065_edge_1  1MFMaM1VYU0Nj7gNwtMu4J9S_MahW_RE_   \n",
       "4  Terminal_2155_base_GC  1fNlXFhd4nLk4Rg1pIiuVo45BO-_FmzWj   \n",
       "\n",
       "                                           colab_url  \\\n",
       "0  https://drive.google.com/file/d/1UbL5jd_UFUpkh...   \n",
       "1  https://drive.google.com/file/d/1sdiaZ7lc8ZfFz...   \n",
       "2  https://colab.research.google.com/drive/1sTaFA...   \n",
       "3  https://colab.research.google.com/drive/1MFMaM...   \n",
       "4  https://colab.research.google.com/drive/1fNlXF...   \n",
       "\n",
       "                                       colab_name  no_action_script_success  \\\n",
       "0  Agent-1142_base_GC-Initial_with_DB_Setup.ipynb                      True   \n",
       "1   Agent-531_base_GC-Initial_with_DB_Setup.ipynb                      True   \n",
       "2                  Agent-1134_edge_2-Merged.ipynb                      True   \n",
       "3                  Agent-1065_edge_1-Merged.ipynb                      True   \n",
       "4  Agent-2155_base_GC-Initial_with_DB_Setup.ipynb                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "1  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Block: # Final Assertion\\nError Type: Assertio...   \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  contains_golden_answer  \\\n",
       "0  Block: # Action\\nError Type: NameError\\nError ...                   False   \n",
       "1                                                                      False   \n",
       "2                                                                       True   \n",
       "3                                                                       True   \n",
       "4                                                                      False   \n",
       "\n",
       "   contains_final_assert  Execution Status w/o Action  \\\n",
       "0                   True  FA Failed - Assertion Error   \n",
       "1                   True  FA Failed - Assertion Error   \n",
       "2                   True               No Error Found   \n",
       "3                   True               No Error Found   \n",
       "4                   True  FA Failed - Assertion Error   \n",
       "\n",
       "  Execution Status w Action Auto QC Status  \n",
       "0       Non Assertion Error    Needs Fixes  \n",
       "1            No Error Found     Good To Go  \n",
       "2            No Error Found    Needs Fixes  \n",
       "3            No Error Found    Needs Fixes  \n",
       "4            No Error Found     Good To Go  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert colab_id, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vh/827ts7793tq2cp18347yqwc00000gn/T/ipykernel_72449/1619329407.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'colab_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'colab_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGoogleDrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_file_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/e2e_sanity_checks/.venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5161\u001b[0m                 \u001b[0;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5162\u001b[0m             )\n\u001b[1;32m   5163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5164\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5165\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5167\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5168\u001b[0m         \u001b[0;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert colab_id, already exists"
     ]
    }
   ],
   "source": [
    "merged_df.insert(loc=1, column='colab_id', value=merged_df['colab_url'].apply(GoogleDrive.extract_file_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2557 entries, 0 to 2556\n",
      "Data columns (total 13 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   sample_id                    2557 non-null   object\n",
      " 1   colab_id                     2557 non-null   object\n",
      " 2   colab_url                    2557 non-null   object\n",
      " 3   colab_name                   2557 non-null   object\n",
      " 4   no_action_script_success     2557 non-null   object\n",
      " 5   no_action_response           2557 non-null   object\n",
      " 6   with_action_script_success   2557 non-null   object\n",
      " 7   with_action_response         2557 non-null   object\n",
      " 8   contains_golden_answer       2557 non-null   object\n",
      " 9   contains_final_assert        2557 non-null   object\n",
      " 10  Execution Status w/o Action  2557 non-null   object\n",
      " 11  Execution Status w Action    2557 non-null   object\n",
      " 12  Auto QC Status               2557 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 259.8+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\"> does not exist in the spreadsheet. Creating a new tab.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mTab \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m does not exist in the spreadsheet. Creating a new tab.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Successfully added new tab: </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\"> with ID: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1275880329</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mSuccessfully added new tab: \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m with ID: \u001b[0m\u001b[1;36m1275880329\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Uploading </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2557</span><span style=\"color: #008080; text-decoration-color: #008080\"> rows to tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mUploading \u001b[0m\u001b[1;36m2557\u001b[0m\u001b[36m rows to tab \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33254</span><span style=\"color: #008080; text-decoration-color: #008080\"> cells updated in tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m33254\u001b[0m\u001b[36m cells updated in tab \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tab = 'auto_qc_response_test'\n",
    "GoogleSheet.add_dataframe_to_sheet(sheet_id, merged_df, output_tab, drop_duplicates_on = ['colab_id'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pivotal_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
