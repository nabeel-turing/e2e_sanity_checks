{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import io\n",
    "import re\n",
    "# import sys\n",
    "import json\n",
    "import math\n",
    "# import copy\n",
    "# import time\n",
    "# import shlex\n",
    "# import shutil\n",
    "# import random\n",
    "# import pathlib\n",
    "# import subprocess\n",
    "# import traceback\n",
    "# import concurrent.futures\n",
    "\n",
    "# import docker\n",
    "# import nbformat\n",
    "# import gspread\n",
    "\n",
    "from pathlib import Path\n",
    "# from functools import partial\n",
    "from datetime import datetime\n",
    "# from tqdm.notebook import tqdm\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from typing import Callable, Dict, List, Sequence, Iterable, Union\n",
    "\n",
    "import pandas as pd\n",
    "# from rclone_python import rclone\n",
    "# from nbclient import NotebookClient\n",
    "# from rclone_python.remote_types import RemoteTypes\n",
    "\n",
    "# from google.auth import default\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "# from googleapiclient.http import BatchHttpRequest, MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_FILE = 'turing-delivery-g-ga-e36eb2300714.json'\n",
    "\n",
    "# Combine scopes for both Drive and Sheets\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "]\n",
    "\n",
    "def authenticate_with_service_account():\n",
    "    \"\"\"Authenticate using a service account and return credentials.\"\"\"\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    return creds\n",
    "\n",
    "# Get the shared credentials object\n",
    "credentials = authenticate_with_service_account()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "# @title Logger Configs\n",
    "custom_theme = Theme({\n",
    "    \"info\": \"cyan\",\n",
    "    \"warning\": \"magenta\",\n",
    "    \"error\": \"bold red\"\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "class Logger:\n",
    "  @staticmethod\n",
    "  def log(message):\n",
    "    console.print(message, style=\"info\")\n",
    "\n",
    "  def error(message):\n",
    "    console.print(message, style=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleService Class\n",
    "class GoogleService:\n",
    "\n",
    "  @classmethod\n",
    "  def extract_file_id(cls, url):\n",
    "      patterns = [\n",
    "          r\"/spreadsheets/d/([^/]+)\",\n",
    "          r\"/file/d/([^/]+)\",     # Matches /file/d/{file_id}\n",
    "          r\"[?&]id=([^&]+)\",       # Matches ?id={file_id} or &id={file_id}\n",
    "          r\"/drive/([^/?#]+)\",     # Matches /drive/{file_id} and stops at /, ?, or #\n",
    "          r\"/folders/([^/]+)\"      # Matches /folders/{folder_id}\n",
    "      ]\n",
    "\n",
    "      for pattern in patterns:\n",
    "          match_ = re.search(pattern, url)\n",
    "          if match_:\n",
    "              return match_.group(1).strip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleDrive Functionality\n",
    "class GoogleDrive(GoogleService):\n",
    "    \n",
    "    service = build(\"drive\", \"v3\", credentials=credentials)\n",
    "\n",
    "    @classmethod\n",
    "    def get_file_names_in_batch(cls, file_ids):\n",
    "        \"\"\"\n",
    "        Retrieves the names of multiple files from Google Drive in a single batch request.\n",
    "        \n",
    "        Args:\n",
    "            drive_service: An authenticated Google Drive API service object.\n",
    "            file_ids: A list of file IDs.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary mapping file IDs to their names.\n",
    "        \"\"\"\n",
    "        file_names = []\n",
    "    \n",
    "        def callback(request_id, response, exception):\n",
    "            \"\"\"\n",
    "            Callback function to process the result of each individual request.\n",
    "            \"\"\"\n",
    "            if exception:\n",
    "                print(f\"Error for file ID {request_id}: {exception}\")\n",
    "                \n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': None\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': response.get('name')\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "        # Create a batch request with the callback\n",
    "        batch = cls.service.new_batch_http_request(callback=callback)\n",
    "    \n",
    "        # Add a 'files().get()' request for each file ID\n",
    "        for file_id in file_ids:\n",
    "            batch.add(\n",
    "                cls.service.files().get(\n",
    "                    fileId=file_id,\n",
    "                    fields='name',\n",
    "                    supportsAllDrives=True\n",
    "                ),\n",
    "                request_id=file_id  # Use the file ID to track each request\n",
    "            )\n",
    "    \n",
    "        # Execute the batch request\n",
    "        batch.execute()\n",
    "    \n",
    "        return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleSheets Functionality\n",
    "class GoogleSheet(GoogleService):\n",
    "\n",
    "  # service = build(\"sheets\", \"v4\")\n",
    "  service = build(\"sheets\", \"v4\", credentials=credentials)\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_data(cls, sheet_id: str, tab_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from existing Google Sheet and returns it as Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sheet_id: The ID of the existing Google Sheet.\n",
    "        tab_name: The desired name for the new tab.\n",
    "        filter_col [Optional]: column name to filter the data.\n",
    "        filter_val [Optional]: value to filter the data on.\n",
    "    \"\"\"\n",
    "    vals = (\n",
    "        cls.service.spreadsheets()\n",
    "        .values()\n",
    "        .get(spreadsheetId=sheet_id, range=tab_name)\n",
    "        .execute()\n",
    "        .get(\"values\", [])\n",
    "    )\n",
    "    if len(vals) > 0:\n",
    "      header = vals[0]\n",
    "      data_values = vals[1:]\n",
    "      max_columns = min(len(header), len(data_values[0]))\n",
    "      data_values = [row[:max_columns] for row in data_values]\n",
    "      header = header[:max_columns]\n",
    "      df = pd.DataFrame(data_values, columns=header)\n",
    "      df.columns = [column.strip() for column in df.columns]\n",
    "      filter_cols = [col.strip() for col in kwargs.keys()]\n",
    "      if filter_cols:\n",
    "        if all(col in df.columns for col in filter_cols):\n",
    "          query = \" & \".join([\n",
    "              f\"{col}=='{kwargs[col]}'\"\n",
    "              if isinstance(kwargs[col], str)\n",
    "              else f\"{col}=={kwargs[col]}\"\n",
    "              for col in filter_cols])\n",
    "          df = df.query(query)\n",
    "        else:\n",
    "          missing_cols = [col for col in filter_cols if col not in df.columns]\n",
    "          raise Exception(f\"Could not find column(s) in the sheet. {missing_cols}\")\n",
    "      return df\n",
    "    sheet_name = cls.get_spreadsheet_name_by_id(sheet_id)\n",
    "    raise Exception(f\"No data found in the Tab: {tab_name}. Sheet ID: {sheet_name}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def tab_exists(cls, spreadsheet_id, tab_name):\n",
    "\n",
    "    spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        fields='sheets.properties'\n",
    "    ).execute()\n",
    "\n",
    "    sheets = spreadsheet_metadata.get('sheets', [])\n",
    "    for sheet in sheets:\n",
    "        properties = sheet.get('properties')\n",
    "        if properties and (properties.get('title') == tab_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dataframe_to_sheet(cls, spreadsheet_id, df, tab_name, valueInputOption='RAW', drop_duplicates_on=['sample_id']):\n",
    "    \"\"\"\n",
    "    Adds a new tab to an existing Google Sheet and populates it with data from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spreadsheet_id: The ID of the existing Google Sheet.\n",
    "        df: The Pandas DataFrame to export.\n",
    "        tab_name: The desired name for the new tab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      if cls.tab_exists(spreadsheet_id, tab_name):\n",
    "        Logger.log(f\"Tab '{tab_name}' already exists in the spreadsheet.\")\n",
    "        existing_df = cls.get_sheet_data(spreadsheet_id, tab_name)\n",
    "        # TODO: Add dataframe validation check\n",
    "        Logger.log(f\"Existing Dataframe\")\n",
    "        Logger.log(existing_df.info())\n",
    "\n",
    "        combined_df = pd.concat([df, existing_df], ignore_index=True)\n",
    "        df_to_upload = combined_df.drop_duplicates(subset=drop_duplicates_on, keep='first', ignore_index=True)\n",
    "        Logger.log(f\"Combined Dataframe\")\n",
    "        Logger.log(df_to_upload.info())\n",
    "\n",
    "      else:\n",
    "        Logger.log(f\"Tab '{tab_name}' does not exist in the spreadsheet. Creating a new tab.\")\n",
    "        requests = [{\n",
    "            'addSheet': {\n",
    "                'properties': {\n",
    "                    'title': tab_name\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        batch_update_body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=batch_update_body\n",
    "        ).execute()\n",
    "        # Get the ID of the newly created sheet (optional, but useful)\n",
    "        new_sheet_id = response.get('replies')[0].get('addSheet').get('properties').get('sheetId')\n",
    "        Logger.log(f\"Successfully added new tab: '{tab_name}' with ID: {new_sheet_id}\")\n",
    "        df_to_upload = df\n",
    "\n",
    "      values = [df_to_upload.columns.tolist()] + df_to_upload.values.tolist()\n",
    "      Logger.log(f\"Uploading {len(df_to_upload)} rows to tab '{tab_name}'.\")\n",
    "      range_name = f\"'{tab_name}'!A1\" # Ensure tab name is quoted if it has spaces or special characters\n",
    "      body = {\n",
    "          'values': values\n",
    "      }\n",
    "      result = cls.service.spreadsheets().values().update(\n",
    "          spreadsheetId=spreadsheet_id,\n",
    "          range=range_name,\n",
    "          valueInputOption=valueInputOption,\n",
    "          body=body\n",
    "      ).execute()\n",
    "\n",
    "      Logger.log(f\"{result.get('updatedCells')} cells updated in tab '{tab_name}'.\")\n",
    "\n",
    "    except HttpError as err:\n",
    "      Logger.error(f\"An error occurred: {err}\")\n",
    "      if err.resp.status == 400: # Bad Request, often due to sheet name already existing\n",
    "        Logger.error(\"Error details: The tab name might already exist or the request is malformed.\")\n",
    "      elif err.resp.status == 403: # Forbidden, often due to incorrect permissions\n",
    "        Logger.error(\"Error details: Check your API permissions or if the service account/user has access to the sheet.\")\n",
    "      elif err.resp.status == 404: # Not Found, often due to incorrect spreadsheet ID\n",
    "        Logger.error(\"Error details: The spreadsheet ID might be incorrect.\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_spreadsheet_name_by_id(cls, spreadsheet_id):\n",
    "      \"\"\"\n",
    "      Retrieves the name (title) of a Google Spreadsheet given its ID.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id: The ID of the Google Spreadsheet.\n",
    "\n",
    "      Returns:\n",
    "          The title of the spreadsheet, or None if an error occurs or spreadsheet is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request the 'properties.title' field for efficiency\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='properties.title'\n",
    "          ).execute()\n",
    "\n",
    "          # Extract the title from the properties\n",
    "          title = spreadsheet_metadata.get('properties', {}).get('title')\n",
    "          return title\n",
    "      except HttpError as error:\n",
    "          print(f'An error occurred: {error}')\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found.\")\n",
    "          return None\n",
    "\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dropdown_to_range(cls, spreadsheet_id: str, sheet_id: str,\n",
    "                            dropdown_options: list,\n",
    "                            range_start_row: int, range_end_row: int,\n",
    "                            range_start_col: int, range_end_col: int):\n",
    "    requests = [\n",
    "        {\n",
    "            'setDataValidation': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': range_start_row,\n",
    "                    'endRowIndex': range_end_row,\n",
    "                    'startColumnIndex': range_start_col,\n",
    "                    'endColumnIndex': range_end_col\n",
    "                },\n",
    "                'rule': {\n",
    "                    'condition': {\n",
    "                        'type': 'ONE_OF_LIST',\n",
    "                        'values': [{'userEnteredValue': option} for option in dropdown_options]\n",
    "                    },\n",
    "                    'strict': True,  # Users can only enter values from the list\n",
    "                    'showCustomUi': True, # Show dropdown arrow\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # --- Execute the batch update request ---\n",
    "    try:\n",
    "        body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=body\n",
    "        ).execute()\n",
    "        print(f\"Dropdown added to Sheet ID {sheet_id}, Range row{range_start_row+1}:row{range_end_row}.\")\n",
    "        # You can inspect the response for more details if needed\n",
    "        # print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_id_by_name(cls, spreadsheet_id: str, tab_name: str):\n",
    "      \"\"\"\n",
    "      Retrieves the numerical Sheet ID (gid) for a given tab name within a spreadsheet.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id (str): The ID of the Google Spreadsheet.\n",
    "          tab_name (str): The exact name (title) of the tab/sheet to find.\n",
    "\n",
    "      Returns:\n",
    "          int: The numerical sheet ID (gid) if found.\n",
    "          None: If an error occurs, the spreadsheet is not found, or the tab name is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request 'sheets.properties' to get sheet IDs and titles efficiently\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='sheets.properties'\n",
    "          ).execute()\n",
    "\n",
    "          sheets = spreadsheet_metadata.get('sheets', [])\n",
    "          for sheet in sheets:\n",
    "              properties = sheet.get('properties')\n",
    "              # Check if properties exist and if the title matches the tab_name\n",
    "              if properties and properties.get('title') == tab_name:\n",
    "                  return properties.get('sheetId') # Return the sheetId (gid)\n",
    "\n",
    "          # If the loop completes, the tab was not found\n",
    "          print(f\"Tab '{tab_name}' not found in spreadsheet with ID '{spreadsheet_id}'.\")\n",
    "          return None\n",
    "      except HttpError as error:\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Error: {error}\")\n",
    "          else:\n",
    "              print(f'An HTTP error occurred: {error}')\n",
    "          return None\n",
    "      except Exception as e:\n",
    "          print(f\"An unexpected error occurred while retrieving sheet ID for tab '{tab_name}': {e}\")\n",
    "          return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download APIs Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_apis(VERSION=\"0.1.0\", download_datasets=False, save_directory='clean_workspace'):\n",
    "    import io\n",
    "    import os\n",
    "    import sys\n",
    "    import zipfile\n",
    "    import shutil\n",
    "    import re\n",
    "    # from google.colab import auth\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "    drive_service = GoogleDrive.service\n",
    "    # Version to download\n",
    "    # VERSION = \"0.0.rev22final\" # Version of the API\n",
    "    \n",
    "    # Define paths\n",
    "    CONTENT_DIR = os.path.join(save_directory, VERSION)\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        os.remove(CONTENT_DIR)\n",
    "    os.makedirs(CONTENT_DIR, exist_ok=True)\n",
    "    \n",
    "    APIS_DIR = os.path.join(CONTENT_DIR, 'APIs')\n",
    "    DBS_DIR = os.path.join(CONTENT_DIR, 'DBs')\n",
    "    SCRIPTS_DIR = os.path.join(CONTENT_DIR, 'Scripts')\n",
    "    FC_DIR = os.path.join(CONTENT_DIR, 'Schemas')\n",
    "    ZIP_PATH = os.path.join(CONTENT_DIR, f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # Google Drive Folder ID where versioned APIs zip files are stored\n",
    "    APIS_FOLDER_ID = '1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4'\n",
    "    \n",
    "    # List of items to extract from the zip file\n",
    "    ITEMS_TO_EXTRACT = ['APIs/', 'DBs/', 'Scripts/']\n",
    "    \n",
    "    # Clean up existing directories and files\n",
    "    for path in [APIS_DIR, DBS_DIR, SCRIPTS_DIR, FC_DIR, ZIP_PATH]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "    \n",
    "    # Authenticate and create the drive service\n",
    "    # auth.authenticate_user()\n",
    "    # drive_service = build('drive', 'v3')\n",
    "    # drive_service\n",
    "    # Helper function to download a file from Google Drive\n",
    "    def download_drive_file(service, file_id, output_path, file_name=None, show_progress=True):\n",
    "        \"\"\"Downloads a file from Google Drive\"\"\"\n",
    "        destination = output_path\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        with io.FileIO(destination, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if show_progress:\n",
    "                    print(f\"Download progress: {int(status.progress() * 100)}%\")\n",
    "    \n",
    "    \n",
    "    # 1. List files in the specified APIs folder\n",
    "    print(f\"Searching for APIs zip file with version {VERSION} in folder: {APIS_FOLDER_ID}...\")\n",
    "    apis_file_id = None\n",
    "    \n",
    "    try:\n",
    "        query = f\"'{APIS_FOLDER_ID}' in parents and trashed=false\"\n",
    "        results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "        files = results.get('files', [])\n",
    "        for file in files:\n",
    "            file_name = file.get('name', '')\n",
    "            if file_name.lower() == f'apis_v{VERSION.lower()}.zip':\n",
    "                apis_file_id = file.get('id')\n",
    "                print(f\"Found matching file: {file_name} (ID: {apis_file_id})\")\n",
    "                break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing files in Google Drive: {e}\")\n",
    "    \n",
    "    if not apis_file_id:\n",
    "        print(f\"Error: Could not find APIs zip file with version {VERSION} in the specified folder.\")\n",
    "        sys.exit(\"Required APIs zip file not found.\")\n",
    "    \n",
    "    # 2. Download the found APIs zip file\n",
    "    print(f\"Downloading APIs zip file with ID: {apis_file_id}...\")\n",
    "    download_drive_file(drive_service, apis_file_id, ZIP_PATH, file_name=f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # 3. Extract specific items from the zip file to /content\n",
    "    print(f\"Extracting specific items from {ZIP_PATH} to {CONTENT_DIR}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_contents = zip_ref.namelist()\n",
    "    \n",
    "            for member in zip_contents:\n",
    "                extracted = False\n",
    "                for item_prefix in ITEMS_TO_EXTRACT:\n",
    "                  if member == item_prefix or member.startswith(item_prefix):\n",
    "                        zip_ref.extract(member, CONTENT_DIR)\n",
    "                        extracted = True\n",
    "                        break\n",
    "    \n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The downloaded file at {ZIP_PATH} is not a valid zip file.\")\n",
    "        sys.exit(\"Invalid zip file downloaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "        sys.exit(\"Extraction failed.\")\n",
    "    \n",
    "    \n",
    "    # 4. Clean up\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)\n",
    "    \n",
    "    # 5. Add APIs to path\n",
    "    if os.path.exists(APIS_DIR):\n",
    "        sys.path.append(APIS_DIR)\n",
    "    else:\n",
    "        print(f\"Error: APIS directory not found at {APIS_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    # 6. Quick verification\n",
    "    # Check for the presence of the extracted items\n",
    "    verification_paths = [APIS_DIR, DBS_DIR, SCRIPTS_DIR]\n",
    "    all_present = True\n",
    "    print(\"\\nVerifying extracted items:\")\n",
    "    for path in verification_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✅ {path} is present.\")\n",
    "        else:\n",
    "            print(f\"❌ {path} is MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(f\"\\n✅ Setup complete! Required items extracted to {CONTENT_DIR}.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Setup failed! Not all required items were extracted.\")\n",
    "\n",
    "    # 7. Generate Schemas\n",
    "\n",
    "    # Add Scripts to path\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        sys.path.append(CONTENT_DIR)\n",
    "    else:\n",
    "        print(f\"Error: CONTENT_DIR directory not found at {CONTENT_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    from Scripts.FCSpec import generate_package_schema\n",
    "    \n",
    "    print(\"\\nGenerating FC Schemas\")\n",
    "    os.makedirs(FC_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Iterate through the packages in the /content/APIs directory\n",
    "    for package_name in os.listdir(APIS_DIR):\n",
    "        package_path = os.path.join(APIS_DIR, package_name)\n",
    "    \n",
    "        # Check if it's a directory (to avoid processing files)\n",
    "        if os.path.isdir(package_path):\n",
    "            # Call the function to generate schema for the current package\n",
    "            generate_package_schema(package_path, output_folder_path=FC_DIR)\n",
    "    print(f\"✅ Successfully generated {len(os.listdir(FC_DIR))} FC Schemas to {FC_DIR}\")\n",
    "\n",
    "    if download_datasets:\n",
    "        def download_drive_folder(service, folder_id, destination_path):\n",
    "            \"\"\"\n",
    "            Recursively downloads all files in a Google Drive folder using the `download_drive_file`\n",
    "            \"\"\"\n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "        \n",
    "            query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "            page_token = None\n",
    "        \n",
    "            while True:\n",
    "                results = service.files().list(\n",
    "                    q=query,\n",
    "                    spaces='drive',\n",
    "                    fields='nextPageToken, files(id, name, mimeType)',\n",
    "                    pageToken=page_token\n",
    "                ).execute()\n",
    "        \n",
    "                for item in results.get('files', []):\n",
    "                    file_id = item['id']\n",
    "                    file_name = item['name']\n",
    "                    mime_type = item['mimeType']\n",
    "        \n",
    "                    if mime_type == 'application/vnd.google-apps.folder':\n",
    "                        # Recursively download subfolders\n",
    "                        new_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Creating subfolder and downloading: {new_path}\")\n",
    "                        download_drive_folder(service, file_id, new_path)\n",
    "                    else:\n",
    "                        # Construct full file path and pass it as output_path\n",
    "                        full_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Downloading file: {file_name} to {full_path}\")\n",
    "                        download_drive_file(service, file_id, full_path, file_name=file_name, show_progress=False)\n",
    "        \n",
    "                page_token = results.get('nextPageToken', None)\n",
    "                if not page_token:\n",
    "                    break\n",
    "        \n",
    "        # --- Configuration for Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'Quotewk.csv' file.\n",
    "        FOLDER_ID = \"1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml\"\n",
    "        # DATASET_FOLDER = \"./workspace/Datasets\"\n",
    "        DATASET_FOLDER = os.path.join(CONTENT_DIR, 'workspace/Datasets')\n",
    "        \n",
    "        print(f\"Starting download of folder {FOLDER_ID} to {DATASET_FOLDER}...\")\n",
    "        download_drive_folder(drive_service, FOLDER_ID, DATASET_FOLDER)\n",
    "        print(\"Dataset download complete.\")\n",
    "\n",
    "        # --- Configuration for WS Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'WS Multihop Datasets' file.\n",
    "        WS_DATA_ID = \"1kmXZ1oarBPlE0OQL52eGoc1xPbupJ1n9\"\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        \n",
    "        print(f\"Downloading WS Dataset zip file with ID: {WS_DATA_ID}...\")\n",
    "        download_drive_file(drive_service, WS_DATA_ID, WS_DATA_ZIP_PATH, file_name=f'WS_DATA.zip')\n",
    "        print(\"Dataset download complete.\")\n",
    "        \n",
    "        # Extract the Datasets\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        with zipfile.ZipFile(WS_DATA_ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(CONTENT_DIR)\n",
    "        print(f\"Extracted to {CONTENT_DIR}\")\n",
    "        \n",
    "        # Moving 'file_dataset_pb2.py' to root directory\n",
    "        src_path = os.path.join(CONTENT_DIR, 'WS_DATA', 'file_dataset_pb2.py')\n",
    "        dst_path = os.path.join(CONTENT_DIR, 'file_dataset_pb2.py')\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"Moved {src_path} to {dst_path}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {src_path}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(WS_DATA_ZIP_PATH):\n",
    "            os.remove(WS_DATA_ZIP_PATH)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for APIs zip file with version 0.1.0 in folder: 1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4...\n",
      "Found matching file: APIs_V0.1.0.zip (ID: 1hLV2slrHhH0RquKU-8oWRJRs_nHh5CT_)\n",
      "Downloading APIs zip file with ID: 1hLV2slrHhH0RquKU-8oWRJRs_nHh5CT_...\n",
      "Download progress: 100%\n",
      "Extracting specific items from clean_workspace/0.1.0/APIs_V0.1.0.zip to clean_workspace/0.1.0...\n",
      "\n",
      "Verifying extracted items:\n",
      "✅ clean_workspace/0.1.0/APIs is present.\n",
      "✅ clean_workspace/0.1.0/DBs is present.\n",
      "✅ clean_workspace/0.1.0/Scripts is present.\n",
      "\n",
      "✅ Setup complete! Required items extracted to clean_workspace/0.1.0.\n",
      "\n",
      "Generating FC Schemas\n",
      "✅ notes_and_lists Schema generation complete: clean_workspace/0.1.0/Schemas/notes_and_lists.json\n",
      "\n",
      "\n",
      "Processing mutation notes_and_lists.mutations.m01...\n",
      "✅ notes_and_lists.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/notes_and_lists.json\n",
      "\n",
      "✅ google_maps Schema generation complete: clean_workspace/0.1.0/Schemas/google_maps.json\n",
      "\n",
      "\n",
      "Processing mutation google_maps.mutations.m01...\n",
      "✅ google_maps.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_maps.json\n",
      "\n",
      "Error: Could not find a valid _function_map in clean_workspace/0.1.0/APIs/common_utils/__init__.py.\n",
      "✅ google_home Schema generation complete: clean_workspace/0.1.0/Schemas/google_home.json\n",
      "\n",
      "\n",
      "Processing mutation google_home.mutations.m01...\n",
      "✅ google_home.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_home.json\n",
      "\n",
      "✅ google_calendar Schema generation complete: clean_workspace/0.1.0/Schemas/google_calendar.json\n",
      "\n",
      "\n",
      "Processing mutation google_calendar.mutations.m01...\n",
      "✅ google_calendar.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_calendar.json\n",
      "\n",
      "✅ device_setting Schema generation complete: clean_workspace/0.1.0/Schemas/device_setting.json\n",
      "\n",
      "\n",
      "Processing mutation device_setting.mutations.m01...\n",
      "✅ device_setting.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/device_setting.json\n",
      "\n",
      "✅ call_llm Schema generation complete: clean_workspace/0.1.0/Schemas/call_llm.json\n",
      "\n",
      "\n",
      "Processing mutation call_llm.mutations.m01...\n",
      "✅ call_llm.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/call_llm.json\n",
      "\n",
      "✅ generic_media Schema generation complete: clean_workspace/0.1.0/Schemas/generic_media.json\n",
      "\n",
      "\n",
      "Processing mutation generic_media.mutations.m01...\n",
      "✅ generic_media.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/generic_media.json\n",
      "\n",
      "✅ messages Schema generation complete: clean_workspace/0.1.0/Schemas/messages.json\n",
      "\n",
      "\n",
      "Processing mutation messages.mutations.m01...\n",
      "✅ messages.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/messages.json\n",
      "\n",
      "✅ google_sheets Schema generation complete: clean_workspace/0.1.0/Schemas/google_sheets.json\n",
      "\n",
      "\n",
      "Processing mutation google_sheets.mutations.m01...\n",
      "✅ google_sheets.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_sheets.json\n",
      "\n",
      "✅ google_chat Schema generation complete: clean_workspace/0.1.0/Schemas/google_chat.json\n",
      "\n",
      "\n",
      "Processing mutation google_chat.mutations.m01...\n",
      "✅ google_chat.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_chat.json\n",
      "\n",
      "✅ google_cloud_storage Schema generation complete: clean_workspace/0.1.0/Schemas/google_cloud_storage.json\n",
      "\n",
      "\n",
      "Processing mutation google_cloud_storage.mutations.m01...\n",
      "✅ google_cloud_storage.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_cloud_storage.json\n",
      "\n",
      "✅ puppeteer Schema generation complete: clean_workspace/0.1.0/Schemas/puppeteer.json\n",
      "\n",
      "\n",
      "Processing mutation puppeteer.mutations.m01...\n",
      "Error: Could not find a valid _function_map in clean_workspace/0.1.0/APIs/puppeteer/mutations/m01/__init__.py.\n",
      "✅ copilot Schema generation complete: clean_workspace/0.1.0/Schemas/copilot.json\n",
      "\n",
      "\n",
      "Processing mutation copilot.mutations.m01...\n",
      "✅ copilot.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/copilot.json\n",
      "\n",
      "✅ cursor Schema generation complete: clean_workspace/0.1.0/Schemas/cursor.json\n",
      "\n",
      "\n",
      "Processing mutation cursor.mutations.m01...\n",
      "✅ cursor.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/cursor.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/cursor/mutations/m01/cursorAPI.py:128: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ workday Schema generation complete: clean_workspace/0.1.0/Schemas/workday.json\n",
      "\n",
      "\n",
      "Processing mutation workday.mutations.m01...\n",
      "✅ workday.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/workday.json\n",
      "\n",
      "✅ azure Schema generation complete: clean_workspace/0.1.0/Schemas/azure.json\n",
      "\n",
      "\n",
      "Processing mutation azure.mutations.m01...\n",
      "✅ azure.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/azure.json\n",
      "\n",
      "✅ media_control Schema generation complete: clean_workspace/0.1.0/Schemas/media_control.json\n",
      "\n",
      "\n",
      "Processing mutation media_control.mutations.m01...\n",
      "✅ media_control.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/media_control.json\n",
      "\n",
      "✅ google_meet Schema generation complete: clean_workspace/0.1.0/Schemas/google_meet.json\n",
      "\n",
      "\n",
      "Processing mutation google_meet.mutations.m01...\n",
      "✅ google_meet.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_meet.json\n",
      "\n",
      "✅ google_maps_live Schema generation complete: clean_workspace/0.1.0/Schemas/google_maps_live.json\n",
      "\n",
      "\n",
      "Processing mutation google_maps_live.mutations.m01...\n",
      "✅ google_maps_live.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_maps_live.json\n",
      "\n",
      "✅ contacts Schema generation complete: clean_workspace/0.1.0/Schemas/contacts.json\n",
      "\n",
      "\n",
      "Processing mutation contacts.mutations.m01...\n",
      "✅ contacts.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/contacts.json\n",
      "\n",
      "✅ spotify Schema generation complete: clean_workspace/0.1.0/Schemas/spotify.json\n",
      "\n",
      "\n",
      "Processing mutation spotify.mutations.m01...\n",
      "✅ spotify.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/spotify.json\n",
      "\n",
      "✅ canva Schema generation complete: clean_workspace/0.1.0/Schemas/canva.json\n",
      "\n",
      "\n",
      "Processing mutation canva.mutations.m01...\n",
      "✅ canva.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/canva.json\n",
      "\n",
      "✅ shopify Schema generation complete: clean_workspace/0.1.0/Schemas/shopify.json\n",
      "\n",
      "\n",
      "Processing mutation shopify.mutations.m01...\n",
      "✅ shopify.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/shopify.json\n",
      "\n",
      "✅ terminal Schema generation complete: clean_workspace/0.1.0/Schemas/terminal.json\n",
      "\n",
      "\n",
      "Processing mutation terminal.mutations.m01...\n",
      "✅ terminal.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/terminal.json\n",
      "\n",
      "✅ youtube_tool Schema generation complete: clean_workspace/0.1.0/Schemas/youtube_tool.json\n",
      "\n",
      "\n",
      "Processing mutation youtube_tool.mutations.m01...\n",
      "✅ youtube_tool.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/youtube_tool.json\n",
      "\n",
      "✅ tool_explorer Schema generation complete: clean_workspace/0.1.0/Schemas/tool_explorer.json\n",
      "\n",
      "\n",
      "Processing mutation tool_explorer.mutations.m01...\n",
      "✅ tool_explorer.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/tool_explorer.json\n",
      "\n",
      "✅ retail Schema generation complete: clean_workspace/0.1.0/Schemas/retail.json\n",
      "\n",
      "\n",
      "Processing mutation retail.mutations.m01...\n",
      "✅ retail.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/retail.json\n",
      "\n",
      "\n",
      "Processing mutation retail.mutations.smaller_toolset...\n",
      "✅ retail.mutations.smaller_toolset Schema generation complete: clean_workspace/0.1.0/MutationSchemas/smaller_toolset/retail.json\n",
      "\n",
      "✅ bigquery Schema generation complete: clean_workspace/0.1.0/Schemas/bigquery.json\n",
      "\n",
      "\n",
      "Processing mutation bigquery.mutations.m01...\n",
      "✅ bigquery.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/bigquery.json\n",
      "\n",
      "✅ google_docs Schema generation complete: clean_workspace/0.1.0/Schemas/google_docs.json\n",
      "\n",
      "\n",
      "Processing mutation google_docs.mutations.m01...\n",
      "✅ google_docs.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_docs.json\n",
      "\n",
      "✅ gdrive Schema generation complete: clean_workspace/0.1.0/Schemas/gdrive.json\n",
      "\n",
      "\n",
      "Processing mutation gdrive.mutations.m01...\n",
      "✅ gdrive.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/gdrive.json\n",
      "\n",
      "✅ youtube Schema generation complete: clean_workspace/0.1.0/Schemas/youtube.json\n",
      "\n",
      "\n",
      "Processing mutation youtube.mutations.m01...\n",
      "✅ youtube.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/youtube.json\n",
      "\n",
      "✅ google_slides Schema generation complete: clean_workspace/0.1.0/Schemas/google_slides.json\n",
      "\n",
      "\n",
      "Processing mutation google_slides.mutations.m01...\n",
      "✅ google_slides.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_slides.json\n",
      "\n",
      "✅ mongodb Schema generation complete: clean_workspace/0.1.0/Schemas/mongodb.json\n",
      "\n",
      "\n",
      "Processing mutation mongodb.mutations.m01...\n",
      "✅ mongodb.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/mongodb.json\n",
      "\n",
      "✅ sapconcur Schema generation complete: clean_workspace/0.1.0/Schemas/sapconcur.json\n",
      "\n",
      "\n",
      "Processing mutation sapconcur.mutations.m01...\n",
      "✅ sapconcur.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/sapconcur.json\n",
      "\n",
      "✅ device_actions Schema generation complete: clean_workspace/0.1.0/Schemas/device_actions.json\n",
      "\n",
      "\n",
      "Processing mutation device_actions.mutations.m01...\n",
      "✅ device_actions.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/device_actions.json\n",
      "\n",
      "✅ supabase Schema generation complete: clean_workspace/0.1.0/Schemas/supabase.json\n",
      "\n",
      "\n",
      "Processing mutation supabase.mutations.m01...\n",
      "✅ supabase.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/supabase.json\n",
      "\n",
      "✅ hubspot Schema generation complete: clean_workspace/0.1.0/Schemas/hubspot.json\n",
      "\n",
      "\n",
      "Processing mutation hubspot.mutations.m01...\n",
      "✅ hubspot.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/hubspot.json\n",
      "\n",
      "✅ google_search Schema generation complete: clean_workspace/0.1.0/Schemas/google_search.json\n",
      "\n",
      "\n",
      "Processing mutation google_search.mutations.m01...\n",
      "✅ google_search.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_search.json\n",
      "\n",
      "✅ gemini_cli Schema generation complete: clean_workspace/0.1.0/Schemas/gemini_cli.json\n",
      "\n",
      "\n",
      "Processing mutation gemini_cli.mutations.m01...\n",
      "✅ gemini_cli.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/gemini_cli.json\n",
      "\n",
      "✅ home_assistant Schema generation complete: clean_workspace/0.1.0/Schemas/home_assistant.json\n",
      "\n",
      "\n",
      "Processing mutation home_assistant.mutations.m01...\n",
      "✅ home_assistant.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/home_assistant.json\n",
      "\n",
      "✅ jira Schema generation complete: clean_workspace/0.1.0/Schemas/jira.json\n",
      "\n",
      "\n",
      "Processing mutation jira.mutations.m01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/gemini_cli/mutations/m01/file_system_api.py:162: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/gemini_cli/mutations/m01/file_system_api.py:162: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/gemini_cli/mutations/m01/file_system_api.py:162: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/gemini_cli/mutations/m01/file_system_api.py:162: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/gemini_cli/mutations/m01/file_system_api.py:162: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/Users/nabeel/PycharmProjects/e2e_sanity_checks/clean_workspace/0.1.0/APIs/gemini_cli/mutations/m01/file_system_api.py:162: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ jira.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/jira.json\n",
      "\n",
      "✅ github Schema generation complete: clean_workspace/0.1.0/Schemas/github.json\n",
      "\n",
      "\n",
      "Processing mutation github.mutations.m01...\n",
      "✅ github.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/github.json\n",
      "\n",
      "✅ figma Schema generation complete: clean_workspace/0.1.0/Schemas/figma.json\n",
      "\n",
      "\n",
      "Processing mutation figma.mutations.m01...\n",
      "✅ figma.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/figma.json\n",
      "\n",
      "✅ github_actions Schema generation complete: clean_workspace/0.1.0/Schemas/github_actions.json\n",
      "\n",
      "\n",
      "Processing mutation github_actions.mutations.m01...\n",
      "✅ github_actions.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/github_actions.json\n",
      "\n",
      "✅ zendesk Schema generation complete: clean_workspace/0.1.0/Schemas/zendesk.json\n",
      "\n",
      "\n",
      "Processing mutation zendesk.mutations.m01...\n",
      "✅ zendesk.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/zendesk.json\n",
      "\n",
      "✅ google_people Schema generation complete: clean_workspace/0.1.0/Schemas/google_people.json\n",
      "\n",
      "\n",
      "Processing mutation google_people.mutations.m01...\n",
      "✅ google_people.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/google_people.json\n",
      "\n",
      "✅ mysql Schema generation complete: clean_workspace/0.1.0/Schemas/mysql.json\n",
      "\n",
      "\n",
      "Processing mutation mysql.mutations.m01...\n",
      "✅ mysql.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/mysql.json\n",
      "\n",
      "✅ reddit Schema generation complete: clean_workspace/0.1.0/Schemas/reddit.json\n",
      "\n",
      "\n",
      "Processing mutation reddit.mutations.m01...\n",
      "✅ reddit.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/reddit.json\n",
      "\n",
      "✅ confluence Schema generation complete: clean_workspace/0.1.0/Schemas/confluence.json\n",
      "\n",
      "\n",
      "Processing mutation confluence.mutations.m01...\n",
      "✅ confluence.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/confluence.json\n",
      "\n",
      "✅ phone Schema generation complete: clean_workspace/0.1.0/Schemas/phone.json\n",
      "\n",
      "\n",
      "Processing mutation phone.mutations.m01...\n",
      "✅ phone.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/phone.json\n",
      "\n",
      "✅ sdm Schema generation complete: clean_workspace/0.1.0/Schemas/sdm.json\n",
      "\n",
      "\n",
      "Processing mutation sdm.mutations.m01...\n",
      "✅ sdm.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/sdm.json\n",
      "\n",
      "✅ salesforce Schema generation complete: clean_workspace/0.1.0/Schemas/salesforce.json\n",
      "\n",
      "\n",
      "Processing mutation salesforce.mutations.m01...\n",
      "✅ salesforce.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/salesforce.json\n",
      "\n",
      "✅ slack Schema generation complete: clean_workspace/0.1.0/Schemas/slack.json\n",
      "\n",
      "\n",
      "Processing mutation slack.mutations.m01...\n",
      "✅ slack.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/slack.json\n",
      "\n",
      "✅ tiktok Schema generation complete: clean_workspace/0.1.0/Schemas/tiktok.json\n",
      "\n",
      "\n",
      "Processing mutation tiktok.mutations.m01...\n",
      "✅ tiktok.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/tiktok.json\n",
      "\n",
      "✅ service_template Schema generation complete: clean_workspace/0.1.0/Schemas/service_template.json\n",
      "\n",
      "\n",
      "Processing mutation service_template.mutations.m01...\n",
      "✅ service_template.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/service_template.json\n",
      "\n",
      "✅ code_execution Schema generation complete: clean_workspace/0.1.0/Schemas/code_execution.json\n",
      "\n",
      "\n",
      "Processing mutation code_execution.mutations.m01...\n",
      "✅ code_execution.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/code_execution.json\n",
      "\n",
      "✅ blender Schema generation complete: clean_workspace/0.1.0/Schemas/blender.json\n",
      "\n",
      "\n",
      "Processing mutation blender.mutations.m01...\n",
      "✅ blender.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/blender.json\n",
      "\n",
      "✅ authentication Schema generation complete: clean_workspace/0.1.0/Schemas/authentication.json\n",
      "\n",
      "\n",
      "Processing mutation authentication.mutations.m01...\n",
      "✅ authentication.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/authentication.json\n",
      "\n",
      "✅ airline Schema generation complete: clean_workspace/0.1.0/Schemas/airline.json\n",
      "\n",
      "\n",
      "Processing mutation airline.mutations.m01...\n",
      "✅ airline.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/airline.json\n",
      "\n",
      "✅ instagram Schema generation complete: clean_workspace/0.1.0/Schemas/instagram.json\n",
      "\n",
      "\n",
      "Processing mutation instagram.mutations.m01...\n",
      "✅ instagram.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/instagram.json\n",
      "\n",
      "✅ generic_reminders Schema generation complete: clean_workspace/0.1.0/Schemas/generic_reminders.json\n",
      "\n",
      "\n",
      "Processing mutation generic_reminders.mutations.m01...\n",
      "✅ generic_reminders.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/generic_reminders.json\n",
      "\n",
      "✅ notifications Schema generation complete: clean_workspace/0.1.0/Schemas/notifications.json\n",
      "\n",
      "\n",
      "Processing mutation notifications.mutations.m01...\n",
      "✅ notifications.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/notifications.json\n",
      "\n",
      "✅ gmail Schema generation complete: clean_workspace/0.1.0/Schemas/gmail.json\n",
      "\n",
      "\n",
      "Processing mutation gmail.mutations.m01...\n",
      "✅ gmail.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/gmail.json\n",
      "\n",
      "✅ whatsapp Schema generation complete: clean_workspace/0.1.0/Schemas/whatsapp.json\n",
      "\n",
      "\n",
      "Processing mutation whatsapp.mutations.m01...\n",
      "✅ whatsapp.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/whatsapp.json\n",
      "\n",
      "✅ clock Schema generation complete: clean_workspace/0.1.0/Schemas/clock.json\n",
      "\n",
      "\n",
      "Processing mutation clock.mutations.m01...\n",
      "✅ clock.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/clock.json\n",
      "\n",
      "✅ linkedin Schema generation complete: clean_workspace/0.1.0/Schemas/linkedin.json\n",
      "\n",
      "\n",
      "Processing mutation linkedin.mutations.m01...\n",
      "✅ linkedin.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/linkedin.json\n",
      "\n",
      "✅ stripe Schema generation complete: clean_workspace/0.1.0/Schemas/stripe.json\n",
      "\n",
      "\n",
      "Processing mutation stripe.mutations.m01...\n",
      "✅ stripe.mutations.m01 Schema generation complete: clean_workspace/0.1.0/MutationSchemas/m01/stripe.json\n",
      "\n",
      "✅ Successfully generated 67 FC Schemas to clean_workspace/0.1.0/Schemas\n",
      "Starting download of folder 1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml to clean_workspace/0.1.0/workspace/Datasets...\n",
      "Downloading file: ICEQueries_Files_md5Checksum_17_July_2025.csv to clean_workspace/0.1.0/workspace/Datasets/ICEQueries_Files_md5Checksum_17_July_2025.csv\n",
      "Downloading file: B Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/B Plan Budget.csv\n",
      "Downloading file: Operatorsexcavatio.csv to clean_workspace/0.1.0/workspace/Datasets/Operatorsexcavatio.csv\n",
      "Downloading file: libro_gastronomia.csv to clean_workspace/0.1.0/workspace/Datasets/libro_gastronomia.csv\n",
      "Downloading file: September-November 2021 Billing.csv to clean_workspace/0.1.0/workspace/Datasets/September-November 2021 Billing.csv\n",
      "Downloading file: NOVEMBER MP REPORT - axel bond.csv to clean_workspace/0.1.0/workspace/Datasets/NOVEMBER MP REPORT - axel bond.csv\n",
      "Downloading file: monte_sells.csv to clean_workspace/0.1.0/workspace/Datasets/monte_sells.csv\n",
      "Downloading file: PhenolicBoards.csv to clean_workspace/0.1.0/workspace/Datasets/PhenolicBoards.csv\n",
      "Downloading file: Company_finances_markup.csv to clean_workspace/0.1.0/workspace/Datasets/Company_finances_markup.csv\n",
      "Downloading file: prices_chapas.csv to clean_workspace/0.1.0/workspace/Datasets/prices_chapas.csv\n",
      "Downloading file: MARKAS S.A. BOX Sales.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Sales.csv\n",
      "Downloading file: Waiter_Hourly_Schedule.csv to clean_workspace/0.1.0/workspace/Datasets/Waiter_Hourly_Schedule.csv\n",
      "Downloading file: Untitled spreadsheet - Jill Norton.csv to clean_workspace/0.1.0/workspace/Datasets/Untitled spreadsheet - Jill Norton.csv\n",
      "Downloading file: Services - Dawn Alderson.csv to clean_workspace/0.1.0/workspace/Datasets/Services - Dawn Alderson.csv\n",
      "Downloading file: SC fancy customer survey Aug 2023_Nov 2023... - Aurore Munyeshyaka.csv to clean_workspace/0.1.0/workspace/Datasets/SC fancy customer survey Aug 2023_Nov 2023... - Aurore Munyeshyaka.csv\n",
      "Downloading file: base holii matterport - carla mccadden.csv to clean_workspace/0.1.0/workspace/Datasets/base holii matterport - carla mccadden.csv\n",
      "Downloading file: Vouchers_Dec_2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vouchers_Dec_2023.csv\n",
      "Downloading file: Book_Classes.csv to clean_workspace/0.1.0/workspace/Datasets/Book_Classes.csv\n",
      "Downloading file: ExpensesSushiRest.csv to clean_workspace/0.1.0/workspace/Datasets/ExpensesSushiRest.csv\n",
      "Downloading file: EMERGENCIA 2023 YAKU.csv to clean_workspace/0.1.0/workspace/Datasets/EMERGENCIA 2023 YAKU.csv\n",
      "Downloading file: prices_chapas.csv to clean_workspace/0.1.0/workspace/Datasets/prices_chapas.csv\n",
      "Downloading file: monte_sells.csv to clean_workspace/0.1.0/workspace/Datasets/monte_sells.csv\n",
      "Downloading file: cdlm_purchases.csv to clean_workspace/0.1.0/workspace/Datasets/cdlm_purchases.csv\n",
      "Downloading file: invoices_tracking.csv to clean_workspace/0.1.0/workspace/Datasets/invoices_tracking.csv\n",
      "Downloading file: ControlPanel.csv to clean_workspace/0.1.0/workspace/Datasets/ControlPanel.csv\n",
      "Downloading file: MARKAS S.A. BOX Sales.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Sales.csv\n",
      "Downloading file: W Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/W Plan Budget.csv\n",
      "Downloading file: trash_valet - Jordan Layne.csv to clean_workspace/0.1.0/workspace/Datasets/trash_valet - Jordan Layne.csv\n",
      "Downloading file: P Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/P Plan Budget.csv\n",
      "Downloading file: PhenolicBoards.csv to clean_workspace/0.1.0/workspace/Datasets/PhenolicBoards.csv\n",
      "Downloading file: Final_sales.csv to clean_workspace/0.1.0/workspace/Datasets/Final_sales.csv\n",
      "Downloading file: base holii matterport - carla mccadden.csv to clean_workspace/0.1.0/workspace/Datasets/base holii matterport - carla mccadden.csv\n",
      "Downloading file: Decoration_Items_List_Abril.csv to clean_workspace/0.1.0/workspace/Datasets/Decoration_Items_List_Abril.csv\n",
      "Downloading file: BestBatterEtsySales  - Renae Fultz.csv to clean_workspace/0.1.0/workspace/Datasets/BestBatterEtsySales  - Renae Fultz.csv\n",
      "Downloading file: Services - Dawn Alderson.csv to clean_workspace/0.1.0/workspace/Datasets/Services - Dawn Alderson.csv\n",
      "Downloading file: price_list_2023.csv to clean_workspace/0.1.0/workspace/Datasets/price_list_2023.csv\n",
      "Downloading file: Decoration_Items_List_Abril.csv to clean_workspace/0.1.0/workspace/Datasets/Decoration_Items_List_Abril.csv\n",
      "Downloading file: MARKAS S.A. BOX Sales.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Sales.csv\n",
      "Downloading file: Earnings Report 2023-01-01 to 2023-03-31 - Ryan Meza.csv to clean_workspace/0.1.0/workspace/Datasets/Earnings Report 2023-01-01 to 2023-03-31 - Ryan Meza.csv\n",
      "Downloading file: September-November 2021 Billing.csv to clean_workspace/0.1.0/workspace/Datasets/September-November 2021 Billing.csv\n",
      "Downloading file: FINANCIAL TRANSACTIONS.csv to clean_workspace/0.1.0/workspace/Datasets/FINANCIAL TRANSACTIONS.csv\n",
      "Downloading file: supermarket_stock_december.csv to clean_workspace/0.1.0/workspace/Datasets/supermarket_stock_december.csv\n",
      "Downloading file: Cards Movements.csv to clean_workspace/0.1.0/workspace/Datasets/Cards Movements.csv\n",
      "Downloading file: Untitled spreadsheet - Jill Norton.csv to clean_workspace/0.1.0/workspace/Datasets/Untitled spreadsheet - Jill Norton.csv\n",
      "Downloading file: supermarket_sales_tffb.csv to clean_workspace/0.1.0/workspace/Datasets/supermarket_sales_tffb.csv\n",
      "Downloading file: POPEYES_ANNUALIZED_PROFIT.csv to clean_workspace/0.1.0/workspace/Datasets/POPEYES_ANNUALIZED_PROFIT.csv\n",
      "Downloading file: price_list_2023.csv to clean_workspace/0.1.0/workspace/Datasets/price_list_2023.csv\n",
      "Downloading file: Vouchers_Dec_2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vouchers_Dec_2023.csv\n",
      "Downloading file: e_com.csv to clean_workspace/0.1.0/workspace/Datasets/e_com.csv\n",
      "Downloading file: envios_ventas_exterior.csv to clean_workspace/0.1.0/workspace/Datasets/envios_ventas_exterior.csv\n",
      "Downloading file: cash flow GapIng.csv to clean_workspace/0.1.0/workspace/Datasets/cash flow GapIng.csv\n",
      "Downloading file: cash flow GapIng.csv to clean_workspace/0.1.0/workspace/Datasets/cash flow GapIng.csv\n",
      "Creating subfolder and downloading: clean_workspace/0.1.0/workspace/Datasets/Whole set\n",
      "Downloading file: Quotewk.csv to clean_workspace/0.1.0/workspace/Datasets/Quotewk.csv\n",
      "Downloading file: ICEQueries_Files_md5Checksum.csv to clean_workspace/0.1.0/workspace/Datasets/ICEQueries_Files_md5Checksum.csv\n",
      "Downloading file: torque and flow performance.csv to clean_workspace/0.1.0/workspace/Datasets/torque and flow performance.csv\n",
      "Downloading file: eBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv to clean_workspace/0.1.0/workspace/Datasets/eBay-ListingsSalesReport-Dec-22-2023-08_21_39-0700-12145337749 - Karina Arango.csv\n",
      "Downloading file: visits_2023-11-01 - Spencer Miller.csv to clean_workspace/0.1.0/workspace/Datasets/visits_2023-11-01 - Spencer Miller.csv\n",
      "Downloading file: CLIENTS AB USD_CLEAN.csv to clean_workspace/0.1.0/workspace/Datasets/CLIENTS AB USD_CLEAN.csv\n",
      "Downloading file: Sales_Report.csv to clean_workspace/0.1.0/workspace/Datasets/Sales_Report.csv\n",
      "Downloading file: Truck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv to clean_workspace/0.1.0/workspace/Datasets/Truck Repairs for 2023 - Sheet1 - Dr. Victoria Gamble.csv\n",
      "Downloading file: prodqualityanalysis.csv to clean_workspace/0.1.0/workspace/Datasets/prodqualityanalysis.csv\n",
      "Downloading file: GARDEN LOG - Visits (1) - Erica Redling.csv to clean_workspace/0.1.0/workspace/Datasets/GARDEN LOG - Visits (1) - Erica Redling.csv\n",
      "Downloading file: business users.csv to clean_workspace/0.1.0/workspace/Datasets/business users.csv\n",
      "Downloading file: Earnings And Placements - statement.csv - Jonathan Foster.csv to clean_workspace/0.1.0/workspace/Datasets/Earnings And Placements - statement.csv - Jonathan Foster.csv\n",
      "Downloading file: Clockify Detailed Time Report - 2023.csv to clean_workspace/0.1.0/workspace/Datasets/Clockify Detailed Time Report - 2023.csv\n",
      "Downloading file: Retrogasm sales 2023 1 - Rachel Robinson.csv to clean_workspace/0.1.0/workspace/Datasets/Retrogasm sales 2023 1 - Rachel Robinson.csv\n",
      "Downloading file: Construction_costs_delivery.csv to clean_workspace/0.1.0/workspace/Datasets/Construction_costs_delivery.csv\n",
      "Downloading file: Expenses_20152.csv to clean_workspace/0.1.0/workspace/Datasets/Expenses_20152.csv\n",
      "Downloading file: Vat_sales_ledger 02-2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vat_sales_ledger 02-2023.csv\n",
      "Downloading file: Salary and Equity Data.csv to clean_workspace/0.1.0/workspace/Datasets/Salary and Equity Data.csv\n",
      "Downloading file: 2020 sales - Shannon O.csv to clean_workspace/0.1.0/workspace/Datasets/2020 sales - Shannon O.csv\n",
      "Downloading file: Comparison with Wal Mart Catalog Stock.csv to clean_workspace/0.1.0/workspace/Datasets/Comparison with Wal Mart Catalog Stock.csv\n",
      "Downloading file: burbujas_sales_july.csv to clean_workspace/0.1.0/workspace/Datasets/burbujas_sales_july.csv\n",
      "Downloading file: Wood purchased - Patrick Bernier.csv to clean_workspace/0.1.0/workspace/Datasets/Wood purchased - Patrick Bernier.csv\n",
      "Downloading file: Dairy Solids (fat+protein).csv to clean_workspace/0.1.0/workspace/Datasets/Dairy Solids (fat+protein).csv\n",
      "Downloading file: Prices July 2022 EQUILIBRIO.csv to clean_workspace/0.1.0/workspace/Datasets/Prices July 2022 EQUILIBRIO.csv\n",
      "Downloading file: payroll_sheet.csv to clean_workspace/0.1.0/workspace/Datasets/payroll_sheet.csv\n",
      "Downloading file: [External] order_items.csv to clean_workspace/0.1.0/workspace/Datasets/[External] order_items.csv\n",
      "Downloading file: orders - orders - Kimm Topping.csv to clean_workspace/0.1.0/workspace/Datasets/orders - orders - Kimm Topping.csv\n",
      "Downloading file: 2_Debits_and_Credits_Purchases.csv to clean_workspace/0.1.0/workspace/Datasets/2_Debits_and_Credits_Purchases.csv\n",
      "Downloading file: Content Delivery Data Set - Jacob Goldberg.csv to clean_workspace/0.1.0/workspace/Datasets/Content Delivery Data Set - Jacob Goldberg.csv\n",
      "Downloading file: Purchase_invoice_graphics_company.csv to clean_workspace/0.1.0/workspace/Datasets/Purchase_invoice_graphics_company.csv\n",
      "Downloading file: Accounting - Jake Chase.csv to clean_workspace/0.1.0/workspace/Datasets/Accounting - Jake Chase.csv\n",
      "Downloading file: Higher Ed Threads 2023 - Phil Nance.csv to clean_workspace/0.1.0/workspace/Datasets/Higher Ed Threads 2023 - Phil Nance.csv\n",
      "Downloading file: Cars_Sales_22-23.csv to clean_workspace/0.1.0/workspace/Datasets/Cars_Sales_22-23.csv\n",
      "Downloading file: COMAFI AND SUPER HISTORY.csv to clean_workspace/0.1.0/workspace/Datasets/COMAFI AND SUPER HISTORY.csv\n",
      "Downloading file: Survey Meters.csv to clean_workspace/0.1.0/workspace/Datasets/Survey Meters.csv\n",
      "Downloading file: Juana Events Rentals.csv to clean_workspace/0.1.0/workspace/Datasets/Juana Events Rentals.csv\n",
      "Downloading file: 2201_VoucherCheker.csv to clean_workspace/0.1.0/workspace/Datasets/2201_VoucherCheker.csv\n",
      "Downloading file: Blueprint_budget.csv to clean_workspace/0.1.0/workspace/Datasets/Blueprint_budget.csv\n",
      "Downloading file: Storage (version 1).csv - Sheet1 - Erica Redling.csv to clean_workspace/0.1.0/workspace/Datasets/Storage (version 1).csv - Sheet1 - Erica Redling.csv\n",
      "Downloading file: Productivity2.csv to clean_workspace/0.1.0/workspace/Datasets/Productivity2.csv\n",
      "Downloading file: Woodworking Class Data - Patrick Bernier.csv to clean_workspace/0.1.0/workspace/Datasets/Woodworking Class Data - Patrick Bernier.csv\n",
      "Downloading file: March Sales.csv to clean_workspace/0.1.0/workspace/Datasets/March Sales.csv\n",
      "Downloading file: Survey - Videogames - Brazil 2015 - Alexandre Papanis.csv to clean_workspace/0.1.0/workspace/Datasets/Survey - Videogames - Brazil 2015 - Alexandre Papanis.csv\n",
      "Downloading file: Wine Inventory - Andrew Nelson.csv to clean_workspace/0.1.0/workspace/Datasets/Wine Inventory - Andrew Nelson.csv\n",
      "Downloading file: Toggl Detailed Time Report - 2022.csv to clean_workspace/0.1.0/workspace/Datasets/Toggl Detailed Time Report - 2022.csv\n",
      "Downloading file: IllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv to clean_workspace/0.1.0/workspace/Datasets/IllinoisChicago_SuiteRentals_Aug2023 - Dalron J. Robertson.csv\n",
      "Downloading file: Dataset for Solar Panel Cleaning - Nestor Sanchez.csv to clean_workspace/0.1.0/workspace/Datasets/Dataset for Solar Panel Cleaning - Nestor Sanchez.csv\n",
      "Downloading file: 2022 Districts Monthly Transfer Dataset.csv to clean_workspace/0.1.0/workspace/Datasets/2022 Districts Monthly Transfer Dataset.csv\n",
      "Downloading file: balance_sheet.csv to clean_workspace/0.1.0/workspace/Datasets/balance_sheet.csv\n",
      "Downloading file: Parcels_2023Q1_CO - Guillermo Pardon.csv to clean_workspace/0.1.0/workspace/Datasets/Parcels_2023Q1_CO - Guillermo Pardon.csv\n",
      "Downloading file: OOC-55 - William Webster (SickBoy).csv to clean_workspace/0.1.0/workspace/Datasets/OOC-55 - William Webster (SickBoy).csv\n",
      "Downloading file: Materials_CTM.csv to clean_workspace/0.1.0/workspace/Datasets/Materials_CTM.csv\n",
      "Downloading file: cash_flow_Bakery.csv to clean_workspace/0.1.0/workspace/Datasets/cash_flow_Bakery.csv\n",
      "Downloading file: chicken_groceries_prices.csv to clean_workspace/0.1.0/workspace/Datasets/chicken_groceries_prices.csv\n",
      "Downloading file: Data Set Remotasks - Anita Portnoy.csv to clean_workspace/0.1.0/workspace/Datasets/Data Set Remotasks - Anita Portnoy.csv\n",
      "Downloading file: financial_credits.csv to clean_workspace/0.1.0/workspace/Datasets/financial_credits.csv\n",
      "Downloading file: Cristine's Corner January - Cristine Marquez.csv to clean_workspace/0.1.0/workspace/Datasets/Cristine's Corner January - Cristine Marquez.csv\n",
      "Downloading file: Tecno_MP_activities-feeder_collection-20231206183310-efa71.csv to clean_workspace/0.1.0/workspace/Datasets/Tecno_MP_activities-feeder_collection-20231206183310-efa71.csv\n",
      "Downloading file: airbnb_tax_01_2023-01_2024 - Philip Ferraro.csv to clean_workspace/0.1.0/workspace/Datasets/airbnb_tax_01_2023-01_2024 - Philip Ferraro.csv\n",
      "Downloading file: Log_2016_2020_Redacted - Adam Johnson.csv to clean_workspace/0.1.0/workspace/Datasets/Log_2016_2020_Redacted - Adam Johnson.csv\n",
      "Downloading file: U0089_Clients.csv to clean_workspace/0.1.0/workspace/Datasets/U0089_Clients.csv\n",
      "Downloading file: MarAug2023_ProjInst_Dataset - Diana.csv to clean_workspace/0.1.0/workspace/Datasets/MarAug2023_ProjInst_Dataset - Diana.csv\n",
      "Downloading file: Vat_purchases_journal_02-2023.csv to clean_workspace/0.1.0/workspace/Datasets/Vat_purchases_journal_02-2023.csv\n",
      "Downloading file: ServerProcesses.csv to clean_workspace/0.1.0/workspace/Datasets/ServerProcesses.csv\n",
      "Downloading file: Bold21 Data set - orders_export_1.csv to clean_workspace/0.1.0/workspace/Datasets/Bold21 Data set - orders_export_1.csv\n",
      "Downloading file: Payroll_oct_16-31.csv to clean_workspace/0.1.0/workspace/Datasets/Payroll_oct_16-31.csv\n",
      "Downloading file: Expense_summary.csv to clean_workspace/0.1.0/workspace/Datasets/Expense_summary.csv\n",
      "Downloading file: KookyArtsAi DATABASE - Christy Rivers.csv to clean_workspace/0.1.0/workspace/Datasets/KookyArtsAi DATABASE - Christy Rivers.csv\n",
      "Downloading file: Sales_RLC2.csv to clean_workspace/0.1.0/workspace/Datasets/Sales_RLC2.csv\n",
      "Downloading file: Fragrance June.csv to clean_workspace/0.1.0/workspace/Datasets/Fragrance June.csv\n",
      "Downloading file: Vector360-TravelResponse - Ernesto Herrero.csv to clean_workspace/0.1.0/workspace/Datasets/Vector360-TravelResponse - Ernesto Herrero.csv\n",
      "Downloading file: sales.csv to clean_workspace/0.1.0/workspace/Datasets/sales.csv\n",
      "Downloading file: Paper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv to clean_workspace/0.1.0/workspace/Datasets/Paper_Pen_Pavilion_Sales_09_12 - Cordejah Walker.csv\n",
      "Downloading file: EtsySoldOrders2021_2023 - Sara Gerges.csv to clean_workspace/0.1.0/workspace/Datasets/EtsySoldOrders2021_2023 - Sara Gerges.csv\n",
      "Downloading file: Student supplies delivery costs Fall 2023 - Robert Russell.csv to clean_workspace/0.1.0/workspace/Datasets/Student supplies delivery costs Fall 2023 - Robert Russell.csv\n",
      "Downloading file: KiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv to clean_workspace/0.1.0/workspace/Datasets/KiltX Web3 Summits Anonymized Data Keri Kilty - keri kilty.csv\n",
      "Downloading file: BurguerhouseJuly21.csv to clean_workspace/0.1.0/workspace/Datasets/BurguerhouseJuly21.csv\n",
      "Downloading file: mill_operations.csv to clean_workspace/0.1.0/workspace/Datasets/mill_operations.csv\n",
      "Downloading file: Store Survey - Patrick Bernier.csv to clean_workspace/0.1.0/workspace/Datasets/Store Survey - Patrick Bernier.csv\n",
      "Downloading file: Monthly Cash Flow.csv to clean_workspace/0.1.0/workspace/Datasets/Monthly Cash Flow.csv\n",
      "Downloading file: supplies.csv to clean_workspace/0.1.0/workspace/Datasets/supplies.csv\n",
      "Downloading file: public_lighting.csv to clean_workspace/0.1.0/workspace/Datasets/public_lighting.csv\n",
      "Downloading file: rtshare - Kris Best.csv to clean_workspace/0.1.0/workspace/Datasets/rtshare - Kris Best.csv\n",
      "Downloading file: First School Term 2023_2024 - Robert Russell.csv to clean_workspace/0.1.0/workspace/Datasets/First School Term 2023_2024 - Robert Russell.csv\n",
      "Downloading file: order.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv to clean_workspace/0.1.0/workspace/Datasets/order.list.export 2023-12-22 (2) - MOZZAM SHAHID.csv\n",
      "Downloading file: R Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/R Plan Budget.csv\n",
      "Downloading file: Report_periodical_records.csv to clean_workspace/0.1.0/workspace/Datasets/Report_periodical_records.csv\n",
      "Downloading file: Submission id - Miss Kaye Estacio.csv to clean_workspace/0.1.0/workspace/Datasets/Submission id - Miss Kaye Estacio.csv\n",
      "Downloading file: Product Dims.csv to clean_workspace/0.1.0/workspace/Datasets/Product Dims.csv\n",
      "Downloading file: SalesFiguresQ1Q2 - Travis Colahan.csv to clean_workspace/0.1.0/workspace/Datasets/SalesFiguresQ1Q2 - Travis Colahan.csv\n",
      "Downloading file: Campaigns BIMO.csv to clean_workspace/0.1.0/workspace/Datasets/Campaigns BIMO.csv\n",
      "Downloading file: prices_ss.csv to clean_workspace/0.1.0/workspace/Datasets/prices_ss.csv\n",
      "Downloading file: ABM_WAGES.csv to clean_workspace/0.1.0/workspace/Datasets/ABM_WAGES.csv\n",
      "Downloading file: Bill_A2.csv to clean_workspace/0.1.0/workspace/Datasets/Bill_A2.csv\n",
      "Downloading file: HEALTHCARE AFFILIATE MANAGEMENT.csv to clean_workspace/0.1.0/workspace/Datasets/HEALTHCARE AFFILIATE MANAGEMENT.csv\n",
      "Downloading file: Gwynstone.Originz.dat - Gwynne Rife.csv to clean_workspace/0.1.0/workspace/Datasets/Gwynstone.Originz.dat - Gwynne Rife.csv\n",
      "Downloading file: Purchase_orders_GAP_ZEN.csv to clean_workspace/0.1.0/workspace/Datasets/Purchase_orders_GAP_ZEN.csv\n",
      "Downloading file: data-export (3) - Harsh Bakshi.csv to clean_workspace/0.1.0/workspace/Datasets/data-export (3) - Harsh Bakshi.csv\n",
      "Downloading file: Waiters Cash_Sales.csv to clean_workspace/0.1.0/workspace/Datasets/Waiters Cash_Sales.csv\n",
      "Downloading file: Mama Milk Flow Data - paul miles.csv to clean_workspace/0.1.0/workspace/Datasets/Mama Milk Flow Data - paul miles.csv\n",
      "Downloading file: JAZ Ceramics.csv to clean_workspace/0.1.0/workspace/Datasets/JAZ Ceramics.csv\n",
      "Downloading file: AccumulatorReadingsReport.csv to clean_workspace/0.1.0/workspace/Datasets/AccumulatorReadingsReport.csv\n",
      "Downloading file: Checks.csv to clean_workspace/0.1.0/workspace/Datasets/Checks.csv\n",
      "Downloading file: Bank_accreditations.csv to clean_workspace/0.1.0/workspace/Datasets/Bank_accreditations.csv\n",
      "Downloading file: vaccinations_city_flores.csv to clean_workspace/0.1.0/workspace/Datasets/vaccinations_city_flores.csv\n",
      "Downloading file: Sheet1 - Brooks Chiro Rehab.csv to clean_workspace/0.1.0/workspace/Datasets/Sheet1 - Brooks Chiro Rehab.csv\n",
      "Downloading file: Values Survey - June 2023.csv to clean_workspace/0.1.0/workspace/Datasets/Values Survey - June 2023.csv\n",
      "Downloading file: ChemicalProductsApplications.csv to clean_workspace/0.1.0/workspace/Datasets/ChemicalProductsApplications.csv\n",
      "Downloading file: BurguerhouseJuly21csv.csv to clean_workspace/0.1.0/workspace/Datasets/BurguerhouseJuly21csv.csv\n",
      "Downloading file: TD_HET Shipping Report - Phil Nance.csv to clean_workspace/0.1.0/workspace/Datasets/TD_HET Shipping Report - Phil Nance.csv\n",
      "Downloading file: shop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv to clean_workspace/0.1.0/workspace/Datasets/shop_orders_export_shop_id_5874055_from_2023-11-01_to_2023-11-30 - Phil Nance.csv\n",
      "Downloading file: endowment_gm.csv to clean_workspace/0.1.0/workspace/Datasets/endowment_gm.csv\n",
      "Downloading file: Transaction List - Dawn Alderson.csv to clean_workspace/0.1.0/workspace/Datasets/Transaction List - Dawn Alderson.csv\n",
      "Downloading file: high_portability_local_cellphone.csv to clean_workspace/0.1.0/workspace/Datasets/high_portability_local_cellphone.csv\n",
      "Downloading file: Technical_services.csv to clean_workspace/0.1.0/workspace/Datasets/Technical_services.csv\n",
      "Downloading file: MARKAS S.A. BOX Purchases.csv to clean_workspace/0.1.0/workspace/Datasets/MARKAS S.A. BOX Purchases.csv\n",
      "Downloading file: 2023-12-21_transactions_export - Bailey Talley.csv to clean_workspace/0.1.0/workspace/Datasets/2023-12-21_transactions_export - Bailey Talley.csv\n",
      "Downloading file: OCT-QA-Report-2023 - Marlene Portillo.csv to clean_workspace/0.1.0/workspace/Datasets/OCT-QA-Report-2023 - Marlene Portillo.csv\n",
      "Downloading file: Bookshop Sales and Inventory Dataset.csv to clean_workspace/0.1.0/workspace/Datasets/Bookshop Sales and Inventory Dataset.csv\n",
      "Downloading file: november_supermarket_sales_per_product.csv to clean_workspace/0.1.0/workspace/Datasets/november_supermarket_sales_per_product.csv\n",
      "Downloading file: Art Gallery Spending Log 2017 - Erica Redling.csv to clean_workspace/0.1.0/workspace/Datasets/Art Gallery Spending Log 2017 - Erica Redling.csv\n",
      "Downloading file: BranchTransac.csv to clean_workspace/0.1.0/workspace/Datasets/BranchTransac.csv\n",
      "Downloading file: Sparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv to clean_workspace/0.1.0/workspace/Datasets/Sparkle and Sash Sales Detail 1_1_2022-6_30_2022 - Sheet1 (1) - Sparkle Sash.csv\n",
      "Downloading file: Cegin list of shifts of the month.csv to clean_workspace/0.1.0/workspace/Datasets/Cegin list of shifts of the month.csv\n",
      "Downloading file: Purchase_RLC.csv to clean_workspace/0.1.0/workspace/Datasets/Purchase_RLC.csv\n",
      "Downloading file: 2023 Inventory - Elisabeth Gracyalny.csv to clean_workspace/0.1.0/workspace/Datasets/2023 Inventory - Elisabeth Gracyalny.csv\n",
      "Downloading file: production_costs_Bakery.csv to clean_workspace/0.1.0/workspace/Datasets/production_costs_Bakery.csv\n",
      "Downloading file: metzeler_tires.csv to clean_workspace/0.1.0/workspace/Datasets/metzeler_tires.csv\n",
      "Downloading file: E Plan Budget.csv to clean_workspace/0.1.0/workspace/Datasets/E Plan Budget.csv\n",
      "Downloading file: Sales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv to clean_workspace/0.1.0/workspace/Datasets/Sales records  report_ 12_2021- 12_2023 (2 years) - Sheet1 - Chef Walt.csv\n",
      "Downloading file: Sale_Detail.csv to clean_workspace/0.1.0/workspace/Datasets/Sale_Detail.csv\n",
      "Downloading file: Expenses_2015.csv to clean_workspace/0.1.0/workspace/Datasets/Expenses_2015.csv\n",
      "Downloading file: online_retail.csv to clean_workspace/0.1.0/workspace/Datasets/online_retail.csv\n",
      "Downloading file: Commercial_Inventory.csv to clean_workspace/0.1.0/workspace/Datasets/Commercial_Inventory.csv\n",
      "Downloading file: sales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv to clean_workspace/0.1.0/workspace/Datasets/sales_data JAN 2020 through DEC 2021 - DARKOTHICA - Mel.csv\n",
      "Downloading file: Dairy 0051-01.csv to clean_workspace/0.1.0/workspace/Datasets/Dairy 0051-01.csv\n",
      "Downloading file: Champro SKU Specs.csv to clean_workspace/0.1.0/workspace/Datasets/Champro SKU Specs.csv\n",
      "Downloading file: Info_FTTH.xlsx - PPP_INFO.csv to clean_workspace/0.1.0/workspace/Datasets/Info_FTTH.xlsx - PPP_INFO.csv\n",
      "Downloading file: Delivery_Sales Report.csv to clean_workspace/0.1.0/workspace/Datasets/Delivery_Sales Report.csv\n",
      "Downloading file: Tecno_sales_2018-12-06_2023-12-06.csv to clean_workspace/0.1.0/workspace/Datasets/Tecno_sales_2018-12-06_2023-12-06.csv\n",
      "Downloading file: Children's White Sales.csv to clean_workspace/0.1.0/workspace/Datasets/Children's White Sales.csv\n",
      "Downloading file: data - Kayla Banger.csv to clean_workspace/0.1.0/workspace/Datasets/data - Kayla Banger.csv\n",
      "Downloading file: Stamping_Dataset.csv to clean_workspace/0.1.0/workspace/Datasets/Stamping_Dataset.csv\n",
      "Downloading file: LSC _ 2022_sales_activity_report - Casey Montante.csv to clean_workspace/0.1.0/workspace/Datasets/LSC _ 2022_sales_activity_report - Casey Montante.csv\n",
      "Downloading file: BUILDING EXPENSES.csv to clean_workspace/0.1.0/workspace/Datasets/BUILDING EXPENSES.csv\n",
      "Downloading file: BusinessReport-7-28-22 - Travis Colahan.csv to clean_workspace/0.1.0/workspace/Datasets/BusinessReport-7-28-22 - Travis Colahan.csv\n",
      "Downloading file: Bybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv to clean_workspace/0.1.0/workspace/Datasets/Bybit-Derivatives-TradeHistory-20221001-20230111 - Syed Jafri.csv\n",
      "Downloading file: Untitled spreadsheet - Melissa Jaycox.csv to clean_workspace/0.1.0/workspace/Datasets/Untitled spreadsheet - Melissa Jaycox.csv\n",
      "Downloading file: VF Sample Dataset - rafael guzman.csv to clean_workspace/0.1.0/workspace/Datasets/VF Sample Dataset - rafael guzman.csv\n",
      "Downloading file: SubSystemIO.csv to clean_workspace/0.1.0/workspace/Datasets/SubSystemIO.csv\n",
      "Downloading file: businesspayrolls.csv to clean_workspace/0.1.0/workspace/Datasets/businesspayrolls.csv\n",
      "Downloading file: C1_dbjg.csv to clean_workspace/0.1.0/workspace/Datasets/C1_dbjg.csv\n",
      "Dataset download complete.\n",
      "Downloading WS Dataset zip file with ID: 1kmXZ1oarBPlE0OQL52eGoc1xPbupJ1n9...\n",
      "Download progress: 100%\n",
      "Dataset download complete.\n",
      "Extracted to clean_workspace/0.1.0\n",
      "Moved clean_workspace/0.1.0/WS_DATA/file_dataset_pb2.py to clean_workspace/0.1.0/file_dataset_pb2.py\n"
     ]
    }
   ],
   "source": [
    "download_apis(download_datasets=True, save_directory='clean_workspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch & Download Colabs / Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222 entries, 0 to 221\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sample_id  222 non-null    object\n",
      " 1   colab_url  222 non-null    object\n",
      " 2   status     222 non-null    object\n",
      " 3   colab_id   222 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.1+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet_id = \"1V6IyjZMqXcQ07zc0naOm6cbYySKLxN95GRjuqLKzVDU\"\n",
    "data_tab = \"auto_qc_data\"\n",
    "\n",
    "colabs_df = GoogleSheet.get_sheet_data(sheet_id, data_tab)\n",
    "\n",
    "# colabs_df = colabs_df.loc[(colabs_df['status'] == \"FALSE\")]\n",
    "\n",
    "# colabs_df = colabs_df.loc[((colabs_df['w/o'] != 'No Error Found') | (colabs_df['with'] != 'No Error Found')) & (colabs_df['status'] == 'Needs Fixes')]\n",
    "\n",
    "colabs_df['colab_id'] = colabs_df['colab_url'].apply(GoogleService.extract_file_id)\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222 entries, 0 to 221\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   colab_id    222 non-null    object\n",
      " 1   colab_name  222 non-null    object\n",
      " 2   sample_id   222 non-null    object\n",
      " 3   colab_url   222 non-null    object\n",
      " 4   status      222 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 8.8+ KB\n"
     ]
    }
   ],
   "source": [
    "colab_names = []\n",
    "name_request_batch_size = 99\n",
    "for start in range(0, len(colabs_df['colab_id']), name_request_batch_size):\n",
    "    colab_names += GoogleDrive.get_file_names_in_batch(colabs_df['colab_id'].tolist()[start:start+name_request_batch_size])\n",
    "colab_name_df = pd.DataFrame(colab_names)\n",
    "colab_name_df = colab_name_df[~colab_name_df['colab_name'].isna()]\n",
    "colabs_df = pd.merge(colab_name_df, colabs_df, on='colab_id')\n",
    "# colabs_df = colabs_df.drop_duplicates(['colab_id'])\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batches and Configuration Files for Docker Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Batches: 45\n",
      "Max Samples Per Batch: 5\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(list(set(colabs_df['colab_id'])))\n",
    "max_container = 50\n",
    "max_batch_size = math.ceil(total_samples / max_container)\n",
    "print(f'Max Batches: {math.ceil(total_samples/max_batch_size)}\\nMax Samples Per Batch: {max_batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = '0.1.0'\n",
    "notebooks = [{'path': notebook, 'api_version': api_version} for notebook in list(set(colabs_df['colab_id']))]\n",
    "notebooks_df = pd.DataFrame(notebooks)\n",
    "for idx, api_version in enumerate(set(notebooks_df['api_version'])):\n",
    "    count_notebooks = len(notebooks_df[notebooks_df['api_version']==api_version])\n",
    "    batches = []\n",
    "    for idx in range(count_notebooks):\n",
    "        batches.append(idx//max_batch_size)\n",
    "    batch_ids = [f\"{api_version}_{batch}\" for batch in batches]\n",
    "    notebooks_df.loc[notebooks_df['api_version'] == api_version, 'batch_id'] = batch_ids\n",
    "\n",
    "notebooks_df.to_csv('execution_configs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local Run (where you have root access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_config = pd.read_csv(\"execution_configs.csv\")\n",
    "run_identifiers = list(set(exec_config['batch_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Validating Host Environment ---\n",
      "✅ Docker client connected.\n",
      "\n",
      "--- Step 2: Preparing Host Directories ---\n",
      "✅ Created log directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/execution_logs/sanity_check_20250821_131047\n",
      "✅ Created result directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/results/sanity_check_20250821_131047\n",
      "✅ Created result directory for this run at: /Users/nabeel/PycharmProjects/e2e_sanity_checks/executed_notebooks/sanity_check_20250821_131047\n",
      "\n",
      "--- Step 4: Launching Containers in Parallel ---\n",
      "  -> Launching container 'sanity_check_20250821_131047-0' for batch 0...\n",
      "  -> Launching container 'sanity_check_20250821_131047-1' for batch 1...\n",
      "  -> Launching container 'sanity_check_20250821_131047-2' for batch 2...\n",
      "  -> Launching container 'sanity_check_20250821_131047-3' for batch 3...\n",
      "  -> Launching container 'sanity_check_20250821_131047-4' for batch 4...\n",
      "  -> Launching container 'sanity_check_20250821_131047-5' for batch 5...\n",
      "  -> Launching container 'sanity_check_20250821_131047-6' for batch 6...\n",
      "  -> Launching container 'sanity_check_20250821_131047-7' for batch 7...\n",
      "  -> Launching container 'sanity_check_20250821_131047-8' for batch 8...\n",
      "  -> Launching container 'sanity_check_20250821_131047-9' for batch 9...\n",
      "  -> Launching container 'sanity_check_20250821_131047-10' for batch 10...\n",
      "  -> Launching container 'sanity_check_20250821_131047-11' for batch 11...\n",
      "  -> Launching container 'sanity_check_20250821_131047-12' for batch 12...\n",
      "  -> Launching container 'sanity_check_20250821_131047-13' for batch 13...\n",
      "  -> Launching container 'sanity_check_20250821_131047-14' for batch 14...\n",
      "  -> Launching container 'sanity_check_20250821_131047-15' for batch 15...\n",
      "  -> Launching container 'sanity_check_20250821_131047-16' for batch 16...\n",
      "  -> Launching container 'sanity_check_20250821_131047-17' for batch 17...\n",
      "  -> Launching container 'sanity_check_20250821_131047-18' for batch 18...\n",
      "  -> Launching container 'sanity_check_20250821_131047-19' for batch 19...\n",
      "  -> Launching container 'sanity_check_20250821_131047-20' for batch 20...\n",
      "  -> Launching container 'sanity_check_20250821_131047-21' for batch 21...\n",
      "  -> Launching container 'sanity_check_20250821_131047-22' for batch 22...\n",
      "  -> Launching container 'sanity_check_20250821_131047-23' for batch 23...\n",
      "  -> Launching container 'sanity_check_20250821_131047-24' for batch 24...\n",
      "  -> Launching container 'sanity_check_20250821_131047-25' for batch 25...\n",
      "  -> Launching container 'sanity_check_20250821_131047-26' for batch 26...\n",
      "  -> Launching container 'sanity_check_20250821_131047-27' for batch 27...\n",
      "  -> Launching container 'sanity_check_20250821_131047-28' for batch 28...\n",
      "  -> Launching container 'sanity_check_20250821_131047-29' for batch 29...\n",
      "  -> Launching container 'sanity_check_20250821_131047-30' for batch 30...\n",
      "  -> Launching container 'sanity_check_20250821_131047-31' for batch 31...\n",
      "  -> Launching container 'sanity_check_20250821_131047-32' for batch 32...\n",
      "  -> Launching container 'sanity_check_20250821_131047-33' for batch 33...\n",
      "  -> Launching container 'sanity_check_20250821_131047-34' for batch 34...\n",
      "  -> Launching container 'sanity_check_20250821_131047-35' for batch 35...\n",
      "  -> Launching container 'sanity_check_20250821_131047-36' for batch 36...\n",
      "  -> Launching container 'sanity_check_20250821_131047-37' for batch 37...\n",
      "  -> Launching container 'sanity_check_20250821_131047-38' for batch 38...\n",
      "  -> Launching container 'sanity_check_20250821_131047-39' for batch 39...\n",
      "  -> Launching container 'sanity_check_20250821_131047-40' for batch 40...\n",
      "  -> Launching container 'sanity_check_20250821_131047-41' for batch 41...\n",
      "  -> Launching container 'sanity_check_20250821_131047-42' for batch 42...\n",
      "  -> Launching container 'sanity_check_20250821_131047-43' for batch 43...\n",
      "  -> Launching container 'sanity_check_20250821_131047-44' for batch 44...\n",
      "\n",
      "--- Step 5: Waiting for All Containers to Finish ---\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-0' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-1' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-2' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-3' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-4' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-5' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-6' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-7' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-8' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-9' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-10' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-11' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-12' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-13' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-14' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-15' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-16' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-17' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-18' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-19' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-20' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-21' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-22' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-23' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-24' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-25' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-26' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-27' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-28' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-29' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-30' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-31' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-32' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-33' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-34' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-35' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-36' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-37' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-38' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-39' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-40' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-41' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-42' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-43' finished with exit code 0.\n",
      "  -> ✅ SUCCESS | Container 'sanity_check_20250821_131047-44' finished with exit code 0.\n",
      "\n",
      "--- Orchestration Complete ---\n",
      "📄 All results and logs are stored in: /Users/nabeel/PycharmProjects/e2e_sanity_checks/execution_logs/sanity_check_20250821_131047\n",
      "Finished Docker Run. Time Taken: 335 Seconds\n"
     ]
    }
   ],
   "source": [
    "import sanity_orchestrator_with_download as orchestrator\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    run_name = f'sanity_check_{start_time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    orchestrator.run_orchestration(run_name, run_identifiers)\n",
    "    print(f\"Finished Docker Run. Time Taken: {(datetime.now()-start_time).seconds} Seconds\")\n",
    "except (FileNotFoundError, FileExistsError, ConnectionError) as e:\n",
    "    print(f\"\\n❌ A critical error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For VM Run (where you need to use sudo to run docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:sudo: a password is required\n"
     ]
    }
   ],
   "source": [
    "!sudo .venv/bin/python runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222 entries, 0 to 221\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                                Non-Null Count  Dtype \n",
      "---  ------                                                --------------  ----- \n",
      " 0   notebook                                              222 non-null    object\n",
      " 1   contains_golden_answer                                222 non-null    bool  \n",
      " 2   contains_final_assert                                 222 non-null    bool  \n",
      " 3   script_passed                                         222 non-null    bool  \n",
      " 4   script_failure_msg                                    222 non-null    object\n",
      " 5   Set Up - Install Dependencies and Clone Repositories  222 non-null    object\n",
      " 6   Set Up - Import APIs and initiate DBs                 222 non-null    object\n",
      " 7   Final Assertion_NO_ACTION                             222 non-null    object\n",
      " 8   Initial Assertion                                     222 non-null    object\n",
      " 9   Action                                                222 non-null    object\n",
      " 10  Final Assertion                                       222 non-null    object\n",
      "dtypes: bool(3), object(8)\n",
      "memory usage: 14.7+ KB\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'results/{run_name}'\n",
    "output_files = os.listdir(output_dir)\n",
    "complete_data = []\n",
    "for file in output_files:\n",
    "    full_path = Path(output_dir) / file\n",
    "    with open(full_path, 'r') as f:\n",
    "        complete_data += json.load(f)['result']\n",
    "# Use json_normalize to flatten the data\n",
    "sanity_df = pd.json_normalize(complete_data)\n",
    "sanity_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "      <th>script_passed</th>\n",
       "      <th>script_failure_msg</th>\n",
       "      <th>Set Up - Install Dependencies and Clone Repositories</th>\n",
       "      <th>Set Up - Import APIs and initiate DBs</th>\n",
       "      <th>Final Assertion_NO_ACTION</th>\n",
       "      <th>Initial Assertion</th>\n",
       "      <th>Action</th>\n",
       "      <th>Final Assertion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1xWzV1ZwJYVqJBc2xoYzOsYzF2F2hS0RW</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: NameError\\nError Description: name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_vPLqKE0J-nLQxjPkwDVN6kP42qvVK4l</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1n5qtNhll6vZnnHMh7H4Ec6FQuSmYVFon</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AttributeError\\nError Description: ...</td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17I3m3qQGZE9BuBnwB7GSUYhQUsvv-v_d</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: NameError\\nError Description: name ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_sMiRI0ieIgdtHY-bzVMBdxqTX6QoF8-</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AttributeError\\nError Description: ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            notebook  contains_golden_answer  \\\n",
       "0  1xWzV1ZwJYVqJBc2xoYzOsYzF2F2hS0RW                   False   \n",
       "1  1_vPLqKE0J-nLQxjPkwDVN6kP42qvVK4l                    True   \n",
       "2  1n5qtNhll6vZnnHMh7H4Ec6FQuSmYVFon                    True   \n",
       "3  17I3m3qQGZE9BuBnwB7GSUYhQUsvv-v_d                    True   \n",
       "4  1_sMiRI0ieIgdtHY-bzVMBdxqTX6QoF8-                    True   \n",
       "\n",
       "   contains_final_assert  script_passed  \\\n",
       "0                   True           True   \n",
       "1                  False          False   \n",
       "2                   True           True   \n",
       "3                   True           True   \n",
       "4                   True           True   \n",
       "\n",
       "                                  script_failure_msg  \\\n",
       "0                                                N/A   \n",
       "1  Traceback (most recent call last):\\n  File \"/a...   \n",
       "2                                                N/A   \n",
       "3                                                N/A   \n",
       "4                                                N/A   \n",
       "\n",
       "  Set Up - Install Dependencies and Clone Repositories  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "\n",
       "  Set Up - Import APIs and initiate DBs  \\\n",
       "0                                         \n",
       "1                                         \n",
       "2                                         \n",
       "3                                         \n",
       "4                                         \n",
       "\n",
       "                           Final Assertion_NO_ACTION Initial Assertion  \\\n",
       "0  ErrorType: AssertionError\\nError Description: ...                     \n",
       "1                                                                        \n",
       "2  ErrorType: AssertionError\\nError Description: ...                     \n",
       "3  ErrorType: NameError\\nError Description: name ...                     \n",
       "4  ErrorType: AttributeError\\nError Description: ...                     \n",
       "\n",
       "                                              Action  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  ErrorType: AttributeError\\nError Description: ...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                     Final Assertion  \n",
       "0  ErrorType: NameError\\nError Description: name ...  \n",
       "1                                                     \n",
       "2  ErrorType: AssertionError\\nError Description: ...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_FAILED_ASSERTION = 'FA Failed - Assertion Error'\n",
    "IA_FAILED_ASSERTION = 'IA Failed - Assertion Error'\n",
    "NON_ASSERTION_ERROR = 'Non Assertion Error'\n",
    "ASSERTION_ERROR = \"Assertion Error\"\n",
    "NO_ERROR_FOUND = 'No Error Found'\n",
    "UNDEFINED_ERROR = 'Undefined Error Type'\n",
    "\n",
    "NEEDS_FIXES = 'Needs Fixes'\n",
    "GOOD_TO_GO = 'Good To Go'\n",
    "NEEDS_MANUAL_REVIEW = 'Needs Manual Review'\n",
    "CHECK_NOT_EXECUTED = 'Check Not Executed'\n",
    "\n",
    "def add_error_type(error_message):\n",
    "    if error_message == \"\":\n",
    "        return NO_ERROR_FOUND\n",
    "    error_type = error_message.split('\\n')[0].split(':')[-1].strip()\n",
    "\n",
    "    if error_type != 'AssertionError':\n",
    "        return NON_ASSERTION_ERROR\n",
    "    if error_type == 'AssertionError':\n",
    "        return ASSERTION_ERROR\n",
    "\n",
    "    return UNDEFINED_ERROR\n",
    "\n",
    "def get_auto_qc_status(row):\n",
    "    init_status = row['Execution Status Initialisation']\n",
    "    status_fa_no_action = row['Execution Status FA w/o Action']\n",
    "    status_ia = row['Execution Status IA']\n",
    "    status_action = row['Execution Status Action']\n",
    "    status_fa = row['Execution Status FA'] \n",
    "    contains_final_assert = row['contains_final_assert']\n",
    "    script_success = row['script_passed']\n",
    "\n",
    "\n",
    "    status = \"\"\n",
    "    message = \"\"\n",
    "    \n",
    "    if not script_success:\n",
    "        status = NEEDS_FIXES\n",
    "        message = \"Failed: Script to run Auto QC failed\"\n",
    "        return pd.Series((status, message))\n",
    "    \n",
    "    if NON_ASSERTION_ERROR in [init_status, status_fa_no_action, status_ia, status_action, status_fa]:\n",
    "        status = NEEDS_FIXES\n",
    "        message = \"Failed: One of the code block contains Non Assertion Error(s)\"\n",
    "\n",
    "    elif ASSERTION_ERROR in [status_ia]:\n",
    "        status = NEEDS_FIXES\n",
    "        message = \"Failed: Assertion Failure in Initial Assertion.\"\n",
    "\n",
    "    elif ASSERTION_ERROR in [status_fa]:\n",
    "        status = NEEDS_FIXES\n",
    "        message = \"Failed: Final Assertion Failure even when Action is executed. Either Final Assertion or Action needs to be fixed.\"\n",
    "\n",
    "    elif ASSERTION_ERROR in [status_fa_no_action]:\n",
    "        status = GOOD_TO_GO\n",
    "        message = \"Passes: All Steps executed successfully and FA failed w/o action.\"\n",
    "        \n",
    "    else:\n",
    "        if all(status==NO_ERROR_FOUND for status in [init_status, status_fa_no_action, status_ia, status_action, status_fa]):\n",
    "            if contains_final_assert:\n",
    "                status = NEEDS_FIXES\n",
    "                message = \"Failed: If FA is present, it must fail in absense of the action\"\n",
    "            else:\n",
    "                status = GOOD_TO_GO\n",
    "                message = \"Passed: No FA block found so FA without action is expected to pass.\"\n",
    "        \n",
    "    return pd.Series((status, message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['notebook', 'contains_golden_answer', 'contains_final_assert',\n",
       "       'script_passed', 'script_failure_msg',\n",
       "       'Set Up - Install Dependencies and Clone Repositories',\n",
       "       'Set Up - Import APIs and initiate DBs', 'Final Assertion_NO_ACTION',\n",
       "       'Initial Assertion', 'Action', 'Final Assertion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_df['Execution Status Install Dependencies and Clone Repositories'] = sanity_df['Set Up - Install Dependencies and Clone Repositories'].apply(add_error_type)\n",
    "sanity_df['Execution Status Initialisation'] = sanity_df['Set Up - Import APIs and initiate DBs'].apply(add_error_type)\n",
    "sanity_df['Execution Status FA w/o Action'] = sanity_df['Final Assertion_NO_ACTION'].apply(add_error_type)\n",
    "sanity_df['Execution Status IA'] = sanity_df['Initial Assertion'].apply(add_error_type)\n",
    "sanity_df['Execution Status Action'] = sanity_df['Action'].apply(add_error_type)\n",
    "sanity_df['Execution Status FA'] = sanity_df['Final Assertion'].apply(add_error_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_df = sanity_df.rename(columns={'notebook': 'colab_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(colabs_df[['colab_id', 'sample_id']], sanity_df, on='colab_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colab_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "      <th>script_passed</th>\n",
       "      <th>script_failure_msg</th>\n",
       "      <th>Set Up - Install Dependencies and Clone Repositories</th>\n",
       "      <th>Set Up - Import APIs and initiate DBs</th>\n",
       "      <th>Final Assertion_NO_ACTION</th>\n",
       "      <th>Initial Assertion</th>\n",
       "      <th>Action</th>\n",
       "      <th>Final Assertion</th>\n",
       "      <th>Execution Status Install Dependencies and Clone Repositories</th>\n",
       "      <th>Execution Status Initialisation</th>\n",
       "      <th>Execution Status FA w/o Action</th>\n",
       "      <th>Execution Status IA</th>\n",
       "      <th>Execution Status Action</th>\n",
       "      <th>Execution Status FA</th>\n",
       "      <th>Auto QC Status</th>\n",
       "      <th>Auto QC Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1WZF_kmS0Z78ZaSbRT3s69grD5EuvFTFg</td>\n",
       "      <td>1WZF_kmS0Z78ZaSbRT3s69grD5EuvFTFg</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Good To Go</td>\n",
       "      <td>Passes: All Steps executed successfully and FA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ddOYUiLnmflqzQQ7r9AYb3p-D-THcE7j</td>\n",
       "      <td>1ddOYUiLnmflqzQQ7r9AYb3p-D-THcE7j</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: NameError\\nError Description: name ...</td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "      <td>Failed: One of the code block contains Non Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1oSI1iciSHXJd5WPS6gN-ZFl8NtK7F_rC</td>\n",
       "      <td>1oSI1iciSHXJd5WPS6gN-ZFl8NtK7F_rC</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: NameError\\nError Description: name ...</td>\n",
       "      <td>ErrorType: AssertionError\\nError Description: ...</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "      <td>Failed: One of the code block contains Non Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1sOHIUssSmkKuXK8IASssFHaS9s1WMtgV</td>\n",
       "      <td>1sOHIUssSmkKuXK8IASssFHaS9s1WMtgV</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: NameError\\nError Description: name ...</td>\n",
       "      <td></td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Needs Fixes</td>\n",
       "      <td>Failed: One of the code block contains Non Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149DQS_PcWIVB81k248hatMQlQiJLk0wJ</td>\n",
       "      <td>149DQS_PcWIVB81k248hatMQlQiJLk0wJ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ErrorType: CommandExecutionError\\nError Descri...</td>\n",
       "      <td></td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>No Error Found</td>\n",
       "      <td>Needs Fixes</td>\n",
       "      <td>Failed: One of the code block contains Non Ass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            colab_id                          sample_id  \\\n",
       "0  1WZF_kmS0Z78ZaSbRT3s69grD5EuvFTFg  1WZF_kmS0Z78ZaSbRT3s69grD5EuvFTFg   \n",
       "1  1ddOYUiLnmflqzQQ7r9AYb3p-D-THcE7j  1ddOYUiLnmflqzQQ7r9AYb3p-D-THcE7j   \n",
       "2  1oSI1iciSHXJd5WPS6gN-ZFl8NtK7F_rC  1oSI1iciSHXJd5WPS6gN-ZFl8NtK7F_rC   \n",
       "3  1sOHIUssSmkKuXK8IASssFHaS9s1WMtgV  1sOHIUssSmkKuXK8IASssFHaS9s1WMtgV   \n",
       "4  149DQS_PcWIVB81k248hatMQlQiJLk0wJ  149DQS_PcWIVB81k248hatMQlQiJLk0wJ   \n",
       "\n",
       "   contains_golden_answer  contains_final_assert  script_passed  \\\n",
       "0                   False                   True           True   \n",
       "1                   False                   True           True   \n",
       "2                    True                   True           True   \n",
       "3                    True                  False           True   \n",
       "4                    True                  False           True   \n",
       "\n",
       "  script_failure_msg Set Up - Install Dependencies and Clone Repositories  \\\n",
       "0                N/A                                                        \n",
       "1                N/A                                                        \n",
       "2                N/A                                                        \n",
       "3                N/A                                                        \n",
       "4                N/A                                                        \n",
       "\n",
       "  Set Up - Import APIs and initiate DBs  \\\n",
       "0                                         \n",
       "1                                         \n",
       "2                                         \n",
       "3                                         \n",
       "4                                         \n",
       "\n",
       "                           Final Assertion_NO_ACTION Initial Assertion  \\\n",
       "0  ErrorType: AssertionError\\nError Description: ...                     \n",
       "1  ErrorType: AssertionError\\nError Description: ...                     \n",
       "2  ErrorType: AssertionError\\nError Description: ...                     \n",
       "3                                                                        \n",
       "4                                                                        \n",
       "\n",
       "                                              Action  \\\n",
       "0                                                      \n",
       "1  ErrorType: NameError\\nError Description: name ...   \n",
       "2  ErrorType: NameError\\nError Description: name ...   \n",
       "3  ErrorType: NameError\\nError Description: name ...   \n",
       "4  ErrorType: CommandExecutionError\\nError Descri...   \n",
       "\n",
       "                                     Final Assertion  \\\n",
       "0                                                      \n",
       "1  ErrorType: AssertionError\\nError Description: ...   \n",
       "2  ErrorType: AssertionError\\nError Description: ...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "  Execution Status Install Dependencies and Clone Repositories  \\\n",
       "0                                     No Error Found             \n",
       "1                                     No Error Found             \n",
       "2                                     No Error Found             \n",
       "3                                     No Error Found             \n",
       "4                                     No Error Found             \n",
       "\n",
       "  Execution Status Initialisation Execution Status FA w/o Action  \\\n",
       "0                  No Error Found                Assertion Error   \n",
       "1                  No Error Found                Assertion Error   \n",
       "2                  No Error Found                Assertion Error   \n",
       "3                  No Error Found                 No Error Found   \n",
       "4                  No Error Found                 No Error Found   \n",
       "\n",
       "  Execution Status IA Execution Status Action Execution Status FA  \\\n",
       "0      No Error Found          No Error Found      No Error Found   \n",
       "1      No Error Found     Non Assertion Error     Assertion Error   \n",
       "2      No Error Found     Non Assertion Error     Assertion Error   \n",
       "3      No Error Found     Non Assertion Error      No Error Found   \n",
       "4      No Error Found     Non Assertion Error      No Error Found   \n",
       "\n",
       "  Auto QC Status                                    Auto QC Message  \n",
       "0     Good To Go  Passes: All Steps executed successfully and FA...  \n",
       "1    Needs Fixes  Failed: One of the code block contains Non Ass...  \n",
       "2    Needs Fixes  Failed: One of the code block contains Non Ass...  \n",
       "3    Needs Fixes  Failed: One of the code block contains Non Ass...  \n",
       "4    Needs Fixes  Failed: One of the code block contains Non Ass...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Auto QC Status\n",
       "Needs Fixes    191\n",
       "Good To Go      31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[['Auto QC Status', 'Auto QC Message']] = merged_df.apply(get_auto_qc_status, axis=1)\n",
    "merged_df['Auto QC Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "script_passed\n",
       "True     176\n",
       "False     46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['script_passed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 222 entries, 0 to 221\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                                        Non-Null Count  Dtype \n",
      "---  ------                                                        --------------  ----- \n",
      " 0   colab_id                                                      222 non-null    object\n",
      " 1   sample_id                                                     222 non-null    object\n",
      " 2   contains_golden_answer                                        222 non-null    bool  \n",
      " 3   contains_final_assert                                         222 non-null    bool  \n",
      " 4   script_passed                                                 222 non-null    bool  \n",
      " 5   script_failure_msg                                            222 non-null    object\n",
      " 6   Set Up - Install Dependencies and Clone Repositories          222 non-null    object\n",
      " 7   Set Up - Import APIs and initiate DBs                         222 non-null    object\n",
      " 8   Final Assertion_NO_ACTION                                     222 non-null    object\n",
      " 9   Initial Assertion                                             222 non-null    object\n",
      " 10  Action                                                        222 non-null    object\n",
      " 11  Final Assertion                                               222 non-null    object\n",
      " 12  Execution Status Install Dependencies and Clone Repositories  222 non-null    object\n",
      " 13  Execution Status Initialisation                               222 non-null    object\n",
      " 14  Execution Status FA w/o Action                                222 non-null    object\n",
      " 15  Execution Status IA                                           222 non-null    object\n",
      " 16  Execution Status Action                                       222 non-null    object\n",
      " 17  Execution Status FA                                           222 non-null    object\n",
      " 18  Auto QC Status                                                222 non-null    object\n",
      " 19  Auto QC Message                                               222 non-null    object\n",
      "dtypes: bool(3), object(17)\n",
      "memory usage: 30.3+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_text(text):\n",
    "    return text[:49999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in merged_df.select_dtypes(include=['object', 'string']).columns.tolist():\n",
    "    merged_df[col] = merged_df[col].apply(trim_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_parser'</span><span style=\"color: #008080; text-decoration-color: #008080\"> does not exist in the spreadsheet. Creating a new tab.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mTab \u001b[0m\u001b[32m'auto_qc_response_parser'\u001b[0m\u001b[36m does not exist in the spreadsheet. Creating a new tab.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Successfully added new tab: </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_parser'</span><span style=\"color: #008080; text-decoration-color: #008080\"> with ID: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1619311774</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mSuccessfully added new tab: \u001b[0m\u001b[32m'auto_qc_response_parser'\u001b[0m\u001b[36m with ID: \u001b[0m\u001b[1;36m1619311774\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Uploading </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">222</span><span style=\"color: #008080; text-decoration-color: #008080\"> rows to tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_parser'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mUploading \u001b[0m\u001b[1;36m222\u001b[0m\u001b[36m rows to tab \u001b[0m\u001b[32m'auto_qc_response_parser'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4460</span><span style=\"color: #008080; text-decoration-color: #008080\"> cells updated in tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_parser'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4460\u001b[0m\u001b[36m cells updated in tab \u001b[0m\u001b[32m'auto_qc_response_parser'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tab = 'auto_qc_response_parser'\n",
    "dev_sheet_id = '1V6IyjZMqXcQ07zc0naOm6cbYySKLxN95GRjuqLKzVDU'\n",
    "GoogleSheet.add_dataframe_to_sheet(dev_sheet_id, merged_df, output_tab, drop_duplicates_on = ['notebook'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
