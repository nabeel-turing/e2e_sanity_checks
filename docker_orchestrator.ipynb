{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docker \n",
    "# !pip install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import shlex\n",
    "import shutil\n",
    "import random\n",
    "import pathlib\n",
    "import subprocess\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "\n",
    "import docker\n",
    "import nbformat\n",
    "import gspread\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable, Dict, List, Sequence, Iterable, Union\n",
    "\n",
    "import pandas as pd\n",
    "from rclone_python import rclone\n",
    "from nbclient import NotebookClient\n",
    "from rclone_python.remote_types import RemoteTypes\n",
    "\n",
    "from google.auth import default\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build, Resource\n",
    "from googleapiclient.http import BatchHttpRequest, MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONRECURSIONLIMIT'] = '5000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_FILE = 'turing-delivery-g-ga-e36eb2300714.json'\n",
    "\n",
    "# Combine scopes for both Drive and Sheets\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "]\n",
    "\n",
    "def authenticate_with_service_account():\n",
    "    \"\"\"Authenticate using a service account and return credentials.\"\"\"\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    return creds\n",
    "\n",
    "# Get the shared credentials object\n",
    "credentials = authenticate_with_service_account()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "# @title Logger Configs\n",
    "custom_theme = Theme({\n",
    "    \"info\": \"cyan\",\n",
    "    \"warning\": \"magenta\",\n",
    "    \"error\": \"bold red\"\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "class Logger:\n",
    "  @staticmethod\n",
    "  def log(message):\n",
    "    console.print(message, style=\"info\")\n",
    "\n",
    "  def error(message):\n",
    "    console.print(message, style=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleService Class\n",
    "class GoogleService:\n",
    "\n",
    "  @classmethod\n",
    "  def extract_file_id(cls, url):\n",
    "      patterns = [\n",
    "          r\"/spreadsheets/d/([^/]+)\",\n",
    "          r\"/file/d/([^/]+)\",     # Matches /file/d/{file_id}\n",
    "          r\"[?&]id=([^&]+)\",       # Matches ?id={file_id} or &id={file_id}\n",
    "          r\"/drive/([^/?#]+)\",     # Matches /drive/{file_id} and stops at /, ?, or #\n",
    "          r\"/folders/([^/]+)\"      # Matches /folders/{folder_id}\n",
    "      ]\n",
    "\n",
    "      for pattern in patterns:\n",
    "          match_ = re.search(pattern, url)\n",
    "          if match_:\n",
    "              return match_.group(1).strip()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleDrive Functionality\n",
    "class GoogleDrive(GoogleService):\n",
    "    \n",
    "    service = build(\"drive\", \"v3\", credentials=credentials)\n",
    "\n",
    "    @classmethod\n",
    "    def get_file_names_in_batch(cls, file_ids):\n",
    "        \"\"\"\n",
    "        Retrieves the names of multiple files from Google Drive in a single batch request.\n",
    "        \n",
    "        Args:\n",
    "            drive_service: An authenticated Google Drive API service object.\n",
    "            file_ids: A list of file IDs.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary mapping file IDs to their names.\n",
    "        \"\"\"\n",
    "        file_names = []\n",
    "    \n",
    "        def callback(request_id, response, exception):\n",
    "            \"\"\"\n",
    "            Callback function to process the result of each individual request.\n",
    "            \"\"\"\n",
    "            if exception:\n",
    "                print(f\"Error for file ID {request_id}: {exception}\")\n",
    "                \n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': None\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                file_names.append(\n",
    "                    {\n",
    "                        'colab_id': request_id,\n",
    "                        'colab_name': response.get('name')\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "        # Create a batch request with the callback\n",
    "        batch = cls.service.new_batch_http_request(callback=callback)\n",
    "    \n",
    "        # Add a 'files().get()' request for each file ID\n",
    "        for file_id in file_ids:\n",
    "            batch.add(\n",
    "                cls.service.files().get(\n",
    "                    fileId=file_id,\n",
    "                    fields='name',\n",
    "                    supportsAllDrives=True\n",
    "                ),\n",
    "                request_id=file_id  # Use the file ID to track each request\n",
    "            )\n",
    "    \n",
    "        # Execute the batch request\n",
    "        batch.execute()\n",
    "    \n",
    "        return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GoogleSheets Functionality\n",
    "class GoogleSheet(GoogleService):\n",
    "\n",
    "  # service = build(\"sheets\", \"v4\")\n",
    "  service = build(\"sheets\", \"v4\", credentials=credentials)\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_data(cls, sheet_id: str, tab_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from existing Google Sheet and returns it as Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sheet_id: The ID of the existing Google Sheet.\n",
    "        tab_name: The desired name for the new tab.\n",
    "        filter_col [Optional]: column name to filter the data.\n",
    "        filter_val [Optional]: value to filter the data on.\n",
    "    \"\"\"\n",
    "    vals = (\n",
    "        cls.service.spreadsheets()\n",
    "        .values()\n",
    "        .get(spreadsheetId=sheet_id, range=tab_name)\n",
    "        .execute()\n",
    "        .get(\"values\", [])\n",
    "    )\n",
    "    if len(vals) > 0:\n",
    "      header = vals[0]\n",
    "      data_values = vals[1:]\n",
    "      max_columns = min(len(header), len(data_values[0]))\n",
    "      data_values = [row[:max_columns] for row in data_values]\n",
    "      header = header[:max_columns]\n",
    "      df = pd.DataFrame(data_values, columns=header)\n",
    "      df.columns = [column.strip() for column in df.columns]\n",
    "      filter_cols = [col.strip() for col in kwargs.keys()]\n",
    "      if filter_cols:\n",
    "        if all(col in df.columns for col in filter_cols):\n",
    "          query = \" & \".join([\n",
    "              f\"{col}=='{kwargs[col]}'\"\n",
    "              if isinstance(kwargs[col], str)\n",
    "              else f\"{col}=={kwargs[col]}\"\n",
    "              for col in filter_cols])\n",
    "          df = df.query(query)\n",
    "        else:\n",
    "          missing_cols = [col for col in filter_cols if col not in df.columns]\n",
    "          raise Exception(f\"Could not find column(s) in the sheet. {missing_cols}\")\n",
    "      return df\n",
    "    sheet_name = cls.get_spreadsheet_name_by_id(sheet_id)\n",
    "    raise Exception(f\"No data found in the Tab: {tab_name}. Sheet ID: {sheet_name}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def tab_exists(cls, spreadsheet_id, tab_name):\n",
    "\n",
    "    spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        fields='sheets.properties'\n",
    "    ).execute()\n",
    "\n",
    "    sheets = spreadsheet_metadata.get('sheets', [])\n",
    "    for sheet in sheets:\n",
    "        properties = sheet.get('properties')\n",
    "        if properties and (properties.get('title') == tab_name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dataframe_to_sheet(cls, spreadsheet_id, df, tab_name, valueInputOption='RAW', drop_duplicates_on=['sample_id']):\n",
    "    \"\"\"\n",
    "    Adds a new tab to an existing Google Sheet and populates it with data from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spreadsheet_id: The ID of the existing Google Sheet.\n",
    "        df: The Pandas DataFrame to export.\n",
    "        tab_name: The desired name for the new tab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      if cls.tab_exists(spreadsheet_id, tab_name):\n",
    "        Logger.log(f\"Tab '{tab_name}' already exists in the spreadsheet.\")\n",
    "        existing_df = cls.get_sheet_data(spreadsheet_id, tab_name)\n",
    "        # TODO: Add dataframe validation check\n",
    "        Logger.log(f\"Existing Dataframe\")\n",
    "        Logger.log(existing_df.info())\n",
    "\n",
    "        combined_df = pd.concat([df, existing_df], ignore_index=True)\n",
    "        df_to_upload = combined_df.drop_duplicates(subset=drop_duplicates_on, keep='first', ignore_index=True)\n",
    "        Logger.log(f\"Combined Dataframe\")\n",
    "        Logger.log(df_to_upload.info())\n",
    "\n",
    "      else:\n",
    "        Logger.log(f\"Tab '{tab_name}' does not exist in the spreadsheet. Creating a new tab.\")\n",
    "        requests = [{\n",
    "            'addSheet': {\n",
    "                'properties': {\n",
    "                    'title': tab_name\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        batch_update_body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=batch_update_body\n",
    "        ).execute()\n",
    "        # Get the ID of the newly created sheet (optional, but useful)\n",
    "        new_sheet_id = response.get('replies')[0].get('addSheet').get('properties').get('sheetId')\n",
    "        Logger.log(f\"Successfully added new tab: '{tab_name}' with ID: {new_sheet_id}\")\n",
    "        df_to_upload = df\n",
    "\n",
    "      values = [df_to_upload.columns.tolist()] + df_to_upload.values.tolist()\n",
    "      Logger.log(f\"Uploading {len(df_to_upload)} rows to tab '{tab_name}'.\")\n",
    "      range_name = f\"'{tab_name}'!A1\" # Ensure tab name is quoted if it has spaces or special characters\n",
    "      body = {\n",
    "          'values': values\n",
    "      }\n",
    "      result = cls.service.spreadsheets().values().update(\n",
    "          spreadsheetId=spreadsheet_id,\n",
    "          range=range_name,\n",
    "          valueInputOption=valueInputOption,\n",
    "          body=body\n",
    "      ).execute()\n",
    "\n",
    "      Logger.log(f\"{result.get('updatedCells')} cells updated in tab '{tab_name}'.\")\n",
    "\n",
    "    except HttpError as err:\n",
    "      Logger.error(f\"An error occurred: {err}\")\n",
    "      if err.resp.status == 400: # Bad Request, often due to sheet name already existing\n",
    "        Logger.error(\"Error details: The tab name might already exist or the request is malformed.\")\n",
    "      elif err.resp.status == 403: # Forbidden, often due to incorrect permissions\n",
    "        Logger.error(\"Error details: Check your API permissions or if the service account/user has access to the sheet.\")\n",
    "      elif err.resp.status == 404: # Not Found, often due to incorrect spreadsheet ID\n",
    "        Logger.error(\"Error details: The spreadsheet ID might be incorrect.\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_spreadsheet_name_by_id(cls, spreadsheet_id):\n",
    "      \"\"\"\n",
    "      Retrieves the name (title) of a Google Spreadsheet given its ID.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id: The ID of the Google Spreadsheet.\n",
    "\n",
    "      Returns:\n",
    "          The title of the spreadsheet, or None if an error occurs or spreadsheet is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request the 'properties.title' field for efficiency\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='properties.title'\n",
    "          ).execute()\n",
    "\n",
    "          # Extract the title from the properties\n",
    "          title = spreadsheet_metadata.get('properties', {}).get('title')\n",
    "          return title\n",
    "      except HttpError as error:\n",
    "          print(f'An error occurred: {error}')\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found.\")\n",
    "          return None\n",
    "\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def add_dropdown_to_range(cls, spreadsheet_id: str, sheet_id: str,\n",
    "                            dropdown_options: list,\n",
    "                            range_start_row: int, range_end_row: int,\n",
    "                            range_start_col: int, range_end_col: int):\n",
    "    requests = [\n",
    "        {\n",
    "            'setDataValidation': {\n",
    "                'range': {\n",
    "                    'sheetId': sheet_id,\n",
    "                    'startRowIndex': range_start_row,\n",
    "                    'endRowIndex': range_end_row,\n",
    "                    'startColumnIndex': range_start_col,\n",
    "                    'endColumnIndex': range_end_col\n",
    "                },\n",
    "                'rule': {\n",
    "                    'condition': {\n",
    "                        'type': 'ONE_OF_LIST',\n",
    "                        'values': [{'userEnteredValue': option} for option in dropdown_options]\n",
    "                    },\n",
    "                    'strict': True,  # Users can only enter values from the list\n",
    "                    'showCustomUi': True, # Show dropdown arrow\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # --- Execute the batch update request ---\n",
    "    try:\n",
    "        body = {\n",
    "            'requests': requests\n",
    "        }\n",
    "        response = cls.service.spreadsheets().batchUpdate(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            body=body\n",
    "        ).execute()\n",
    "        print(f\"Dropdown added to Sheet ID {sheet_id}, Range row{range_start_row+1}:row{range_end_row}.\")\n",
    "        # You can inspect the response for more details if needed\n",
    "        # print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def get_sheet_id_by_name(cls, spreadsheet_id: str, tab_name: str):\n",
    "      \"\"\"\n",
    "      Retrieves the numerical Sheet ID (gid) for a given tab name within a spreadsheet.\n",
    "\n",
    "      Args:\n",
    "          spreadsheet_id (str): The ID of the Google Spreadsheet.\n",
    "          tab_name (str): The exact name (title) of the tab/sheet to find.\n",
    "\n",
    "      Returns:\n",
    "          int: The numerical sheet ID (gid) if found.\n",
    "          None: If an error occurs, the spreadsheet is not found, or the tab name is not found.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Use spreadsheets().get() to retrieve metadata\n",
    "          # We only request 'sheets.properties' to get sheet IDs and titles efficiently\n",
    "          spreadsheet_metadata = cls.service.spreadsheets().get(\n",
    "              spreadsheetId=spreadsheet_id,\n",
    "              fields='sheets.properties'\n",
    "          ).execute()\n",
    "\n",
    "          sheets = spreadsheet_metadata.get('sheets', [])\n",
    "          for sheet in sheets:\n",
    "              properties = sheet.get('properties')\n",
    "              # Check if properties exist and if the title matches the tab_name\n",
    "              if properties and properties.get('title') == tab_name:\n",
    "                  return properties.get('sheetId') # Return the sheetId (gid)\n",
    "\n",
    "          # If the loop completes, the tab was not found\n",
    "          print(f\"Tab '{tab_name}' not found in spreadsheet with ID '{spreadsheet_id}'.\")\n",
    "          return None\n",
    "      except HttpError as error:\n",
    "          if error.resp.status == 404:\n",
    "              print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Error: {error}\")\n",
    "          else:\n",
    "              print(f'An HTTP error occurred: {error}')\n",
    "          return None\n",
    "      except Exception as e:\n",
    "          print(f\"An unexpected error occurred while retrieving sheet ID for tab '{tab_name}': {e}\")\n",
    "          return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download APIs Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_apis(VERSION=\"0.1.0\", download_datasets=False):\n",
    "    import io\n",
    "    import os\n",
    "    import sys\n",
    "    import zipfile\n",
    "    import shutil\n",
    "    import re\n",
    "    # from google.colab import auth\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "    drive_service = GoogleDrive.service\n",
    "    # Version to download\n",
    "    # VERSION = \"0.0.rev22final\" # Version of the API\n",
    "    \n",
    "    # Define paths\n",
    "    CONTENT_DIR = os.path.join('clean_workspace', VERSION)\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        if os.path.isdir(CONTENT_DIR):\n",
    "            shutil.rmtree(CONTENT_DIR) \n",
    "        else:\n",
    "            os.remove(CONTENT_DIR)     \n",
    "    os.makedirs(CONTENT_DIR, exist_ok=True)\n",
    "    \n",
    "    APIS_DIR = os.path.join(CONTENT_DIR, 'APIs')\n",
    "    DBS_DIR = os.path.join(CONTENT_DIR, 'DBs')\n",
    "    SCRIPTS_DIR = os.path.join(CONTENT_DIR, 'Scripts')\n",
    "    FC_DIR = os.path.join(CONTENT_DIR, 'Schemas')\n",
    "    ZIP_PATH = os.path.join(CONTENT_DIR, f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # Google Drive Folder ID where versioned APIs zip files are stored\n",
    "    APIS_FOLDER_ID = '1QpkAZxXhVFzIbm8qPGPRP1YqXEvJ4uD4'\n",
    "    \n",
    "    # List of items to extract from the zip file\n",
    "    ITEMS_TO_EXTRACT = ['APIs/', 'DBs/', 'Scripts/']\n",
    "    \n",
    "    # Clean up existing directories and files\n",
    "    for path in [APIS_DIR, DBS_DIR, SCRIPTS_DIR, FC_DIR, ZIP_PATH]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "    \n",
    "    # Authenticate and create the drive service\n",
    "    # auth.authenticate_user()\n",
    "    # drive_service = build('drive', 'v3')\n",
    "    # drive_service\n",
    "    # Helper function to download a file from Google Drive\n",
    "    def download_drive_file(service, file_id, output_path, file_name=None, show_progress=True):\n",
    "        \"\"\"Downloads a file from Google Drive\"\"\"\n",
    "        destination = output_path\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        with io.FileIO(destination, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if show_progress:\n",
    "                    print(f\"Download progress: {int(status.progress() * 100)}%\")\n",
    "    \n",
    "    \n",
    "    # 1. List files in the specified APIs folder\n",
    "    print(f\"Searching for APIs zip file with version {VERSION} in folder: {APIS_FOLDER_ID}...\")\n",
    "    apis_file_id = None\n",
    "    \n",
    "    try:\n",
    "        query = f\"'{APIS_FOLDER_ID}' in parents and trashed=false\"\n",
    "        results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "        files = results.get('files', [])\n",
    "        for file in files:\n",
    "            file_name = file.get('name', '')\n",
    "            if file_name.lower() == f'apis_v{VERSION.lower()}.zip':\n",
    "                apis_file_id = file.get('id')\n",
    "                print(f\"Found matching file: {file_name} (ID: {apis_file_id})\")\n",
    "                break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing files in Google Drive: {e}\")\n",
    "    \n",
    "    if not apis_file_id:\n",
    "        print(f\"Error: Could not find APIs zip file with version {VERSION} in the specified folder.\")\n",
    "        sys.exit(\"Required APIs zip file not found.\")\n",
    "    \n",
    "    # 2. Download the found APIs zip file\n",
    "    print(f\"Downloading APIs zip file with ID: {apis_file_id}...\")\n",
    "    download_drive_file(drive_service, apis_file_id, ZIP_PATH, file_name=f'APIs_V{VERSION}.zip')\n",
    "    \n",
    "    # 3. Extract specific items from the zip file to /content\n",
    "    print(f\"Extracting specific items from {ZIP_PATH} to {CONTENT_DIR}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_contents = zip_ref.namelist()\n",
    "    \n",
    "            for member in zip_contents:\n",
    "                extracted = False\n",
    "                for item_prefix in ITEMS_TO_EXTRACT:\n",
    "                  if member == item_prefix or member.startswith(item_prefix):\n",
    "                        zip_ref.extract(member, CONTENT_DIR)\n",
    "                        extracted = True\n",
    "                        break\n",
    "    \n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The downloaded file at {ZIP_PATH} is not a valid zip file.\")\n",
    "        sys.exit(\"Invalid zip file downloaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "        sys.exit(\"Extraction failed.\")\n",
    "    \n",
    "    \n",
    "    # 4. Clean up\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)\n",
    "    \n",
    "    # 5. Add APIs to path\n",
    "    if os.path.exists(APIS_DIR):\n",
    "        sys.path.append(APIS_DIR)\n",
    "    else:\n",
    "        print(f\"Error: APIS directory not found at {APIS_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    # 6. Quick verification\n",
    "    # Check for the presence of the extracted items\n",
    "    verification_paths = [APIS_DIR, DBS_DIR, SCRIPTS_DIR]\n",
    "    all_present = True\n",
    "    print(\"\\nVerifying extracted items:\")\n",
    "    for path in verification_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"‚úÖ {path} is present.\")\n",
    "        else:\n",
    "            print(f\"‚ùå {path} is MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(f\"\\n‚úÖ Setup complete! Required items extracted to {CONTENT_DIR}.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Setup failed! Not all required items were extracted.\")\n",
    "\n",
    "    # 7. Generate Schemas\n",
    "\n",
    "    # Add Scripts to path\n",
    "    if os.path.exists(CONTENT_DIR):\n",
    "        sys.path.append(CONTENT_DIR)\n",
    "    else:\n",
    "        print(f\"Error: CONTENT_DIR directory not found at {CONTENT_DIR} after extraction. Cannot add to path.\")\n",
    "    \n",
    "    from Scripts.FCSpec import generate_package_schema\n",
    "    \n",
    "    print(\"\\nGenerating FC Schemas\")\n",
    "    os.makedirs(FC_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Iterate through the packages in the /content/APIs directory\n",
    "    for package_name in os.listdir(APIS_DIR):\n",
    "        package_path = os.path.join(APIS_DIR, package_name)\n",
    "    \n",
    "        # Check if it's a directory (to avoid processing files)\n",
    "        if os.path.isdir(package_path):\n",
    "            # Call the function to generate schema for the current package\n",
    "            generate_package_schema(package_path, output_folder_path=FC_DIR)\n",
    "    print(f\"‚úÖ Successfully generated {len(os.listdir(FC_DIR))} FC Schemas to {FC_DIR}\")\n",
    "\n",
    "    if download_datasets:\n",
    "        def download_drive_folder(service, folder_id, destination_path):\n",
    "            \"\"\"\n",
    "            Recursively downloads all files in a Google Drive folder using the `download_drive_file`\n",
    "            \"\"\"\n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "        \n",
    "            query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "            page_token = None\n",
    "        \n",
    "            while True:\n",
    "                results = service.files().list(\n",
    "                    q=query,\n",
    "                    spaces='drive',\n",
    "                    fields='nextPageToken, files(id, name, mimeType)',\n",
    "                    pageToken=page_token\n",
    "                ).execute()\n",
    "        \n",
    "                for item in results.get('files', []):\n",
    "                    file_id = item['id']\n",
    "                    file_name = item['name']\n",
    "                    mime_type = item['mimeType']\n",
    "        \n",
    "                    if mime_type == 'application/vnd.google-apps.folder':\n",
    "                        # Recursively download subfolders\n",
    "                        new_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Creating subfolder and downloading: {new_path}\")\n",
    "                        download_drive_folder(service, file_id, new_path)\n",
    "                    else:\n",
    "                        # Construct full file path and pass it as output_path\n",
    "                        full_path = os.path.join(destination_path, file_name)\n",
    "                        print(f\"Downloading file: {file_name} to {full_path}\")\n",
    "                        download_drive_file(service, file_id, full_path, file_name=file_name, show_progress=False)\n",
    "        \n",
    "                page_token = results.get('nextPageToken', None)\n",
    "                if not page_token:\n",
    "                    break\n",
    "        \n",
    "        # --- Configuration for Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'Quotewk.csv' file.\n",
    "        FOLDER_ID = \"1tZqZB1vAxp4TTxbPm6O2YjfkZD4FM-ml\"\n",
    "        # DATASET_FOLDER = \"./workspace/Datasets\"\n",
    "        DATASET_FOLDER = os.path.join(CONTENT_DIR, 'workspace/Datasets')\n",
    "        \n",
    "        print(f\"Starting download of folder {FOLDER_ID} to {DATASET_FOLDER}...\")\n",
    "        download_drive_folder(drive_service, FOLDER_ID, DATASET_FOLDER)\n",
    "        print(\"Dataset download complete.\")\n",
    "\n",
    "        # --- Configuration for WS Dataset Download ---\n",
    "        # This FOLDER_ID should contain the 'WS Multihop Datasets' file.\n",
    "        WS_DATA_ID = \"1kmXZ1oarBPlE0OQL52eGoc1xPbupJ1n9\"\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        \n",
    "        print(f\"Downloading WS Dataset zip file with ID: {WS_DATA_ID}...\")\n",
    "        download_drive_file(drive_service, WS_DATA_ID, WS_DATA_ZIP_PATH, file_name=f'WS_DATA.zip')\n",
    "        print(\"Dataset download complete.\")\n",
    "        \n",
    "        # Extract the Datasets\n",
    "        WS_DATA_ZIP_PATH = os.path.join(CONTENT_DIR, 'WS_DATA.zip')\n",
    "        with zipfile.ZipFile(WS_DATA_ZIP_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(CONTENT_DIR)\n",
    "        print(f\"Extracted to {CONTENT_DIR}\")\n",
    "        \n",
    "        # Moving 'file_dataset_pb2.py' to root directory\n",
    "        src_path = os.path.join(CONTENT_DIR, 'WS_DATA', 'file_dataset_pb2.py')\n",
    "        dst_path = os.path.join(CONTENT_DIR, 'file_dataset_pb2.py')\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"Moved {src_path} to {dst_path}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {src_path}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(WS_DATA_ZIP_PATH):\n",
    "            os.remove(WS_DATA_ZIP_PATH)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: 'clean_workspace/0.1.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdownload_apis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mdownload_apis\u001b[39m\u001b[34m(VERSION, download_datasets)\u001b[39m\n\u001b[32m     17\u001b[39m CONTENT_DIR = os.path.join(\u001b[33m'\u001b[39m\u001b[33mclean_workspace\u001b[39m\u001b[33m'\u001b[39m, VERSION)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(CONTENT_DIR):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m os.makedirs(CONTENT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m APIS_DIR = os.path.join(CONTENT_DIR, \u001b[33m'\u001b[39m\u001b[33mAPIs\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 1] Operation not permitted: 'clean_workspace/0.1.0'"
     ]
    }
   ],
   "source": [
    "download_apis(download_datasets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch & Download Colabs / Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 250 entries, 3 to 2804\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sample_id  250 non-null    object\n",
      " 1   colab_url  250 non-null    object\n",
      " 2   status     250 non-null    object\n",
      " 3   with       250 non-null    object\n",
      " 4   w/o        250 non-null    object\n",
      " 5   colab_id   250 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 13.7+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet_id = \"15XdJpUXvy7NC9Wb4NprQydcdQ5rQYdkij-NDo1saVzk\"\n",
    "data_tab = \"auto_qc_data\"\n",
    "\n",
    "colabs_df = GoogleSheet.get_sheet_data(sheet_id, data_tab)\n",
    "\n",
    "colabs_df = colabs_df.loc[((colabs_df['w/o'] != 'No Error Found') | (colabs_df['with'] != 'No Error Found')) & (colabs_df['status'] == 'Needs Fixes')]\n",
    "\n",
    "colabs_df['colab_id'] = colabs_df['colab_url'].apply(GoogleService.extract_file_id)\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "colabs_df = colabs_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   colab_id    10 non-null     object\n",
      " 1   colab_name  10 non-null     object\n",
      " 2   sample_id   10 non-null     object\n",
      " 3   colab_url   10 non-null     object\n",
      " 4   status      10 non-null     object\n",
      " 5   with        10 non-null     object\n",
      " 6   w/o         10 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 692.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "colab_names = []\n",
    "name_request_batch_size = 1\n",
    "for start in range(0, len(colabs_df['colab_id']), name_request_batch_size):\n",
    "    colab_names += GoogleDrive.get_file_names_in_batch(colabs_df['colab_id'].tolist()[start:start+name_request_batch_size])\n",
    "colab_name_df = pd.DataFrame(colab_names)\n",
    "colab_name_df = colab_name_df[~colab_name_df['colab_name'].isna()]\n",
    "colabs_df = pd.merge(colab_name_df, colabs_df, on='colab_id')\n",
    "colabs_df = colabs_df.drop_duplicates(['colab_id'])\n",
    "colabs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batches and Configuration Files for Docker Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 25\n",
      "Max Samples Per Batch: 1\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(colabs_df)\n",
    "max_container = 25\n",
    "max_batch_size = math.ceil(total_samples / max_container)\n",
    "print(f'Total Batches: {max_container}\\nMax Samples Per Batch: {max_batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_id\n",
       "0.1.0_0    1\n",
       "0.1.0_1    1\n",
       "0.1.0_2    1\n",
       "0.1.0_3    1\n",
       "0.1.0_4    1\n",
       "0.1.0_5    1\n",
       "0.1.0_6    1\n",
       "0.1.0_7    1\n",
       "0.1.0_8    1\n",
       "0.1.0_9    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_version = '0.1.0'\n",
    "notebooks = [{'path': notebook, 'api_version': api_version} for notebook in colabs_df['colab_id'].tolist()]\n",
    "notebooks_df = pd.DataFrame(notebooks)\n",
    "for idx, api_version in enumerate(set(notebooks_df['api_version'])):\n",
    "    count_notebooks = len(notebooks_df[notebooks_df['api_version']==api_version])\n",
    "    batches = []\n",
    "    for idx in range(count_notebooks):\n",
    "        batches.append(idx//max_batch_size)\n",
    "    batch_ids = [f\"{api_version}_{batch}\" for batch in batches]\n",
    "    notebooks_df.loc[notebooks_df['api_version'] == api_version, 'batch_id'] = batch_ids\n",
    "\n",
    "notebooks_df.to_csv('execution_configs.csv', index=False)\n",
    "\n",
    "notebooks_df = pd.merge(notebooks_df, colabs_df, left_on='path', right_on='colab_id')\n",
    "run_identifiers = list(set(notebooks_df['batch_id']))\n",
    "notebooks_df['batch_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local Run (where you have root access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Validating Host Environment ---\n",
      "‚úÖ Docker client connected.\n",
      "\n",
      "--- Step 2: Preparing Host Directories ---\n",
      "‚úÖ Created log directory for this run at: /Users/chaitraravada/Desktop/workspace/Turing/Auto-Qc/e2e_sanity_checks/execution_logs/sanity_check_20250820_002308\n",
      "‚úÖ Created result directory for this run at: /Users/chaitraravada/Desktop/workspace/Turing/Auto-Qc/e2e_sanity_checks/results/sanity_check_20250820_002308\n",
      "\n",
      "--- Step 4: Launching Containers in Parallel ---\n",
      "  -> Launching container 'sanity_check_20250820_002308-0' for batch 0...\n",
      "  -> Launching container 'sanity_check_20250820_002308-1' for batch 1...\n",
      "  -> Launching container 'sanity_check_20250820_002308-2' for batch 2...\n",
      "  -> Launching container 'sanity_check_20250820_002308-3' for batch 3...\n",
      "  -> Launching container 'sanity_check_20250820_002308-4' for batch 4...\n",
      "  -> Launching container 'sanity_check_20250820_002308-5' for batch 5...\n",
      "  -> Launching container 'sanity_check_20250820_002308-6' for batch 6...\n",
      "  -> Launching container 'sanity_check_20250820_002308-7' for batch 7...\n",
      "  -> Launching container 'sanity_check_20250820_002308-8' for batch 8...\n",
      "  -> Launching container 'sanity_check_20250820_002308-9' for batch 9...\n",
      "\n",
      "--- Step 5: Waiting for All Containers to Finish ---\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-0' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-1' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-2' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-3' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-4' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-5' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-6' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-7' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-8' finished with exit code 0.\n",
      "  -> ‚úÖ SUCCESS | Container 'sanity_check_20250820_002308-9' finished with exit code 0.\n",
      "\n",
      "--- Orchestration Complete ---\n",
      "üìÑ All results and logs are stored in: /Users/chaitraravada/Desktop/workspace/Turing/Auto-Qc/e2e_sanity_checks/execution_logs/sanity_check_20250820_002308\n",
      "Finished Docker Run. Time Taken: 22 Seconds\n"
     ]
    }
   ],
   "source": [
    "import sanity_orchestrator_with_download as orchestrator\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    run_name = f'sanity_check_{start_time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    orchestrator.run_orchestration(run_name, run_identifiers)\n",
    "    print(f\"Finished Docker Run. Time Taken: {(datetime.now()-start_time).seconds} Seconds\")\n",
    "except (FileNotFoundError, FileExistsError, ConnectionError) as e:\n",
    "    print(f\"\\n‚ùå A critical error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For VM Run (where you need to use sudo to run docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo .venv/bin/python runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 7 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   notebook                    10 non-null     object\n",
      " 1   no_action_script_success    10 non-null     bool  \n",
      " 2   no_action_response          10 non-null     object\n",
      " 3   with_action_script_success  10 non-null     bool  \n",
      " 4   with_action_response        10 non-null     object\n",
      " 5   contains_golden_answer      10 non-null     bool  \n",
      " 6   contains_final_assert       10 non-null     bool  \n",
      "dtypes: bool(4), object(3)\n",
      "memory usage: 412.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'results/{run_name}'\n",
    "output_files = os.listdir(output_dir)\n",
    "complete_data = []\n",
    "for file in output_files:\n",
    "    full_path = Path(output_dir) / file\n",
    "    with open(full_path, 'r') as f:\n",
    "        complete_data += json.load(f)['result']\n",
    "# Use json_normalize to flatten the data\n",
    "sanity_df = pd.json_normalize(complete_data)\n",
    "sanity_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notebook</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1g_duG0timZpsO3oWMRWsDe9kcSJYsU_A</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1Wh17vrxhVEgmKqndAq1mv34OSNkh6OcG</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1Z4SN1ul3IcPpmrGcpj9xPHq0J4qxQZI4</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1OviEuB5qhdtTZpxTdYZ-IXbu3PhTl8x6</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19rJV18R_xxCrPobRblyVPnY6vhuEUb0C</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            notebook  no_action_script_success  \\\n",
       "0  1g_duG0timZpsO3oWMRWsDe9kcSJYsU_A                      True   \n",
       "1  1Wh17vrxhVEgmKqndAq1mv34OSNkh6OcG                      True   \n",
       "2  1Z4SN1ul3IcPpmrGcpj9xPHq0J4qxQZI4                      True   \n",
       "3  1OviEuB5qhdtTZpxTdYZ-IXbu3PhTl8x6                      True   \n",
       "4  19rJV18R_xxCrPobRblyVPnY6vhuEUb0C                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "1  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "2  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "3  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "4  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  contains_golden_answer  \\\n",
       "0  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "1  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "2  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "3  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "4  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "\n",
       "   contains_final_assert  \n",
       "0                   True  \n",
       "1                   True  \n",
       "2                   True  \n",
       "3                   True  \n",
       "4                   True  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>colab_url</th>\n",
       "      <th>colab_name</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185_base</td>\n",
       "      <td>https://drive.google.com/file/d/1ntAczmtrIzGQB...</td>\n",
       "      <td>Agent-185-Base.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312_base</td>\n",
       "      <td>https://drive.google.com/file/d/1NGSwUOLK_2reN...</td>\n",
       "      <td>Agent-312-Base.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>506_base</td>\n",
       "      <td>https://drive.google.com/file/d/1Wh17vrxhVEgmK...</td>\n",
       "      <td>Agent-506_base-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414_base</td>\n",
       "      <td>https://drive.google.com/file/d/1g_duG0timZpsO...</td>\n",
       "      <td>Agent-414_base-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>235_base</td>\n",
       "      <td>https://drive.google.com/file/d/19rJV18R_xxCrP...</td>\n",
       "      <td>Agent-235_base-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id                                          colab_url  \\\n",
       "0  185_base  https://drive.google.com/file/d/1ntAczmtrIzGQB...   \n",
       "1  312_base  https://drive.google.com/file/d/1NGSwUOLK_2reN...   \n",
       "2  506_base  https://drive.google.com/file/d/1Wh17vrxhVEgmK...   \n",
       "3  414_base  https://drive.google.com/file/d/1g_duG0timZpsO...   \n",
       "4  235_base  https://drive.google.com/file/d/19rJV18R_xxCrP...   \n",
       "\n",
       "                    colab_name  no_action_script_success  \\\n",
       "0         Agent-185-Base.ipynb                      True   \n",
       "1         Agent-312-Base.ipynb                      True   \n",
       "2  Agent-506_base-Merged.ipynb                      True   \n",
       "3  Agent-414_base-Merged.ipynb                      True   \n",
       "4  Agent-235_base-Merged.ipynb                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "1  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "2  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "3  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "4  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  contains_golden_answer  \\\n",
       "0  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "1  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "2  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "3  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "4  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "\n",
       "   contains_final_assert  \n",
       "0                   True  \n",
       "1                   True  \n",
       "2                   True  \n",
       "3                   True  \n",
       "4                   True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_set = [\n",
    "    'sample_id', \n",
    "    'colab_url', \n",
    "    'colab_name', \n",
    "    'no_action_script_success',\n",
    "    'no_action_response',\n",
    "    'with_action_script_success',\n",
    "    'with_action_response',\n",
    "    'contains_golden_answer',\n",
    "    'contains_final_assert',\n",
    "    ]\n",
    "merged_df = pd.merge(colabs_df, sanity_df, left_on='colab_id', right_on='notebook')[columns_set]\n",
    "merged_df = merged_df.fillna(\"\")\n",
    "for col in ['with_action_response', 'no_action_response']:\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: x[:49999])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 9 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   sample_id                   10 non-null     object\n",
      " 1   colab_url                   10 non-null     object\n",
      " 2   colab_name                  10 non-null     object\n",
      " 3   no_action_script_success    10 non-null     bool  \n",
      " 4   no_action_response          10 non-null     object\n",
      " 5   with_action_script_success  10 non-null     bool  \n",
      " 6   with_action_response        10 non-null     object\n",
      " 7   contains_golden_answer      10 non-null     bool  \n",
      " 8   contains_final_assert       10 non-null     bool  \n",
      "dtypes: bool(4), object(5)\n",
      "memory usage: 572.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_FAILED_ASSERTION = 'FA Failed - Assertion Error'\n",
    "IA_FAILED_ASSERTION = 'IA Failed - Assertion Error'\n",
    "NON_ASSERTION_ERROR = 'Non Assertion Error'\n",
    "NO_ERROR_FOUND = 'No Error Found'\n",
    "UNDEFINED_ERROR = 'Undefined Error Type'\n",
    "\n",
    "NEEDS_FIXES = 'Needs Fixes'\n",
    "GOOD_TO_GO = 'Good To Go'\n",
    "NEEDS_MANUAL_REVIEW = 'Needs Manual Review'\n",
    "CHECK_NOT_EXECUTED = 'Check Not Executed'\n",
    "\n",
    "def add_error_type(error_message):\n",
    "    if error_message == \"\":\n",
    "        return NO_ERROR_FOUND\n",
    "    block = error_message.split('\\n')[0].split(':')[-1].strip()\n",
    "    error_type = error_message.split('\\n')[1].split(':')[-1].strip()\n",
    "    initial_assertion_header = 'Initial Assertion'\n",
    "    final_assertion_header = 'Final Assertion'\n",
    "    # Non Assertion Error\n",
    "    if error_type != 'AssertionError':\n",
    "        return NON_ASSERTION_ERROR\n",
    "    if error_type == 'AssertionError':\n",
    "        if initial_assertion_header.lower() in block.lower():\n",
    "            return IA_FAILED_ASSERTION\n",
    "        if final_assertion_header.lower() in block.lower():\n",
    "            return FA_FAILED_ASSERTION\n",
    "    return UNDEFINED_ERROR\n",
    "\n",
    "def get_auto_qc_status(status_w_action, status_wo_action, contains_final_assert, contains_golden_answer):\n",
    "    if NON_ASSERTION_ERROR in [status_w_action, status_wo_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    if IA_FAILED_ASSERTION in [status_w_action, status_wo_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    if FA_FAILED_ASSERTION in [status_w_action]:\n",
    "        return NEEDS_FIXES\n",
    "\n",
    "    \n",
    "    if status_w_action == NO_ERROR_FOUND:\n",
    "        if status_wo_action == NO_ERROR_FOUND:\n",
    "            if contains_final_assert:\n",
    "                return NEEDS_FIXES\n",
    "            if not contains_final_assert and not contains_golden_answer:\n",
    "                return NEEDS_FIXES\n",
    "            return GOOD_TO_GO\n",
    "        \n",
    "        if status_wo_action == FA_FAILED_ASSERTION:\n",
    "            return GOOD_TO_GO\n",
    "        \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Auto QC Status\n",
       "Needs Fixes    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Execution Status w/o Action'] = merged_df['no_action_response'].apply(add_error_type)\n",
    "merged_df['Execution Status w Action'] = merged_df['with_action_response'].apply(add_error_type)\n",
    "merged_df['Auto QC Status'] = merged_df.apply(lambda row: get_auto_qc_status(row['Execution Status w Action'], \n",
    "                                                                             row['Execution Status w/o Action'],\n",
    "                                                                             row['contains_final_assert'],\n",
    "                                                                             row['contains_golden_answer']), axis=1)\n",
    "merged_df['Auto QC Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>colab_url</th>\n",
       "      <th>colab_name</th>\n",
       "      <th>no_action_script_success</th>\n",
       "      <th>no_action_response</th>\n",
       "      <th>with_action_script_success</th>\n",
       "      <th>with_action_response</th>\n",
       "      <th>contains_golden_answer</th>\n",
       "      <th>contains_final_assert</th>\n",
       "      <th>Execution Status w/o Action</th>\n",
       "      <th>Execution Status w Action</th>\n",
       "      <th>Auto QC Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185_base</td>\n",
       "      <td>https://drive.google.com/file/d/1ntAczmtrIzGQB...</td>\n",
       "      <td>Agent-185-Base.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312_base</td>\n",
       "      <td>https://drive.google.com/file/d/1NGSwUOLK_2reN...</td>\n",
       "      <td>Agent-312-Base.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>506_base</td>\n",
       "      <td>https://drive.google.com/file/d/1Wh17vrxhVEgmK...</td>\n",
       "      <td>Agent-506_base-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414_base</td>\n",
       "      <td>https://drive.google.com/file/d/1g_duG0timZpsO...</td>\n",
       "      <td>Agent-414_base-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>235_base</td>\n",
       "      <td>https://drive.google.com/file/d/19rJV18R_xxCrP...</td>\n",
       "      <td>Agent-235_base-Merged.ipynb</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Block: ## Import APIs and initiate DBs\\nError ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Non Assertion Error</td>\n",
       "      <td>Needs Fixes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id                                          colab_url  \\\n",
       "0  185_base  https://drive.google.com/file/d/1ntAczmtrIzGQB...   \n",
       "1  312_base  https://drive.google.com/file/d/1NGSwUOLK_2reN...   \n",
       "2  506_base  https://drive.google.com/file/d/1Wh17vrxhVEgmK...   \n",
       "3  414_base  https://drive.google.com/file/d/1g_duG0timZpsO...   \n",
       "4  235_base  https://drive.google.com/file/d/19rJV18R_xxCrP...   \n",
       "\n",
       "                    colab_name  no_action_script_success  \\\n",
       "0         Agent-185-Base.ipynb                      True   \n",
       "1         Agent-312-Base.ipynb                      True   \n",
       "2  Agent-506_base-Merged.ipynb                      True   \n",
       "3  Agent-414_base-Merged.ipynb                      True   \n",
       "4  Agent-235_base-Merged.ipynb                      True   \n",
       "\n",
       "                                  no_action_response  \\\n",
       "0  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "1  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "2  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "3  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "4  Block: ## Import APIs and initiate DBs\\nError ...   \n",
       "\n",
       "   with_action_script_success  \\\n",
       "0                        True   \n",
       "1                        True   \n",
       "2                        True   \n",
       "3                        True   \n",
       "4                        True   \n",
       "\n",
       "                                with_action_response  contains_golden_answer  \\\n",
       "0  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "1  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "2  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "3  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "4  Block: ## Import APIs and initiate DBs\\nError ...                   False   \n",
       "\n",
       "   contains_final_assert Execution Status w/o Action  \\\n",
       "0                   True         Non Assertion Error   \n",
       "1                   True         Non Assertion Error   \n",
       "2                   True         Non Assertion Error   \n",
       "3                   True         Non Assertion Error   \n",
       "4                   True         Non Assertion Error   \n",
       "\n",
       "  Execution Status w Action Auto QC Status  \n",
       "0       Non Assertion Error    Needs Fixes  \n",
       "1       Non Assertion Error    Needs Fixes  \n",
       "2       Non Assertion Error    Needs Fixes  \n",
       "3       Non Assertion Error    Needs Fixes  \n",
       "4       Non Assertion Error    Needs Fixes  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.insert(loc=1, column='colab_id', value=merged_df['colab_url'].apply(GoogleDrive.extract_file_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 13 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   sample_id                    10 non-null     object\n",
      " 1   colab_id                     10 non-null     object\n",
      " 2   colab_url                    10 non-null     object\n",
      " 3   colab_name                   10 non-null     object\n",
      " 4   no_action_script_success     10 non-null     bool  \n",
      " 5   no_action_response           10 non-null     object\n",
      " 6   with_action_script_success   10 non-null     bool  \n",
      " 7   with_action_response         10 non-null     object\n",
      " 8   contains_golden_answer       10 non-null     bool  \n",
      " 9   contains_final_assert        10 non-null     bool  \n",
      " 10  Execution Status w/o Action  10 non-null     object\n",
      " 11  Execution Status w Action    10 non-null     object\n",
      " 12  Auto QC Status               10 non-null     object\n",
      "dtypes: bool(4), object(9)\n",
      "memory usage: 892.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\"> does not exist in the spreadsheet. Creating a new tab.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mTab \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m does not exist in the spreadsheet. Creating a new tab.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Successfully added new tab: </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\"> with ID: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">637186508</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mSuccessfully added new tab: \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m with ID: \u001b[0m\u001b[1;36m637186508\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Uploading </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008080; text-decoration-color: #008080\"> rows to tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mUploading \u001b[0m\u001b[1;36m10\u001b[0m\u001b[36m rows to tab \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">143</span><span style=\"color: #008080; text-decoration-color: #008080\"> cells updated in tab </span><span style=\"color: #008000; text-decoration-color: #008000\">'auto_qc_response_test'</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m143\u001b[0m\u001b[36m cells updated in tab \u001b[0m\u001b[32m'auto_qc_response_test'\u001b[0m\u001b[36m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tab = 'auto_qc_response_test'\n",
    "GoogleSheet.add_dataframe_to_sheet(sheet_id, merged_df, output_tab, drop_duplicates_on = ['colab_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turing_e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
